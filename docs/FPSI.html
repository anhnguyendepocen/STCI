<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Fundamental Problem of Statistical Inference | Statistical Tools for Causal Inference</title>
  <meta name="description" content="This is an open source collaborative book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Fundamental Problem of Statistical Inference | Statistical Tools for Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source collaborative book." />
  <meta name="github-repo" content="chabefer/STCI" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Fundamental Problem of Statistical Inference | Statistical Tools for Causal Inference" />
  
  <meta name="twitter:description" content="This is an open source collaborative book." />
  

<meta name="author" content="The SKY Community">


<meta name="date" content="2019-10-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="FPCI.html">
<link rel="next" href="RCT.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\newcommand{\uns}[1]{\mathbf{1}[#1]}
\newcommand{\esp}[1]{\mathbf{E}[#1]}
\newcommand{\Ind}{\perp\kern-5pt\perp}
\newcommand{\var}[1]{\mathbf{V}[ #1 ]}
\newcommand{\cov}[1]{\mathbf{C}[ #1 ]}
\newcommand{\plim}[1]{\text{plim}_{ #1 \rightarrow \infty}}
\newcommand{\plims}{\text{plim}}
\newcommand{\partder}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\diag}{diag}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Tools for Causal Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>I Fundamental Problems of Inference</b></span></li>
<li class="chapter" data-level="" data-path="introduction-the-two-fundamental-problems-of-inference.html"><a href="introduction-the-two-fundamental-problems-of-inference.html"><i class="fa fa-check"></i>Introduction: the Two Fundamental Problems of Inference</a></li>
<li class="chapter" data-level="1" data-path="FPCI.html"><a href="FPCI.html"><i class="fa fa-check"></i><b>1</b> Fundamental Problem of Causal Inference</a><ul>
<li class="chapter" data-level="1.1" data-path="FPCI.html"><a href="FPCI.html#rubin-causal-model"><i class="fa fa-check"></i><b>1.1</b> Rubin Causal Model</a><ul>
<li class="chapter" data-level="1.1.1" data-path="FPCI.html"><a href="FPCI.html#treatment-allocation-rule"><i class="fa fa-check"></i><b>1.1.1</b> Treatment allocation rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="FPCI.html"><a href="FPCI.html#potential-outcomes"><i class="fa fa-check"></i><b>1.1.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="1.1.3" data-path="FPCI.html"><a href="FPCI.html#switching-equation"><i class="fa fa-check"></i><b>1.1.3</b> Switching equation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="FPCI.html"><a href="FPCI.html#treatment-effects"><i class="fa fa-check"></i><b>1.2</b> Treatment effects</a><ul>
<li class="chapter" data-level="1.2.1" data-path="FPCI.html"><a href="FPCI.html#individual-level-treatment-effects"><i class="fa fa-check"></i><b>1.2.1</b> Individual level treatment effects</a></li>
<li class="chapter" data-level="1.2.2" data-path="FPCI.html"><a href="FPCI.html#average-treatment-effect-on-the-treated"><i class="fa fa-check"></i><b>1.2.2</b> Average treatment effect on the treated</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="FPCI.html"><a href="FPCI.html#fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>1.3</b> Fundamental problem of causal inference</a></li>
<li class="chapter" data-level="1.4" data-path="FPCI.html"><a href="FPCI.html#intuitive-estimators-confounding-factors-and-selection-bias"><i class="fa fa-check"></i><b>1.4</b> Intuitive estimators, confounding factors and selection bias</a><ul>
<li class="chapter" data-level="1.4.1" data-path="FPCI.html"><a href="FPCI.html#withwithout-comparison-selection-bias-and-cross-sectional-confounders"><i class="fa fa-check"></i><b>1.4.1</b> With/Without comparison, selection bias and cross-sectional confounders</a></li>
<li class="chapter" data-level="1.4.2" data-path="FPCI.html"><a href="FPCI.html#the-beforeafter-comparison-temporal-confounders-and-time-trend-bias"><i class="fa fa-check"></i><b>1.4.2</b> The before/after comparison, temporal confounders and time trend bias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="FPSI.html"><a href="FPSI.html"><i class="fa fa-check"></i><b>2</b> Fundamental Problem of Statistical Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="FPSI.html"><a href="FPSI.html#sec:sampnoise"><i class="fa fa-check"></i><b>2.1</b> What is sampling noise? Definition and illustration</a><ul>
<li class="chapter" data-level="2.1.1" data-path="FPSI.html"><a href="FPSI.html#sec:definitionnoise"><i class="fa fa-check"></i><b>2.1.1</b> Sampling noise, a definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisepop"><i class="fa fa-check"></i><b>2.1.2</b> Sampling noise for the population treatment effect</a></li>
<li class="chapter" data-level="2.1.3" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisesamp"><i class="fa fa-check"></i><b>2.1.3</b> Sampling noise for the sample treatment effect</a></li>
<li class="chapter" data-level="2.1.4" data-path="FPSI.html"><a href="FPSI.html#sec:confinterv"><i class="fa fa-check"></i><b>2.1.4</b> Building confidence intervals from estimates of sampling noise</a></li>
<li class="chapter" data-level="2.1.5" data-path="FPSI.html"><a href="FPSI.html#reporting-sampling-noise-a-proposal"><i class="fa fa-check"></i><b>2.1.5</b> Reporting sampling noise: a proposal</a></li>
<li class="chapter" data-level="2.1.6" data-path="FPSI.html"><a href="FPSI.html#sec:effectsize"><i class="fa fa-check"></i><b>2.1.6</b> Using effect sizes to normalize the reporting of treatment effects and their precision</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="FPSI.html"><a href="FPSI.html#sec:estimsampnoise"><i class="fa fa-check"></i><b>2.2</b> Estimating sampling noise</a><ul>
<li class="chapter" data-level="2.2.1" data-path="FPSI.html"><a href="FPSI.html#sec:assumptions"><i class="fa fa-check"></i><b>2.2.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.2.2" data-path="FPSI.html"><a href="FPSI.html#sec:cheb"><i class="fa fa-check"></i><b>2.2.2</b> Using Chebyshev’s inequality</a></li>
<li class="chapter" data-level="2.2.3" data-path="FPSI.html"><a href="FPSI.html#sec:CLT"><i class="fa fa-check"></i><b>2.2.3</b> Using the Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.4" data-path="FPSI.html"><a href="FPSI.html#sec:resamp"><i class="fa fa-check"></i><b>2.2.4</b> Using resampling methods</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Methods of Causal Inference</b></span></li>
<li class="chapter" data-level="3" data-path="RCT.html"><a href="RCT.html"><i class="fa fa-check"></i><b>3</b> Randomized Controlled Trials</a><ul>
<li class="chapter" data-level="3.1" data-path="RCT.html"><a href="RCT.html#sec:design1"><i class="fa fa-check"></i><b>3.1</b> Brute Force Design</a><ul>
<li class="chapter" data-level="3.1.1" data-path="RCT.html"><a href="RCT.html#identification"><i class="fa fa-check"></i><b>3.1.1</b> Identification</a></li>
<li class="chapter" data-level="3.1.2" data-path="RCT.html"><a href="RCT.html#estimating-ate"><i class="fa fa-check"></i><b>3.1.2</b> Estimating ATE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="RCT.html"><a href="RCT.html#sec:design2"><i class="fa fa-check"></i><b>3.2</b> Randomization After Self-Selection</a><ul>
<li class="chapter" data-level="3.2.1" data-path="RCT.html"><a href="RCT.html#identification-1"><i class="fa fa-check"></i><b>3.2.1</b> Identification</a></li>
<li class="chapter" data-level="3.2.2" data-path="RCT.html"><a href="RCT.html#estimating-tt"><i class="fa fa-check"></i><b>3.2.2</b> Estimating TT</a></li>
<li class="chapter" data-level="3.2.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-1"><i class="fa fa-check"></i><b>3.2.3</b> Estimating Sampling Noise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="RCT.html"><a href="RCT.html#sec:design3"><i class="fa fa-check"></i><b>3.3</b> Randomization After Eligibility</a><ul>
<li class="chapter" data-level="3.3.1" data-path="RCT.html"><a href="RCT.html#identification-2"><i class="fa fa-check"></i><b>3.3.1</b> Identification</a></li>
<li class="chapter" data-level="3.3.2" data-path="RCT.html"><a href="RCT.html#estimating-the-ite-and-the-tt"><i class="fa fa-check"></i><b>3.3.2</b> Estimating the ITE and the TT</a></li>
<li class="chapter" data-level="3.3.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-2"><i class="fa fa-check"></i><b>3.3.3</b> Estimating sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="RCT.html"><a href="RCT.html#sec:design4"><i class="fa fa-check"></i><b>3.4</b> Encouragement Design</a><ul>
<li class="chapter" data-level="3.4.1" data-path="RCT.html"><a href="RCT.html#identification-3"><i class="fa fa-check"></i><b>3.4.1</b> Identification</a></li>
<li class="chapter" data-level="3.4.2" data-path="RCT.html"><a href="RCT.html#estimating-the-local-average-treatment-effect-and-the-intention-to-treat-effect"><i class="fa fa-check"></i><b>3.4.2</b> Estimating the Local Average Treatment Effect and the Intention to Treat Effect</a></li>
<li class="chapter" data-level="3.4.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-3"><i class="fa fa-check"></i><b>3.4.3</b> Estimating sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="RCT.html"><a href="RCT.html#sec:threats"><i class="fa fa-check"></i><b>3.5</b> Threats to the validity of RCTs</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="NE.html"><a href="NE.html"><i class="fa fa-check"></i><b>4</b> Natural Experiments</a></li>
<li class="chapter" data-level="5" data-path="sec-OM.html"><a href="sec-OM.html"><i class="fa fa-check"></i><b>5</b> Observational Methods</a></li>
<li class="part"><span><b>III Additional Topics</b></span></li>
<li class="chapter" data-level="6" data-path="Power.html"><a href="Power.html"><i class="fa fa-check"></i><b>6</b> Power Analysis</a></li>
<li class="chapter" data-level="7" data-path="Placebo.html"><a href="Placebo.html"><i class="fa fa-check"></i><b>7</b> Placebo Tests</a></li>
<li class="chapter" data-level="8" data-path="cluster.html"><a href="cluster.html"><i class="fa fa-check"></i><b>8</b> Clustering</a></li>
<li class="chapter" data-level="9" data-path="LaLonde.html"><a href="LaLonde.html"><i class="fa fa-check"></i><b>9</b> LaLonde Tests</a></li>
<li class="chapter" data-level="10" data-path="Diffusion.html"><a href="Diffusion.html"><i class="fa fa-check"></i><b>10</b> Diffusion effects</a></li>
<li class="chapter" data-level="11" data-path="Distribution.html"><a href="Distribution.html"><i class="fa fa-check"></i><b>11</b> Distributional effects</a></li>
<li class="chapter" data-level="12" data-path="sec-meta.html"><a href="sec-meta.html"><i class="fa fa-check"></i><b>12</b> Meta-analysis and Publication Bias</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-meta.html"><a href="sec-meta.html#meta-analysis"><i class="fa fa-check"></i><b>12.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="12.1.1" data-path="sec-meta.html"><a href="sec-meta.html#basic-setting"><i class="fa fa-check"></i><b>12.1.1</b> Basic setting</a></li>
<li class="chapter" data-level="12.1.2" data-path="sec-meta.html"><a href="sec-meta.html#MetaWA"><i class="fa fa-check"></i><b>12.1.2</b> Meta-analysis as a weighted average</a></li>
<li class="chapter" data-level="12.1.3" data-path="sec-meta.html"><a href="sec-meta.html#constantly-updated-meta-analysis"><i class="fa fa-check"></i><b>12.1.3</b> Constantly updated meta-analysis</a></li>
<li class="chapter" data-level="12.1.4" data-path="sec-meta.html"><a href="sec-meta.html#testing-for-the-homogeneity-of-treatment-effects"><i class="fa fa-check"></i><b>12.1.4</b> Testing for the homogeneity of treatment effects</a></li>
<li class="chapter" data-level="12.1.5" data-path="sec-meta.html"><a href="sec-meta.html#meta-analysis-when-treatment-effects-are-heterogeneous"><i class="fa fa-check"></i><b>12.1.5</b> Meta-analysis when treatment effects are heterogeneous</a></li>
<li class="chapter" data-level="12.1.6" data-path="sec-meta.html"><a href="sec-meta.html#meta-regression"><i class="fa fa-check"></i><b>12.1.6</b> Meta-regression</a></li>
<li class="chapter" data-level="12.1.7" data-path="sec-meta.html"><a href="sec-meta.html#why-vote-counting-does-not-work"><i class="fa fa-check"></i><b>12.1.7</b> Why vote-counting does not work</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="sec-meta.html"><a href="sec-meta.html#publication-bias"><i class="fa fa-check"></i><b>12.2</b> Publication bias</a><ul>
<li class="chapter" data-level="12.2.1" data-path="sec-meta.html"><a href="sec-meta.html#sources-of-publication-bias"><i class="fa fa-check"></i><b>12.2.1</b> Sources of publication bias</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec-meta.html"><a href="sec-meta.html#detecting-and-correcting-for-publication-bias"><i class="fa fa-check"></i><b>12.2.2</b> Detecting and correcting for publication bias</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec-meta.html"><a href="sec-meta.html#vote-counting-and-publication-bias"><i class="fa fa-check"></i><b>12.2.3</b> Vote counting and publication bias</a></li>
<li class="chapter" data-level="12.2.4" data-path="sec-meta.html"><a href="sec-meta.html#the-value-of-a-statistically-significant-result"><i class="fa fa-check"></i><b>12.2.4</b> The value of a statistically significant result</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Bounds.html"><a href="Bounds.html"><i class="fa fa-check"></i><b>13</b> Bounds</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="proofs.html"><a href="proofs.html"><i class="fa fa-check"></i><b>A</b> Proofs</a><ul>
<li class="chapter" data-level="A.1" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-reffpsi"><i class="fa fa-check"></i><b>A.1</b> Proofs of results in Chapter @ref(FPSI)</a><ul>
<li class="chapter" data-level="A.1.1" data-path="proofs.html"><a href="proofs.html#proofcheb"><i class="fa fa-check"></i><b>A.1.1</b> Proof of Theorem @ref(thm:uppsampnoise)</a></li>
<li class="chapter" data-level="A.1.2" data-path="proofs.html"><a href="proofs.html#proofCLT"><i class="fa fa-check"></i><b>A.1.2</b> Proof of Theorem @ref(thm:asympnoiseWW)</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refrct"><i class="fa fa-check"></i><b>A.2</b> Proofs of results in Chapter @ref(RCT)</a><ul>
<li class="chapter" data-level="A.2.1" data-path="proofs.html"><a href="proofs.html#proofIdentLATE"><i class="fa fa-check"></i><b>A.2.1</b> Proof of Theorem @ref(thm:IdentLATE)</a></li>
<li class="chapter" data-level="A.2.2" data-path="proofs.html"><a href="proofs.html#proofWaldIV"><i class="fa fa-check"></i><b>A.2.2</b> Proof of Theorem @ref(thm:WaldIV)</a></li>
<li class="chapter" data-level="A.2.3" data-path="proofs.html"><a href="proofs.html#ProofAsymWald"><i class="fa fa-check"></i><b>A.2.3</b> Proof of Theorem @ref(thm:asymWald)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Tools for Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="FPSI" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Fundamental Problem of Statistical Inference</h1>
<p>The Fundamental Problem of Statistical Inference (FPSI) states that, even if we have an estimator <span class="math inline">\(E\)</span> that identifies <span class="math inline">\(TT\)</span> in the population, we cannot observe <span class="math inline">\(E\)</span> because we only have access to a finite sample of the population. The only thing that we can form from the sample is a sample equivalent <span class="math inline">\(\hat{E}\)</span> to the population quantity <span class="math inline">\(E\)</span>, and <span class="math inline">\(\hat{E}\neq E\)</span>. For example, the sample analog to <span class="math inline">\(WW\)</span> is the difference in means between treated and untreated units <span class="math inline">\(\hat{WW}\)</span>. As we saw in the last lecture, <span class="math inline">\(\hat{WW}\)</span> is never exactly equal to <span class="math inline">\(WW\)</span>.</p>
<p>Why is <span class="math inline">\(\hat{E}\neq E\)</span>? Because a finite sample is never perfectly representative of the population. In a sample, even in a random sample, the distribution of the observed and unobserved covariates deviates from the true population one. As a consequence, the sample value of the estimator is never precisely equal to the population value, but fluctuates around it with sampling noise. The main problem with the FPSI is that if we find an effect of our treatment, be it small or large, we cannot know whether we should attribute it to the treatment or to the bad or good luck of sampling noise.</p>
<p>What can we do to deal with the FPSI? I am going to argue that there are mainly two things that we might want to do: estimating the extent of sampling noise and decreasing sampling noise.</p>
<p>Estimating sampling noise means measuring how much variability there is in our estimate <span class="math inline">\(\hat{E}\)</span> due to the sampling procedure. This is very useful because it enables us to form a confidence interval that gauges how far from <span class="math inline">\(\hat{E}\)</span> the true value <span class="math inline">\(E\)</span> might be. It is a measure of the precision of our estimation and of the extent to which sampling noise might drive our results. Estimating sampling noise is very hard because we have only access to one sample and we would like to know the behavior of our estimator over repeated samples. We are going to learn four ways to estimate the extent of sampling noise using data from one sample.</p>
<p>Because sampling noise is such a nuisance and makes our estimates imprecise, we would like to be able to make it as small as possible. We are going to study three ways of decreasing sampling noise, two that take place before collecting the data (increasing sample size, stratifying) and one that takes place after (conditioning).</p>
<p>Maybe you are surprised not to find statistical significance tests as an important answer to the FPSI. I argue in this lecture that statistical tests are misleading tools that make us overestimate the confidence in our results and underestimate the scope of sampling noise. Statistical tests are not meant to be used for scientific research, but were originally designed to make decisions in industrial settings where the concept of successive sampling made actual sense. Statistical tests also generate collective behaviors such as publication bias and specification search that undermine the very foundations of science. A general movement in the social sciences, but also in physics, is starting to ban the reporting of p-values.</p>
<div id="sec:sampnoise" class="section level2">
<h2><span class="header-section-number">2.1</span> What is sampling noise? Definition and illustration</h2>
<p>In this section, I am going to define sampling noise and illustrate it with a numerical example. In Section <a href="FPSI.html#sec:definitionnoise">2.1.1</a>, I define sampling noise. In section <a href="FPSI.html#sec:illusnoisepop">2.1.2</a>, I illustrate how sampling noise varies when one is interested in the population treatment effect. In section <a href="FPSI.html#sec:illusnoisesamp">2.1.3</a>, I illustrate how sampling noise varies when one is interesetd in the sample treatment effect. Finally, in section <a href="FPSI.html#sec:confinterv">2.1.4</a>, I show how confidence intervals can be built from an estimate of sampling noise.</p>
<div id="sec:definitionnoise" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Sampling noise, a definition</h3>
<p>Sampling noise measures how much sampling variability moves the sample estimator <span class="math inline">\(\hat{E}\)</span> around. One way to define it more rigorously is to make it equal to the width of a confidence interval:</p>

<div class="definition">
<span id="def:sampnoise" class="definition"><strong>Definition 2.1  (Sampling noise)  </strong></span>Sampling noise is the width of the symmetric interval around TT within which <span class="math inline">\(\delta*100\)</span>% of the sample estimators fall, where <span class="math inline">\(\delta\)</span> is the confidence level. As a consequence, sampling noise is equal to <span class="math inline">\(2\epsilon\)</span> where <span class="math inline">\(\epsilon\)</span> is such that:
<span class="math display">\[\begin{align*}
\Pr(|\hat{E}-TT|\leq\epsilon) &amp;= \delta.
\end{align*}\]</span>
</div>

<p>This definition tries to capture the properties of the distribution of <span class="math inline">\(\hat{E}\)</span> using only one number. As every simplification, it leaves room for dissatisfaction, exactly as a 2D map is a convenient albeit arbitrary betrayal of a 3D phenomenon. For example, there is nothing sacred about the symmetry of the interval. It is just extremely convenient. One might prefer an interval that is symmetric in tail probabilities instead. Feel free to explore with different concepts if you like.</p>
<p>A related concept to that of sampling noise is that of precision: the smaller the sampling noise, the higher the precision. Precision can be defined for example as the inverse of sampling noise <span class="math inline">\(\frac{1}{2\epsilon}\)</span>.</p>
<p>Finally, a very useful concept is that of signal to noise ratio. It is not used in economics, but physicists use this concept all the time. The signal to noise ratio measures the treatment effect in multiple of the sampling noise. If they are of the same order of magnitude, we have a lot of noise and little confidence in our estimates. If the signal is much larger than the noise, we tend to have a lot of confidence in our parameter estimates. The signal to noise ratio can be computed as follows: <span class="math inline">\(\frac{E}{2\epsilon}\)</span> or <span class="math inline">\(\frac{\hat{E}}{2\epsilon}\)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> A very striking result is that the signal to noise ratio of a result that is marginally significant at the 5% level is very small, around one half, meaning that the noise is generally double the signal in these results. We will derive this result after studying how to estimate sampling noise with real data.
</div>

<p>There are two distinct ways of understanding sampling noise, depending on whether we are after the population treatment effect (<span class="math inline">\(\Delta^Y_{TT}\)</span>) or the sample treatment effect (<span class="math inline">\(\Delta^Y_{TT_s}\)</span>). Sampling noise for the population treatment effect stems from the fact that the sample is not perfectly representative of the population. The sample differs from the population and thus the sample estimates differs from the population estimate. Sampling noise for the sample parameter stems from the fact that the control group is not a perfect embodiment of the counterfactual. Discrepancies between treated and control samples are going to generate differences between the <span class="math inline">\(WW\)</span> estimate and the <span class="math inline">\(TT\)</span> effect in the sample.</p>
</div>
<div id="sec:illusnoisepop" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Sampling noise for the population treatment effect</h3>
<p>Sampling noise for the population treatment effect stems from the fact that the sample is not perfectly representative of the population.</p>

<div class="example">
<span id="exm:unnamed-chunk-33" class="example"><strong>Example 2.1  </strong></span>In order to assess the scope of sampling noise for our population treatment effect estimate, let’s first draw a sample. In order to be able to do that, I first have to define the parameter values:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">param &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">8</span>,.<span class="dv">5</span>,.<span class="dv">28</span>,<span class="dv">1500</span>,<span class="fl">0.9</span>,<span class="fl">0.01</span>,<span class="fl">0.05</span>,<span class="fl">0.05</span>,<span class="fl">0.05</span>,<span class="fl">0.1</span>)
<span class="kw">names</span>(param) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;barmu&quot;</span>,<span class="st">&quot;sigma2mu&quot;</span>,<span class="st">&quot;sigma2U&quot;</span>,<span class="st">&quot;barY&quot;</span>,<span class="st">&quot;rho&quot;</span>,<span class="st">&quot;theta&quot;</span>,<span class="st">&quot;sigma2epsilon&quot;</span>,<span class="st">&quot;sigma2eta&quot;</span>,<span class="st">&quot;delta&quot;</span>,<span class="st">&quot;baralpha&quot;</span>)
param</code></pre></div>
<pre><code>##         barmu      sigma2mu       sigma2U          barY           rho 
##          8.00          0.50          0.28       1500.00          0.90 
##         theta sigma2epsilon     sigma2eta         delta      baralpha 
##          0.01          0.05          0.05          0.05          0.10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)

delta.y.ate &lt;-<span class="st"> </span><span class="cf">function</span>(param){
  <span class="kw">return</span>(param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>])
}</code></pre></div>
<p>In this sample, the <span class="math inline">\(WW\)</span> estimator yields an estimate of <span class="math inline">\(\hat{\Delta^y_{WW}}=\)</span> 0.133. Despite random assignment, we have <span class="math inline">\(\hat{\Delta^y_{WW}}\neq\Delta^y_{TT}=\)</span> 0.18, an instance of the FPSI.</p>
<p>In order to see how sampling noise varies, let’s draw another sample. In order to do so, I am going to choose a different seed to initialize the pseudo-random number generator in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12345</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)</code></pre></div>
<p>In this sample, the <span class="math inline">\(WW\)</span> estimator yields an estimate of <span class="math inline">\(\hat{\Delta^y_{WW}}=\)</span> 0.179. Again, despite random assignment, we have <span class="math inline">\(\hat{\Delta^y_{WW}}\neq\Delta^y_{TT}=\)</span> 0.18, an instance of the FPSI. Furthermore, the estimate of the population treatment effect in this sample differs from the previous one, a consequence of sampling noise.</p>
<p>Let’s now visualize the extent of sampling noise by repeating the procedure multiple times with various sample sizes. This is called Monte Carlo replications: in each replication, I choose a sample size, draw one sample from the population and compute the <span class="math inline">\(\hat{WW}\)</span> estimator. At each replication, the sample I’m using is different, reflecting the actual sampling process and enabling me to gauge the extent of sampling noise. In order to focus on sampling noise alone, I am running the replications in the model in which selection into the treatment is independent on potential outcomes, so that <span class="math inline">\(WW=TT\)</span> in the population. In order to speed up the process, I am using parallelized computing: I send each sample to a different core in my computer so that several samples can be run at the same time. You might want to adapt the program below to the number of cores you actually have using the <code>ncpus</code> variable in the beginning of the <code>.Rmd</code> file that generates this page.. In order to parallelize computations, I use the Snowfall package in R, that gives very simple and intuitive parallelization commands. In order to save time when generating the graph, I use the wonderful “cache” option of knitr: it stores the estimates from the code chunk and will not rerun it as long as the code inside the chunk has not been altered nor the code of the chunks that it depends on (parameter values, for example).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.ww &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
  Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  <span class="kw">return</span>(<span class="kw">c</span>((<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>Ds)<span class="op">-</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">-</span>Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)),<span class="kw">var</span>(y[Ds<span class="op">==</span><span class="dv">1</span>]),<span class="kw">var</span>(y[Ds<span class="op">==</span><span class="dv">0</span>]),<span class="kw">mean</span>(Ds)))
}

simuls.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.ww &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">4</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">colnames</span>(simuls.ww) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;V1&#39;</span>,<span class="st">&#39;V0&#39;</span>,<span class="st">&#39;p&#39;</span>)
  <span class="kw">return</span>(simuls.ww)
}

sf.simuls.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span>ncpus)
  sim &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">4</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;V1&#39;</span>,<span class="st">&#39;V0&#39;</span>,<span class="st">&#39;p&#39;</span>)
  <span class="kw">return</span>(sim)
}

simuls.ww &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.ww.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)</code></pre></div>
<pre><code>## R Version:  R version 3.4.3 (2017-11-30)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  <span class="kw">hist</span>(simuls.ww[[i]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(Delta<span class="op">^</span>yWW)),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.55</span>))
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">delta.y.ate</span>(param),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:montecarlo"></span>
<img src="STCI_files/figure-html/montecarlo-1.png" alt="Distribution of the $WW$ estimator over replications of samples of different sizes" width="60%" />
<p class="caption">
Figure 2.1: Distribution of the <span class="math inline">\(WW\)</span> estimator over replications of samples of different sizes
</p>
</div>
<p>Figure <a href="FPSI.html#fig:montecarlo">2.1</a> is essential to understanding statistical inference and the properties of our estimators. We can see on Figure <a href="FPSI.html#fig:montecarlo">2.1</a> that the estimates indeed move around at each sample replication. We can also see that the estimates seem to be concentrated around the truth. We also see that the estimates are more and more concentrated around the truth as sample size grows larger and larger.</p>
<p>How big is sampling noise in all of these examples? We can compute it by using the replications as approximations to the true distribution of the estimator after an infinite number of samples has been drawn. Let’s first choose a confidence level and then compute the empirical equivalent to the formula in Definition <a href="FPSI.html#def:sampnoise">2.1</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta&lt;-<span class="st"> </span><span class="fl">0.99</span>
delta.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="fl">0.95</span>
samp.noise &lt;-<span class="st"> </span><span class="cf">function</span>(estim,delta){
  <span class="kw">return</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(<span class="kw">delta.y.ate</span>(param)<span class="op">-</span>estim),<span class="dt">prob=</span>delta))
}
samp.noise.ww &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.ww,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),samp.noise,<span class="dt">delta=</span>delta)
<span class="kw">names</span>(samp.noise.ww) &lt;-<span class="st"> </span>N.sample
samp.noise.ww</code></pre></div>
<pre><code>##        100       1000      10000      1e+05 
## 1.09916429 0.39083801 0.11582492 0.03527744</code></pre>
<p>Let’s also compute precision and the signal to noise ratio and put all of these results together in a nice table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">precision &lt;-<span class="st"> </span><span class="cf">function</span>(estim,delta){
  <span class="kw">return</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">samp.noise</span>(estim,delta))
}
signal.to.noise &lt;-<span class="st"> </span><span class="cf">function</span>(estim,delta,param){
  <span class="kw">return</span>(<span class="kw">delta.y.ate</span>(param)<span class="op">/</span><span class="kw">samp.noise</span>(estim,delta))
}
precision.ww &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.ww,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),precision,<span class="dt">delta=</span>delta)
<span class="kw">names</span>(precision.ww) &lt;-<span class="st"> </span>N.sample
signal.to.noise.ww &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.ww,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),signal.to.noise,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)
<span class="kw">names</span>(signal.to.noise.ww) &lt;-<span class="st"> </span>N.sample
table.noise &lt;-<span class="st"> </span><span class="kw">cbind</span>(samp.noise.ww,precision.ww,signal.to.noise.ww)
<span class="kw">colnames</span>(table.noise) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Sampling noise&#39;</span>, <span class="st">&#39;Precision&#39;</span>, <span class="st">&#39;Signal to noise ratio&#39;</span>)
knitr<span class="op">::</span><span class="kw">kable</span>(table.noise,<span class="dt">caption=</span><span class="kw">paste</span>(<span class="st">&#39;Sampling noise of $</span><span class="ch">\\</span><span class="st">hat{WW}$ for the population treatment effect with $</span><span class="ch">\\</span><span class="st">delta=$&#39;</span>,delta,<span class="st">&#39;for various sample sizes&#39;</span>,<span class="dt">sep=</span><span class="st">&#39; &#39;</span>),<span class="dt">booktabs=</span><span class="ot">TRUE</span>,<span class="dt">digits =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">align=</span><span class="kw">c</span>(<span class="st">&#39;c&#39;</span>,<span class="st">&#39;c&#39;</span>,<span class="st">&#39;c&#39;</span>))</code></pre></div>
<table>
<caption><span id="tab:precisionsignal">Table 2.1: </span>Sampling noise of <span class="math inline">\(\hat{WW}\)</span> for the population treatment effect with <span class="math inline">\(\delta=\)</span> 0.99 for various sample sizes</caption>
<thead>
<tr class="header">
<th></th>
<th align="center">Sampling noise</th>
<th align="center">Precision</th>
<th align="center">Signal to noise ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100</td>
<td align="center">1.10</td>
<td align="center">0.91</td>
<td align="center">0.16</td>
</tr>
<tr class="even">
<td>1000</td>
<td align="center">0.39</td>
<td align="center">2.56</td>
<td align="center">0.46</td>
</tr>
<tr class="odd">
<td>10000</td>
<td align="center">0.12</td>
<td align="center">8.63</td>
<td align="center">1.55</td>
</tr>
<tr class="even">
<td>1e+05</td>
<td align="center">0.04</td>
<td align="center">28.35</td>
<td align="center">5.10</td>
</tr>
</tbody>
</table>
<p>Finally, a nice way to summarize the extent of sampling noise is to graph how sampling noise varies around the true treatment effect, as shown on Figure <a href="FPSI.html#fig:precision">2.2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colnames</span>(table.noise) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;sampling.noise&#39;</span>, <span class="st">&#39;precision&#39;</span>, <span class="st">&#39;signal.to.noise&#39;</span>)
table.noise &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(table.noise)
table.noise<span class="op">$</span>N &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">rownames</span>(table.noise))
table.noise<span class="op">$</span>TT &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">delta.y.ate</span>(param),<span class="kw">nrow</span>(table.noise))
<span class="kw">ggplot</span>(table.noise, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>TT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:precision"></span>
<img src="STCI_files/figure-html/precision-1.png" alt="Sampling noise of $\hat{WW}$ (99\% confidence) around $TT$ for various sample sizes" width="60%" />
<p class="caption">
Figure 2.2: Sampling noise of <span class="math inline">\(\hat{WW}\)</span> (99% confidence) around <span class="math inline">\(TT\)</span> for various sample sizes
</p>
</div>
<p>With <span class="math inline">\(N=\)</span> 100, we can definitely see on Figure <a href="FPSI.html#fig:precision">2.2</a> that sampling noise is ridiculously large, especially compared with the treatment effect that we are trying to estimate. The signal to noise ratio is 0.16, which means that sampling noise is an order of magnitude bigger than the signal we are trying to extract. As a consequence, in 22.2% of our samples, we are going to estimate a negative effect of the treatment. There is also a 20.4% chance that we end up estimating an effect that is double the true effect. So how much can we trust our estimate from one sample to be close to the true effect of the treatment when <span class="math inline">\(N=\)</span> 100? Not much.</p>
<p>With <span class="math inline">\(N=\)</span> 1000, sampling noise is still large: the signal to noise ratio is 0.46, which means that sampling noise is double the signal we are trying to extract. As a consequence, the chance that we end up with a negative treatment effect has decreased to 0.9% and that we end up with an effect double the true one is 1%. But still, the chances that we end up with an effect that is smaller than three quarters of the true effect is 25.6% and the chances that we end up with an estimator that is 25% bigger than the true effect is 26.2%. These are nontrivial differences: compare a program that increases earnings by 13.5% to one that increases them by 18% and another by 22.5%. They would have completely different cost/benefit ratios. But we at least trust our estimator to give us a correct idea of the sign of the treatment effect and a vague and imprecise idea of its magnitude.</p>
<p>With <span class="math inline">\(N=\)</span> 10^{4}, sampling noise is smaller than the signal, which is encouraging. The signal to noise ratio is 1.55. In only 1% of the samples does the estimated effect of the treatment become smaller than 0.125 or bigger than 0.247. We start gaining a lot of confidence in the relative magnitude of the effect, even if sampling noise is still responsible for economically significant variation.</p>
<p>With <span class="math inline">\(N=\)</span> 10^{5}, sampling noise has become trivial. The signal to noise ratio is 5.1, which means that the signal is now 5 times bigger than the sampling noise. In only 1% of the samples does the estimated effect of the treatment become smaller than 0.163 or bigger than 0.198. Sampling noise is not any more responsible for economically meaningful variation.</p>
</div>
<div id="sec:illusnoisesamp" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Sampling noise for the sample treatment effect</h3>
<p>Sampling noise for the sample parameter stems from the fact that the treated and control groups are not perfectly identical. The distribution of observed and unobserved covariates is actually different, because of sampling variation. This makes the actual comparison of means in the sample a noisy estimate of the true comparison that we would obtain by comparing the potential outcomes of the treated directly.</p>
<p>In order to understand this issue well and to be able to illustrate it correctly, I am going to focus on the average treatment effect in the whole sample, not on the treated: <span class="math inline">\(\Delta^Y_{ATE_s}=\frac{1}{N}\sum_{i=1}^N(Y_i^1-Y_i^0)\)</span>. This enables me to define a sample parameter that is independent of the allocation of <span class="math inline">\(D_i\)</span>. This is without important consequences since these two parameters are equal in the population when there is no selection bias, as we are assuming since the beginning of this lecture. Furthermore, if we view the treatment allocation generating no selection bias as a true random assignment in a Randomized Controlled Trial (RCT), then it is still possible to use this approach to estimate <span class="math inline">\(TT\)</span> if we view the population over which we randomise as the population selected for receiving the treatment, as we will see in the lecture on RCTs.</p>

<div class="example">
<span id="exm:unnamed-chunk-34" class="example"><strong>Example 2.2  </strong></span>In order to assess the scope of sampling noise for our sample treatment effect estimate, we first have to draw a sample:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)</code></pre></div>
<p>In this sample, the treatment effect parameter is <span class="math inline">\(\Delta^y_{ATE_s}=\)</span> 0.171. The <span class="math inline">\(WW\)</span> estimator yields an estimate of <span class="math inline">\(\hat{\Delta^y_{WW}}=\)</span> 0.133. Despite random assignment, we have <span class="math inline">\(\Delta^y_{ATE_s}\neq\hat{\Delta^y_{WW}}\)</span>, an instance of the FPSI.</p>
<p>In order to see how sampling noise varies, let’s draw a new treatment allocation, while retaining the same sample and the same potential outcomes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12345</span>)
N &lt;-<span class="dv">1000</span>
Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)</code></pre></div>
<p>In this sample, the treatment effect parameter is still <span class="math inline">\(\Delta^y_{ATE_s}=\)</span> 0.171. The <span class="math inline">\(WW\)</span> estimator yields now an estimate of <span class="math inline">\(\hat{\Delta^y_{WW}}=\)</span> 0.051. The <span class="math inline">\(WW\)</span> estimate is different from our previous estimate because the treatment was allocated to a different random subset of people.</p>
<p>Why is this second estimate so imprecise? It might because it estimates one of the two components of the average treatment effect badly, or both. The true average potential outcome with the treatment is, in this sample, <span class="math inline">\(\frac{1}{N}\sum_{i=1}^Ny_i^1=\)</span> 8.207 while the <span class="math inline">\(WW\)</span> estimate of this quantity is <span class="math inline">\(\frac{1}{\sum_{i=1}^ND_i}\sum_{i=1}^ND_iy_i=\)</span> 8.113. The true average potential outcome without the treatment is, in this sample, <span class="math inline">\(\frac{1}{N}\sum_{i=1}^Ny_i^0=\)</span> 8.036 while the <span class="math inline">\(WW\)</span> estimate of this quantity is <span class="math inline">\(\frac{1}{\sum_{i=1}^N(1-D_i)}\sum_{i=1}^N(1-D_i)y_i=\)</span> 8.062. It thus seems that most of the bias in the estimated effect stems from the fact that the treatment has been allocated to individuals with lower than expected outcomes with the treatment, be it because they did not react strongly to the treatment, or because they were in worse shape without the treatment. We can check which one of these two explanations is more important. The true average effect of the treatment is, in this sample, <span class="math inline">\(\frac{1}{N}\sum_{i=1}^N(y_i^1-y^0_i)=\)</span> 0.171 while, in the treated group, this quantity is <span class="math inline">\(\frac{1}{\sum_{i=1}^ND_i}\sum_{i=1}^ND_i(y_i^1-y_i^0)=\)</span> 0.18. The true average potential outcome without the treatment is, in this sample, <span class="math inline">\(\frac{1}{N}\sum_{i=1}^Ny^0_i=\)</span> 8.036 while, in the treated group, this quantity is <span class="math inline">\(\frac{1}{\sum_{i=1}^ND_i}\sum_{i=1}^ND_iy_i^0=\)</span> 7.933. The reason for the poor performance of the <span class="math inline">\(WW\)</span> estimator in this sample is that individuals with lower counterfactual outcomes were included in the treated group, not that the treatment had lower effects on them. The bad counterfactual outcomes of the treated generates a bias of -0.103, while the bias due to heterogeneous reactions to the treatment is of 0.009. The last part of the bias is the one due to the fact that the individuals in the control group have slightly better counterfactual outcomes than in the sample: -0.026. The sum of these three terms yields the total bias of our <span class="math inline">\(WW\)</span> estimator in this second sample: -0.12.</p>
<p>Let’s now assess the overall effect of sampling noise on the estimate of the sample treatment effect for various sample sizes. In order to do this, I am going to use parallelized Monte Carlo simulations again. For the sake of simplicity, I am going to generate the same potential outcomes in each replication, using the same seed, and only choose a different treatment allocation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.ww.sample &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  <span class="kw">set.seed</span>(s)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
  Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  <span class="kw">return</span>((<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>Ds)<span class="op">-</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">-</span>Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)))
}

simuls.ww.N.sample &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">return</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.ww.sample,<span class="dt">N=</span>N,<span class="dt">param=</span>param)))
}

sf.simuls.ww.N.sample &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span>ncpus)
  sim &lt;-<span class="st"> </span><span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.ww.sample,<span class="dt">N=</span>N,<span class="dt">param=</span>param)
  <span class="kw">sfStop</span>()
  <span class="kw">return</span>(<span class="kw">unlist</span>(sim))
}

simuls.ww.sample &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.ww.N.sample,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)</code></pre></div>
<pre><code>## R Version:  R version 3.4.3 (2017-11-30)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.ate.sample &lt;-<span class="st"> </span><span class="cf">function</span>(N,s,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
  Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  <span class="kw">return</span>(<span class="kw">mean</span>(alpha))
}

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  <span class="kw">hist</span>(simuls.ww.sample[[i]],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(Delta<span class="op">^</span>yWW)),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.55</span>))
  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">monte.carlo.ate.sample</span>(N.sample[[i]],<span class="dv">1234</span>,param),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:montecarlosample"></span>
<img src="STCI_files/figure-html/montecarlosample-1.png" alt="Distribution of the $WW$ estimator over replications of treatment allocation for samples of different sizes" width="60%" />
<p class="caption">
Figure 2.3: Distribution of the <span class="math inline">\(WW\)</span> estimator over replications of treatment allocation for samples of different sizes
</p>
</div>
<p>Let’s also compute sampling noise, precision and the signal to noise ratio in these examples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp.noise.sample &lt;-<span class="st"> </span><span class="cf">function</span>(i,delta,param){
  <span class="kw">return</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(<span class="kw">monte.carlo.ate.sample</span>(<span class="dv">1234</span>,N.sample[[i]],param)<span class="op">-</span>simuls.ww.sample[[i]]),<span class="dt">prob=</span>delta))
}
samp.noise.ww.sample &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,samp.noise.sample,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)
<span class="kw">names</span>(samp.noise.ww.sample) &lt;-<span class="st"> </span>N.sample

precision.sample &lt;-<span class="st"> </span><span class="cf">function</span>(i,delta,param){
  <span class="kw">return</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">samp.noise.sample</span>(i,delta,<span class="dt">param=</span>param))
}
signal.to.noise.sample &lt;-<span class="st"> </span><span class="cf">function</span>(i,delta,param){
  <span class="kw">return</span>(<span class="kw">monte.carlo.ate.sample</span>(<span class="dv">1234</span>,N.sample[[i]],param)<span class="op">/</span><span class="kw">samp.noise.sample</span>(i,delta,<span class="dt">param=</span>param))
}
precision.ww.sample &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,precision.sample,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)
<span class="kw">names</span>(precision.ww.sample) &lt;-<span class="st"> </span>N.sample
signal.to.noise.ww.sample &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,signal.to.noise.sample,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)
<span class="kw">names</span>(signal.to.noise.ww.sample) &lt;-<span class="st"> </span>N.sample
table.noise.sample &lt;-<span class="st"> </span><span class="kw">cbind</span>(samp.noise.ww.sample,precision.ww.sample,signal.to.noise.ww.sample)
<span class="kw">colnames</span>(table.noise.sample) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Sampling noise&#39;</span>, <span class="st">&#39;Precision&#39;</span>, <span class="st">&#39;Signal to noise ratio&#39;</span>)
knitr<span class="op">::</span><span class="kw">kable</span>(table.noise.sample,<span class="dt">caption=</span><span class="kw">paste</span>(<span class="st">&#39;Sampling noise of $</span><span class="ch">\\</span><span class="st">hat{WW}$ for the sample treatment effect with $</span><span class="ch">\\</span><span class="st">delta=$&#39;</span>,delta,<span class="st">&#39;and for various sample sizes&#39;</span>,<span class="dt">sep=</span><span class="st">&#39; &#39;</span>),<span class="dt">booktabs=</span><span class="ot">TRUE</span>,<span class="dt">align=</span><span class="kw">c</span>(<span class="st">&#39;c&#39;</span>,<span class="st">&#39;c&#39;</span>,<span class="st">&#39;c&#39;</span>),<span class="dt">digits=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>))</code></pre></div>
<table>
<caption><span id="tab:sampnoisesample">Table 2.2: </span>Sampling noise of <span class="math inline">\(\hat{WW}\)</span> for the sample treatment effect with <span class="math inline">\(\delta=\)</span> 0.99 and for various sample sizes</caption>
<thead>
<tr class="header">
<th></th>
<th align="center">Sampling noise</th>
<th align="center">Precision</th>
<th align="center">Signal to noise ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100</td>
<td align="center">1.208</td>
<td align="center">0.828</td>
<td align="center">0.149</td>
</tr>
<tr class="even">
<td>1000</td>
<td align="center">0.366</td>
<td align="center">2.729</td>
<td align="center">0.482</td>
</tr>
<tr class="odd">
<td>10000</td>
<td align="center">0.122</td>
<td align="center">8.218</td>
<td align="center">1.585</td>
</tr>
<tr class="even">
<td>1e+05</td>
<td align="center">0.033</td>
<td align="center">30.283</td>
<td align="center">5.453</td>
</tr>
</tbody>
</table>
<p>Finally, let’s compare the extent of sampling noise for the population and the sample treatment effect parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colnames</span>(table.noise.sample) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;sampling.noise&#39;</span>, <span class="st">&#39;precision&#39;</span>, <span class="st">&#39;signal.to.noise&#39;</span>)
table.noise.sample &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(table.noise.sample)
table.noise.sample<span class="op">$</span>N &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">rownames</span>(table.noise.sample))
table.noise.sample<span class="op">$</span>TT &lt;-<span class="st"> </span><span class="kw">sapply</span>(N.sample,monte.carlo.ate.sample,<span class="dt">s=</span><span class="dv">1234</span>,<span class="dt">param=</span>param)
table.noise.sample<span class="op">$</span>Type &lt;-<span class="st"> &#39;TTs&#39;</span>
table.noise<span class="op">$</span>Type &lt;-<span class="st"> &#39;TT&#39;</span>
table.noise.tot &lt;-<span class="st"> </span><span class="kw">rbind</span>(table.noise,table.noise.sample)
table.noise.tot<span class="op">$</span>Type &lt;-<span class="st"> </span><span class="kw">factor</span>(table.noise.tot<span class="op">$</span>Type)

<span class="kw">ggplot</span>(table.noise.tot, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>TT,<span class="dt">fill=</span>Type)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="kw">c</span>(<span class="fl">0.85</span>,<span class="fl">0.88</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:precisionpopsample"></span>
<img src="STCI_files/figure-html/precisionpopsample-1.png" alt="Sampling noise of $\hat{WW}$ (99\% confidence) around $TT$ and $TT_s$ for various sample sizes" width="60%" />
<p class="caption">
Figure 2.4: Sampling noise of <span class="math inline">\(\hat{WW}\)</span> (99% confidence) around <span class="math inline">\(TT\)</span> and <span class="math inline">\(TT_s\)</span> for various sample sizes
</p>
</div>
<p>Figure <a href="FPSI.html#fig:montecarlosample">2.3</a> and Table <a href="FPSI.html#tab:sampnoisesample">2.2</a> present the results of the simulations of sampling noise for the sample treatment effect parameter. Figure <a href="FPSI.html#fig:precisionpopsample">2.4</a> compares sampling noise for the population and sample treatment effects.<br />
For all practical purposes, the estimates of sampling noise for the sample treatment effect are extremely close to the ones we have estimated for the population treatment effect. I am actually surprised by this result, since I expected that keeping the potential outcomes constant over replications would decrease sampling noise. It seems that the variability in potential outcomes over replications of random allocations of the treatment in a given sample mimicks very well the sampling process from a population. I do not know if this result of similarity of sampling noise for the population and sample treatment effect is a general one, but considering them as similar or close seems innocuous in our example.</p>
</div>
<div id="sec:confinterv" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Building confidence intervals from estimates of sampling noise</h3>
<p>In real life, we do not observe <span class="math inline">\(TT\)</span>. We only have access to <span class="math inline">\(\hat{E}\)</span>. Let’s also assume for now that we have access to an estimate of sampling noise, <span class="math inline">\(2\epsilon\)</span>. How can we use these two quantities to assess the set of values that <span class="math inline">\(TT\)</span> might take? One very useful device that we can use is the confidence interval. Confidence intervals are very useful because they quantify the zone within which we have a chance to find the true effect <span class="math inline">\(TT\)</span>:</p>

<div class="theorem">
<span id="thm:confinter" class="theorem"><strong>Theorem 2.1  (Confidence interval)  </strong></span>For a given level of confidence <span class="math inline">\(\delta\)</span> and corresponding level of sampling noise <span class="math inline">\(2\epsilon\)</span> of the estimator <span class="math inline">\(\hat{E}\)</span> of <span class="math inline">\(TT\)</span>, the confidence interval <span class="math inline">\(\left\{\hat{E}-\epsilon,\hat{E}+\epsilon\right\}\)</span> is such that the probability that it contains <span class="math inline">\(TT\)</span> is equal to <span class="math inline">\(\delta\)</span> over sample replications:
<span class="math display">\[\begin{align*}
  \Pr(\hat{E}-\epsilon\leq TT\leq\hat{E}+\epsilon) &amp; = \delta.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> From the definition of sampling noise, we know that:
<span class="math display">\[\begin{align*}
  \Pr(|\hat{E}-TT|\leq\epsilon) &amp; = \delta.
\end{align*}\]</span>
Now:
<span class="math display">\[\begin{align*}
  \Pr(|\hat{E}-TT|\leq\epsilon) &amp; = \Pr(TT-\epsilon\leq\hat{E}\leq TT+\epsilon)\\
                                &amp; = \Pr(-\hat{E}-\epsilon\leq-TT\leq -\hat{E}+\epsilon)\\
                                &amp; = \Pr(\hat{E}-\epsilon\leq TT\leq\hat{E}+\epsilon),
\end{align*}\]</span>
which proves the result.
</div>

<p>It is very important to note that confidence intervals are centered around <span class="math inline">\(\hat{E}\)</span> and not around <span class="math inline">\(TT\)</span>. When estimating sampling noise and building Figure <a href="FPSI.html#fig:precision">2.2</a>, we have centered our intervals around <span class="math inline">\(TT\)</span>. The interval was fixed and <span class="math inline">\(\hat{E}\)</span> was moving across replications and <span class="math inline">\(2\epsilon\)</span> was defined as the length of the interval around <span class="math inline">\(TT\)</span> containing a proportion <span class="math inline">\(\delta\)</span> of the estimates <span class="math inline">\(\hat{E}\)</span>. A confidence interval cannot be centered around <span class="math inline">\(TT\)</span>, which is unknown, but is centered around <span class="math inline">\(\hat{E}\)</span>, that we can observe. As a consequence, it is the interval that moves around across replications, and <span class="math inline">\(\delta\)</span> is the proportion of samples in which the interval contains <span class="math inline">\(TT\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-36" class="example"><strong>Example 2.3  </strong></span>Let’s see how confidence intervals behave in our numerical example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test &lt;-<span class="st"> </span><span class="kw">sample</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],N.plot)
  test &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta)),<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta.<span class="dv">2</span>))))
  <span class="kw">colnames</span>(test) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;sampling.noise.1&#39;</span>,<span class="st">&#39;sampling.noise.2&#39;</span>)
  test<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
  plot.test &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise.<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise.<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise.<span class="dv">2</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise.<span class="dv">2</span><span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="co">#ylim(-0.5,1.2)+</span>
<span class="st">      </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
  plot.list[[k]] &lt;-<span class="st"> </span>plot.test 
}
plot.CI &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confinterval"></span>
<img src="STCI_files/figure-html/confinterval-1.png" alt="Confidence intervals of $\hat{WW}$ for $\delta=$ 0.99 (red) and 0.95 (blue) over sample replications for various sample sizes" width="60%" />
<p class="caption">
Figure 2.5: Confidence intervals of <span class="math inline">\(\hat{WW}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 (red) and 0.95 (blue) over sample replications for various sample sizes
</p>
</div>
<p>Figure <a href="FPSI.html#fig:confinterval">2.5</a> presents the 99% and 95% confidence intervals for 40 samples selected from our simulations. First, confidence intervals do their job: they contain the true effect most of the time. Second, the 95% confidence interval misses the true effect more often, as expected. For example, with <span class="math inline">\(N=\)</span> 1000, the confidence intervals in samples 13 and 23 do not contain the true effect, but it is not far from their lower bound. Third, confidence intervals faithfully reflect what we can learn from our estimates at each sample size. With <span class="math inline">\(N=\)</span> 100, the confidence intervals make it clear that the effect might be very large or very small, even strongly negative. With <span class="math inline">\(N=\)</span> 1000, the confidence intervals suggest that the effect is either positive or null, but unlikely to be strongly negative. Most of the time, we get the sign right. With <span class="math inline">\(N=\)</span> 10^{4}, we know that the true effect is bigger than 0.1 and smaller than 0.3 and most intervals place the true effect somewhere between 0.11 and 0.25. With <span class="math inline">\(N=\)</span> 10^{5}, we know that the true effect is bigger than 0.15 and smaller than 0.21 and most intervals place the true effect somewhere between 0.16 and 0.20.</p>
</div>
<div id="reporting-sampling-noise-a-proposal" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Reporting sampling noise: a proposal</h3>
<p>Once sampling noise is measured (and we’ll see how to get an estimate in the next section), one still has to communicate it to others. There are many ways to report sampling noise:</p>
<ul>
<li>Sampling noise as defined in this book (<span class="math inline">\(2*\epsilon\)</span>)</li>
<li>The corresponding confidence interval</li>
<li>The signal to noise ratio</li>
<li>A standard error</li>
<li>A significance level</li>
<li>A p-value</li>
<li>A t-statistic</li>
</ul>
<p>The main problem with all of these approaches is that they do not express sampling noise in a way that is directly comparable to the magnitude of the <span class="math inline">\(TT\)</span> estimate. Other ways of reporting sampling noise such as p-values and t-stats are nonlinear transforms of sampling noise, making it difficult to really gauge the size of sampling noise as it relates to the magnitude of <span class="math inline">\(TT\)</span>.</p>
<p>My own preference goes to the following format for reporting results: <span class="math inline">\(TT \pm \epsilon\)</span>. As such, we can readily compare the size of the noise to the sizee of the <span class="math inline">\(TT\)</span> estimate. We can also form all the other ways of expressing sampling noise directly.</p>

<div class="example">
<span id="exm:unnamed-chunk-37" class="example"><strong>Example 2.4  </strong></span>Let’s see how this approach behaves in our numerical example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test.all &lt;-<span class="st"> </span><span class="kw">list</span>()
<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test &lt;-<span class="st"> </span><span class="kw">sample</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],N.plot)
  test &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta)),<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta.<span class="dv">2</span>))))
  <span class="kw">colnames</span>(test) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;sampling.noise.1&#39;</span>,<span class="st">&#39;sampling.noise.2&#39;</span>)
  test<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
  test.all[[k]] &lt;-<span class="st"> </span>test
}</code></pre></div>
<p>With <span class="math inline">\(N=\)</span> 100, the reporting of the results for sample 1 would be something like: “we find an effect of 0.07 <span class="math inline">\(\pm\)</span> 0.55.” Note how the choice of <span class="math inline">\(\delta\)</span> does not matter much for the result. The previous result was for <span class="math inline">\(\delta=0.99\)</span> while the result for <span class="math inline">\(\delta=0.95\)</span> would have been: “we find an effect of 0.07 <span class="math inline">\(\pm\)</span> 0.45.” The precise result changes with <span class="math inline">\(\delta\)</span>, but the qualitative result stays the same: the magnitude of sampling noise is large and it dwarfs the treatment effect estimate.</p>
<p>With <span class="math inline">\(N=\)</span> 1000, the reporting of the results for sample 1 with <span class="math inline">\(\delta=0.99\)</span> would be something like: “we find an effect of 0.19 <span class="math inline">\(\pm\)</span> 0.2.” With <span class="math inline">\(\delta=0.95\)</span>: “we find an effect of 0.19 <span class="math inline">\(\pm\)</span> 0.15.” Again, although the precise quantitative result is affected by the choice of <span class="math inline">\(\delta\)</span>, but hte qualitative message stays the same: sampling noise is of the same order of magnitude as the estimated treatment effect.</p>
<p>With <span class="math inline">\(N=\)</span> 10^{4}, the reporting of the results for sample 1 with <span class="math inline">\(\delta=0.99\)</span> would be something like: “we find an effect of 0.2 <span class="math inline">\(\pm\)</span> 0.06.” With <span class="math inline">\(\delta=0.95\)</span>: “we find an effect of 0.2 <span class="math inline">\(\pm\)</span> 0.04.” Again, see how the qualitative result is independent of the precise choice of <span class="math inline">\(\delta\)</span>: sampling noise is almost one order of magnitude smaller than the treatment effect estimate.</p>
<p>With <span class="math inline">\(N=\)</span> 10^{5}, the reporting of the results for sample 1 with <span class="math inline">\(\delta=0.99\)</span> would be something like: “we find an effect of 0.17 <span class="math inline">\(\pm\)</span> 0.02.” With <span class="math inline">\(\delta=0.95\)</span>: “we find an effect of 0.17 <span class="math inline">\(\pm\)</span> 0.01.” Again, see how the qualitative result is independent of the precise choice of <span class="math inline">\(\delta\)</span>: sampling noise is one order of magnitude smaller than the treatment effect estimate.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> What I hope the example makes clear is that my proposed way of reporting results gives the same importance to sampling noise as it gives to the treatment effect estimate. Also, comparing them is easy, without requiring a huge computational burden on our brain.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> One problem with the approach that I propose is when you have a non-symetric distribution of sampling noise, or when <span class="math inline">\(TT \pm \epsilon\)</span> exceeds natural bounds on <span class="math inline">\(TT\)</span> (such as if the effect cannot be bigger than one, for example). I think these issues are minor and rare and can be dealt with on a case by case basis. The advantage of having one simple and directly readable number comparable to the magnitude of the treatment effect is overwhelming and makes this approach the most natural and adequate, in my opinion.
</div>

</div>
<div id="sec:effectsize" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Using effect sizes to normalize the reporting of treatment effects and their precision</h3>
<p>When looking at the effect of a program on an outcome, we depend on the scaling on that outcome to appreciate the relative size of the estimated treatment effect.<br />
It is often difficult to appreciate the relative importance of the size of an effect, even if we know the scale of the outcome of interest. One useful device to normalize the treatment effects is called Cohen’s <span class="math inline">\(d\)</span>, or effect size. The idea is to compare the magnitude of the treatment effect to an estimate of the usual amount of variation that the outcome undergoes in the population. The way to build Cohen’s <span class="math inline">\(d\)</span> is by dividing the estimated treatment effect by the standard deviation of the outcome. I generally prefer to use the standard devaition of the outcome in the control group, so as not to include the additional amoiunt of variation due to the heterogeneity in treatment effects.</p>

<div class="definition">
<span id="def:unnamed-chunk-40" class="definition"><strong>Definition 2.2  (Cohen’s <span class="math inline">\(d\)</span>)  </strong></span>Cohen’s <span class="math inline">\(d\)</span>, or effect size, is the ratio of the estimated treatment effect to the standard deviation of outcomes in the control group:
</div>

<p><span class="math display">\[
d = \frac{\hat{TT}}{\sqrt{\frac{1}{N^0}\sum_{i=1}^{N^0}(Y_i-\bar{Y^0})^2}}
\]</span> where <span class="math inline">\(\hat{TT}\)</span> is an estimate of the treatment effect, <span class="math inline">\(N^0\)</span> is the number of individuals in the treatment group and <span class="math inline">\(\bar{Y^0}\)</span> is the average outcome in the treatment group.</p>
<p>Cohen’s <span class="math inline">\(d\)</span> can be interpreted in terms of magnitude of effect size:</p>
<ul>
<li>It is generally considered that an effect is large when its <span class="math inline">\(d\)</span> is larger than 0.8.</li>
<li>An effect size around 0.5 is considered medium</li>
<li>An effect size around 0.2 is considered to be small</li>
<li>An effect size around 0.02 is considered to be very small.</li>
</ul>
<p>There probably could be a rescaling of these terms, but that is the actual state of the art.</p>
<p>What I like about effect sizes is that they encourage an interpretation of the order of magnitude of the treatment effect. As such, they enable to include the information on precision by looking at which orders of magnitude are compatible with the estimated effect at the estimated precision level. Effect sizes and orders of magnitude help make us aware that our results might be imprecise, and that the precise value that we have estimated is probably not the truth. What is important is the range of effect sizes compatible with our results (both point estimate and precision).</p>

<div class="example">
<span id="exm:unnamed-chunk-41" class="example"><strong>Example 2.5  </strong></span>Let’s see how Cohen’s <span class="math inline">\(d\)</span> behaves in our numerical example.
</div>

<p>The value of Cohen’s <span class="math inline">\(d\)</span> (or effect size) in the population is equal to:</p>
<span class="math display">\[\begin{align*}
  ES &amp; = \frac{TT}{\sqrt{V^0}} = \frac{\bar{\alpha}+\theta\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\rho^2\sigma^2_{U}+\sigma^2_{\epsilon}}}
\end{align*}\]</span>
<p>We can write a function to compute this parameter, as well as functions to implement its estimator in the simulated samples:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">V0 &lt;-<span class="st"> </span><span class="cf">function</span>(param){
  <span class="kw">return</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">^</span><span class="dv">2</span><span class="op">*</span>param[<span class="st">&quot;sigma2U&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2epsilon&quot;</span>])
}

ES &lt;-<span class="st"> </span><span class="cf">function</span>(param){
  <span class="kw">return</span>(<span class="kw">delta.y.ate</span>(param)<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">V0</span>(param)))
}

samp.noise.ES &lt;-<span class="st"> </span><span class="cf">function</span>(estim,delta,<span class="dt">param=</span>param){
  <span class="kw">return</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(<span class="kw">delta.y.ate</span>(param)<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">V0</span>(param))<span class="op">-</span>estim),<span class="dt">prob=</span>delta))
}

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  simuls.ww[[i]][,<span class="st">&#39;ES&#39;</span>] &lt;-<span class="st"> </span>simuls.ww[[i]][,<span class="st">&#39;WW&#39;</span>]<span class="op">/</span><span class="kw">sqrt</span>(simuls.ww[[i]][,<span class="st">&#39;V0&#39;</span>])
}</code></pre></div>
<p>The true effect size in the population is thus 0.2. It is considered to be small according to the current classification, although I’d say that a treatment able to move the outcomes by 20% of their usual variation is a pretty effective treatment, and this effect should be labelled at least medium. Let’s stick with the classification though. In our example, the effect size does not differ much from the treatment effect since the standard deviation of outcomes in the control group is pretty close to one: it is equal to 0.88. Let’s now build confidence intervals for the effect size and try to comment on the magnitudes of these effects using the normalized classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot.ES &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list.ES &lt;-<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test.ES &lt;-<span class="st"> </span><span class="kw">sample</span>(simuls.ww[[k]][,<span class="st">&#39;ES&#39;</span>],N.plot)
  test.ES &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.ES,<span class="kw">rep</span>(<span class="kw">samp.noise.ES</span>(simuls.ww[[k]][,<span class="st">&#39;ES&#39;</span>],<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)),<span class="kw">rep</span>(<span class="kw">samp.noise.ES</span>(simuls.ww[[k]][,<span class="st">&#39;ES&#39;</span>],<span class="dt">delta=</span>delta.<span class="dv">2</span>,<span class="dt">param=</span>param))))
  <span class="kw">colnames</span>(test.ES) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;ES&#39;</span>,<span class="st">&#39;sampling.noise.ES.1&#39;</span>,<span class="st">&#39;sampling.noise.ES.2&#39;</span>)
  test.ES<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot.ES
  plot.test.ES &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.ES, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>ES)) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>ES<span class="op">-</span>sampling.noise.ES.<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>ES<span class="op">+</span>sampling.noise.ES.<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>ES<span class="op">-</span>sampling.noise.ES.<span class="dv">2</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>ES<span class="op">+</span>sampling.noise.ES.<span class="dv">2</span><span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">ES</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="co">#ylim(-0.5,1.2)+</span>
<span class="st">      </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">ylab</span>(<span class="st">&quot;Effect Size&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
  plot.list.ES[[k]] &lt;-<span class="st"> </span>plot.test.ES 
}
plot.CI.ES &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list.ES[[<span class="dv">1</span>]],plot.list.ES[[<span class="dv">2</span>]],plot.list.ES[[<span class="dv">3</span>]],plot.list.ES[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI.ES)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confintervalES"></span>
<img src="STCI_files/figure-html/confintervalES-1.png" alt="Confidence intervals of $\hat{ES}$ for $\delta=$ 0.99 (red) and 0.95 (blue) over sample replications for various sample sizes" width="60%" />
<p class="caption">
Figure 2.6: Confidence intervals of <span class="math inline">\(\hat{ES}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 (red) and 0.95 (blue) over sample replications for various sample sizes
</p>
</div>
<p>Figure <a href="FPSI.html#fig:confintervalES">2.6</a> presents the 99% and 95% confidence intervals for the effect size estimated in 40 samples selected from our simulations. Let’s regroup our estimate and see how we could present their results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test.all.ES &lt;-<span class="st"> </span><span class="kw">list</span>()
<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test.ES &lt;-<span class="st"> </span><span class="kw">sample</span>(simuls.ww[[k]][,<span class="st">&#39;ES&#39;</span>],N.plot)
  test.ES &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.ES,<span class="kw">rep</span>(<span class="kw">samp.noise.ES</span>(simuls.ww[[k]][,<span class="st">&#39;ES&#39;</span>],<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)),<span class="kw">rep</span>(<span class="kw">samp.noise.ES</span>(simuls.ww[[k]][,<span class="st">&#39;ES&#39;</span>],<span class="dt">delta=</span>delta.<span class="dv">2</span>,<span class="dt">param=</span>param))))
  <span class="kw">colnames</span>(test.ES) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;ES&#39;</span>,<span class="st">&#39;sampling.noise.ES.1&#39;</span>,<span class="st">&#39;sampling.noise.ES.2&#39;</span>)
  test.ES<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot.ES
  test.all.ES[[k]] &lt;-<span class="st"> </span>test.ES
}</code></pre></div>
<p>With <span class="math inline">\(N=\)</span> 100, the reporting of the results for sample 1 would be something like: “we find an effect size of 0.09 <span class="math inline">\(\pm\)</span> 0.66” with <span class="math inline">\(\delta=0,99\)</span>. With <span class="math inline">\(\delta=0.95\)</span> we would say: “we find an effect of 0.09 <span class="math inline">\(\pm\)</span> 0.5.” All in all, our estimate is compatible with the treatment having a large positive effect size and a medium negative effect size. Low precision prevents us from saying much else.</p>
<p>With <span class="math inline">\(N=\)</span> 1000, the reporting of the results for sample 1 with <span class="math inline">\(\delta=0.99\)</span> would be something like: “we find an effect size of 0.21 <span class="math inline">\(\pm\)</span> 0.22.” With <span class="math inline">\(\delta=0.95\)</span>: “we find an effect size of 0.21 <span class="math inline">\(\pm\)</span> 0.17.” Our estimate is compatible with a medium positive effect or a very small positive or even negative effect (depending on the choice of <span class="math inline">\(\delta\)</span>).</p>
<p>With <span class="math inline">\(N=\)</span> 10^{4}, the reporting of the results for sample 1 with <span class="math inline">\(\delta=0.99\)</span> would be something like: “we find an effect size of 0.22 <span class="math inline">\(\pm\)</span> 0.07.” With <span class="math inline">\(\delta=0.95\)</span>: “we find an effect size of 0.22 <span class="math inline">\(\pm\)</span> 0.05.” Our estimate is thus compatible with a small effect of the treatment. We can rule out that the effect of the treatment is medium since the upper bound of the 99% confidence interval is equal to 0.29. We can also rule out that the effect of the treatment is very small since the lower bound of the 99% confidence interval is equal to 0.16. With this sample size, we have been able to reach a precision level sufficient enough to pin down the order of magnitude of the effect size of our treatment. There still remains a considerable amount of uncertainty about the true effect size, though: the upper bound of our confidence interval is almost double the lower bound.</p>
<p>With <span class="math inline">\(N=\)</span> 10^{5}, the reporting of the results for sample 1 with <span class="math inline">\(\delta=0.99\)</span> would be something like: “we find an effect size of 0.2 <span class="math inline">\(\pm\)</span> 0.02.” With <span class="math inline">\(\delta=0.95\)</span>: “we find an effect size of 0.2 <span class="math inline">\(\pm\)</span> 0.02.” Here, the level of precision of our result is such that, first, it does not depend on the choice of <span class="math inline">\(\delta\)</span> in any meaningful way, and second, we can do more than pinpoint the order of magnitude of the effect size, we can start to zero in on its precise value. From our estimate, the true value of the effect size is really close to 0.2. It could be equal to 0.18 or 0.22, but not further away from 0.2 than that. Remember that is actually equal to 0.2.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> One issue with Cohen’s <span class="math inline">\(d\)</span> is that its magnitude depends on the dispersion of the outcomes in the control group. That means that for the same treatment, and same value of the treatment effect, the effect size is larger in a population where oucomes are more homogeneous. This is not an attractive feature of a normalizing scale that its size depends on the particular application. One solution would be, for each outcome, to provide a standardized scale, using for example the estmated standard deviation in a reference population. This would be similar to the invention of the metric system, where a reference scale was agreed uppon once and for all.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Cohen’s <span class="math inline">\(d\)</span> is well defined for continuous outcomes. For discrete outcomes, the use of Cohen’s <span class="math inline">\(d\)</span> poses a series of problems, and alternatives such as relative risk ratios and odds ratios have been proposed. I’ll comment on that in the last chapter.
</div>

</div>
</div>
<div id="sec:estimsampnoise" class="section level2">
<h2><span class="header-section-number">2.2</span> Estimating sampling noise</h2>
<p>Gauging the extent sampling noise is very useful in order to be able to determine how much we should trust our results. Are they precise, so that the true treatment effect lies very close to our estimate? Or are our results imprecise, the true treatment effect maybe lying very far from our estimate?</p>
<p>Estimating sampling noise is hard because we want to infer a property of our estimator over repeated samples using only one sample. In this lecture, I am going to introduce four tools that enable you to gauge sampling noise and to choose sample size. The four tools are Chebyshev’s inequality, the Central Limit Theorem, resampling methods and Fisher’s permutation method. The idea of all these methods is to use the properties of the sample to infer the properties of our estimator over replications. Chebyshev’s inequality gives an upper bound on the sampling noise and a lower bound on sample size, but these bounds are generally too wide to be useful. The Central Limit Theorem (CLT) approximates the distribution of <span class="math inline">\(\hat{E}\)</span> by a normal distribution, and quantifies sampling noise as a multiple of the standard deviation. Resampling methods use the sample as a population and draw new samples from it in order to approximate sampling noise. Fisher’s permutation method, also called randomization inference, derives the distribution of <span class="math inline">\(\hat{E}\)</span> under the assumption that all treatment effects are null, by reallocating the treatment indicator among the treatment and control group. Both the CLT and resampling methods are approximation methods, and their approximation of the true extent of sampling noise gets better and better as sample size increases. Fisher’s permutation method is exact-it is not an approximation-but it only works for the special case of the <span class="math inline">\(WW\)</span> estimator in a randomized design.</p>
<p>The remaining of this section is structured as follows. Section <a href="FPSI.html#sec:assumptions">2.2.1</a> introduces the assumptions that we will need in order to implement the methods. Section <a href="FPSI.html#sec:cheb">2.2.2</a> presents the Chebyshev approach to gauging sampling noise and choosing sample size. Section <a href="FPSI.html#sec:CLT">2.2.3</a> introduces the CLT way of approximating sampling noise and choosing sample size. Section <a href="FPSI.html#sec:resamp">2.2.4</a> presents the resampling methods.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> I am going to derive the estimators for the precision only for the <span class="math inline">\(WW\)</span> estimator. In the following lectures, I will show how these methods adapt to other estimators.
</div>

<div id="sec:assumptions" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Assumptions</h3>
<p>In order to be able to use the theorems that power up the methods that we are going to use to gauge sampling noise, we need to make some assumptions on the properties of the data. The main assumptions that we need are that the estimator identifies the true effect of the treatment in the population, that the estimator is well-defined in the sample, that the observations in the sample are independently and identically distributed (i.i.d.), that there is no interaction between units and that the variances of the outcomes in the treated and untreated group are finite.</p>
<p>We know from last lecture that for the <span class="math inline">\(WW\)</span> estimator to identify <span class="math inline">\(TT\)</span>, we need to assume that there is no selection bias, as stated in Assumption <a href="FPCI.html#def:noselb">1.7</a>. One way to ensure that this assumption holds is to use a RCT.</p>
<p>In order to be able to form the <span class="math inline">\(WW\)</span> estimator in the sample, we also need that there is at least one treated and one untreated in the sample:</p>

<div class="definition">
<span id="def:fullrank" class="definition"><strong>Definition 2.3  (Full rank)  </strong></span>We assume that there is at least one observation in the sample that receives the treatment and one observation that does not receive it:
<span class="math display">\[\begin{align*}
\exists i,j\leq N \text{ such that } &amp; D_i=1 \&amp; D_j=0.
\end{align*}\]</span>
</div>

<p>One way to ensure that this assumption holds is to sample treated and untreated units.</p>
<p>In order to be able to estimate the variance of the estimator easily, we assume that the observations come from random sampling and are i.i.d.:</p>

<div class="definition">
<span id="def:iid" class="definition"><strong>Definition 2.4  (i.i.d. sampling)  </strong></span>We assume that the observations in the sample are identically and independently distributed:
<span class="math display">\[\begin{align*}
\forall i,j\leq N\text{, }i\neq j\text{, } &amp; (Y_i,D_i)\Ind(Y_j,D_j),\\
                                           &amp; (Y_i,D_i)\&amp;(Y_j,D_j)\sim F_{Y,D}.
\end{align*}\]</span>
</div>

<p>We have to assume something on how the observations are related to each other and to the population. Identical sampling is natural in the sense that we are OK to assume that the observations stem from the same population model. Independent sampling is something else altogether. Independence means that the fates of two closely related individuals are assumed to be independent. This rules out two empirically relevant scenarios:</p>
<ol style="list-style-type: decimal">
<li>The fates of individuals are related because of common influences, as for example the environment, etc,</li>
<li>The fates of individuals are related because they directly influence each other, as for example on a market, but also for example because there are diffusion effects, such as contagion of deseases or technological adoption by imitation.</li>
</ol>
<p>We will address both sources of failure of the independence assumption in future lectures.</p>
<p>Finally, in order for all our derivations to make sense, we need to assume that the outcomes in both groups have finite variances, otherwise sampling noise is going to be too extreme to be able to estimate it using the methods developed in this lecture:</p>

<div class="definition">
<span id="def:finitevar" class="definition"><strong>Definition 2.5  (Finite variance of <span class="math inline">\(\hat{\Delta^Y_{WW}}\)</span>)  </strong></span>We assume that <span class="math inline">\(\var{Y^1|D_i=1}\)</span> and <span class="math inline">\(\var{Y^0|D_i=0}\)</span> are finite.
</div>

</div>
<div id="sec:cheb" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Using Chebyshev’s inequality</h3>
<p>Chebyshev’s inequality is a fundamental building block of statistics. It relates the sampling noise of an estimator to its variance. More precisely, it derives an upper bound on the samplig noise of an unbiased estimator:</p>

<div class="theorem">
<p><span id="thm:cheb" class="theorem"><strong>Theorem 2.2  (Chebyshev’s inequality)  </strong></span>For any unbiased estimator <span class="math inline">\(\hat{\theta}\)</span>, sampling noise level <span class="math inline">\(2\epsilon\)</span> and confidence level <span class="math inline">\(\delta\)</span>, sampling noise is bounded from above:</p>
<span class="math display">\[\begin{align*}
2\epsilon \leq 2\sqrt{\frac{\var{\hat{\theta}}}{1-\delta}}.
\end{align*}\]</span>
</div>


<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> The more general version of Chebyshev’s inequality that is generally presented is as follows:</p>
<span class="math display">\[\begin{align*}
\Pr(|\hat{\theta}-\esp{\hat{\theta}}|&gt;\epsilon) &amp; \leq \frac{\var{\hat{\theta}}}{\epsilon^2}.
\end{align*}\]</span>
<p>The version I present in Theorem <a href="FPSI.html#thm:cheb">2.2</a> is adapted to the bouding of sampling noise for a given confidence level, while this version is adapted to bounding the confidence level for a given level of sampling noise. In order to go from this general version to Theorem <a href="FPSI.html#thm:cheb">2.2</a>, simply remember that, for an unbiased estimator, <span class="math inline">\(\esp{\hat{\theta}}=\theta\)</span> and that, by definition of sampling noise, <span class="math inline">\(\Pr(|\hat{\theta}-\theta|&gt;\epsilon)=1-\delta\)</span>. As a result, <span class="math inline">\(1-\delta\leq\var{\hat{\theta}}/\epsilon^2\)</span>, hence the result in Theorem <a href="FPSI.html#thm:cheb">2.2</a>.</p>
</div>

<p>Using Chebyshev’s inequality, we can obtain an upper bound on the sampling noise of the <span class="math inline">\(WW\)</span> estimator:</p>

<div class="theorem">
<p><span id="thm:uppsampnoise" class="theorem"><strong>Theorem 2.3  (Upper bound on the sampling noise of <span class="math inline">\(\hat{WW}\)</span>)  </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#def:fullrank">2.3</a> and <a href="FPSI.html#def:iid">2.4</a>, for a given confidence level <span class="math inline">\(\delta\)</span>, the sampling noise of the <span class="math inline">\(\hat{WW}\)</span> estimator is bounded from above:</p>
<span class="math display">\[\begin{align*}
2\epsilon \leq 2\sqrt{\frac{1}{N(1-\delta)}\left(\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}\right)}\equiv 2\bar{\epsilon}.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> See in Appendix <a href="proofs.html#proofcheb">A.1.1</a>
</div>

<p>Theorem <a href="FPSI.html#thm:uppsampnoise">2.3</a> is a useful step forward for estimating sampling noise. Theorem <a href="FPSI.html#thm:uppsampnoise">2.3</a> states that the actual level of sampling noise of the <span class="math inline">\(\hat{WW}\)</span> estimator (<span class="math inline">\(2\epsilon\)</span>) is never bigger than a quantity that depends on sample size, confidence level and on the variances of outcomes in the treated and control groups. We either know all the components of the formula for <span class="math inline">\(2\bar{\epsilon}\)</span> or we can estimate them in the sample. For example, <span class="math inline">\(\Pr(D_i=1)\)</span>, <span class="math inline">\(\var{Y_i^1|D_i=1}\)</span> and <span class="math inline">\(\var{Y_i^0|D_i=0}\)</span> by can be approximated by, respectively:</p>
<span class="math display">\[\begin{align*}
  \hat{\Pr(D_i=1)} &amp; = \frac{1}{N}\sum_{i=1}^ND_i\\
  \hat{\var{Y_i^1|D_i=1}} &amp; = \frac{1}{\sum_{i=1}^ND_i}\sum_{i=1}^ND_i(Y_i-\frac{1}{\sum_{i=1}^ND_i}\sum_{i=1}^ND_iY_i)^2\\
  \hat{\var{Y_i^0|D_i=0}} &amp; = \frac{1}{\sum_{i=1}^N(1-D_i)}\sum_{i=1}^N(1-D_i)(Y_i-\frac{1}{\sum_{i=1}^N(1-D_i)}\sum_{i=1}^N(1-D_i)Y_i)^2.
\end{align*}\]</span>
<p>Using these approximations for the quantities in the formula, we can compute an estimate of the upper bound on sampling noise, <span class="math inline">\(\hat{2\bar{\epsilon}}\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-47" class="example"><strong>Example 2.6  </strong></span>Let’s write an R function that is going to compute an estimate for the upper bound of sampling noise for any sample:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp.noise.ww.cheb &lt;-<span class="st"> </span><span class="cf">function</span>(N,delta,v1,v0,p){
  <span class="kw">return</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">sqrt</span>((v1<span class="op">/</span>p<span class="op">+</span>v0<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p))<span class="op">/</span>(N<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>delta))))
}</code></pre></div>
<p>Let’s estimate this upper bound in our usual sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
N &lt;-<span class="dv">1000</span>
delta &lt;-<span class="st"> </span><span class="fl">0.99</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)</code></pre></div>
<p>In our sample, for <span class="math inline">\(\delta=\)</span> 0.99, <span class="math inline">\(\hat{2\bar{\epsilon}}=\)</span> 1.35. How does this compare with the true extent of sampling noise when <span class="math inline">\(N=\)</span> 1000? Remember that we have computed an estimate of sampling noise out of our Monte Carlo replications. In Table <a href="FPSI.html#fig:precision">2.2</a>, we can read that sampling noise is actually equal to 0.39. The Chebyshev upper bound overestimates the extent of sampling noise by 245%.</p>
<p>How does the Chebyshev upper bound fares overall? In order to know that, let’s compute the Chebyshev upper bound for all the simulated samples. You might have noticed that, when running the Monte Carlo simulations for the population parameter, I have not only recovered <span class="math inline">\(\hat{WW}\)</span> for each sample, but also the estimates of the components of the formula for the upper bound on sampling noise. I can thus easily compute the Chebyshev upper bound on sampling noise for each replication.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  simuls.ww[[k]]<span class="op">$</span>cheb.noise &lt;-<span class="st"> </span><span class="kw">samp.noise.ww.cheb</span>(N.sample[[k]],delta,simuls.ww[[k]][,<span class="st">&#39;V1&#39;</span>],simuls.ww[[k]][,<span class="st">&#39;V0&#39;</span>],simuls.ww[[k]][,<span class="st">&#39;p&#39;</span>])
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  <span class="kw">hist</span>(simuls.ww[[i]][,<span class="st">&#39;cheb.noise&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">bar</span>(epsilon))),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">0.25</span><span class="op">*</span><span class="kw">min</span>(simuls.ww[[i]][,<span class="st">&#39;cheb.noise&#39;</span>]),<span class="kw">max</span>(simuls.ww[[i]][,<span class="st">&#39;cheb.noise&#39;</span>])))
  <span class="kw">abline</span>(<span class="dt">v=</span>table.noise[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwcheball"></span>
<img src="STCI_files/figure-html/sampnoisewwcheball-1.png" alt="Distribution of the Chebyshev upper bound on sampling noise over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.7: Distribution of the Chebyshev upper bound on sampling noise over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<p>Figure <a href="FPSI.html#fig:sampnoisewwcheball">2.7</a> shows that the upper bound works: it is always bigger than the true sampling noise. Figure <a href="FPSI.html#fig:sampnoisewwcheball">2.7</a> also shows that the upper bound is large: it generally is of an order of magnitude bigger than the true sampling noise, and thus offers a blurry and too pessimistic view of the precision of an estimator. Figure <a href="FPSI.html#fig:sampnoisewwchebplot">2.8</a> shows that the average Chebyshev bound gives an inflated estimate of sampling noise. Figure <a href="FPSI.html#fig:confintervalcheb">2.9</a> shows that the Chebyshev confidence intervals are clearly less precise than the true unknown ones. With <span class="math inline">\(N=\)</span> 1000, the true confidence intervals generally reject large negative effects, whereas the Chebyshev confidence intervals do not rule out this possibility. With <span class="math inline">\(N=\)</span> 10^{4}, the true confidence intervals generally reject effects smaller than 0.1, whereas the Chebyshev confidence intervals cannot rule out small negative effects.</p>
<p>As a conclusion on Chebyshev estimates of sampling noise, their advantage is that they offer an upper bound on the noise: we can never underestimate noise if we use them. A downside of Chebyshev sampling noise estimates is their low precision, which makes it hard to pinpoint the true confidence intervals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  table.noise<span class="op">$</span>cheb.noise[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(simuls.ww[[k]]<span class="op">$</span>cheb.noise)
}
<span class="kw">ggplot</span>(table.noise, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>TT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>cheb.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>cheb.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwchebplot"></span>
<img src="STCI_files/figure-html/sampnoisewwchebplot-1.png" alt="Average Chebyshev upper bound on sampling noise over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.8: Average Chebyshev upper bound on sampling noise over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test.cheb &lt;-<span class="st"> </span>simuls.ww[[k]][<span class="kw">sample</span>(N.plot),<span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;cheb.noise&#39;</span>)]
  test.cheb &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.cheb,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta),N.plot)))
  <span class="kw">colnames</span>(test.cheb) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;cheb.noise&#39;</span>,<span class="st">&#39;sampling.noise&#39;</span>)
  test.cheb<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
  plot.test.cheb &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.cheb, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>cheb.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>cheb.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
  plot.list[[k]] &lt;-<span class="st"> </span>plot.test.cheb 
}
plot.CI &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confintervalcheb"></span>
<img src="STCI_files/figure-html/confintervalcheb-1.png" alt="Chebyshev confidence intervals of $\hat{WW}$ for $\delta=$ 0.99 over sample replications for various sample sizes (true confidence intervals in red)" width="60%" />
<p class="caption">
Figure 2.9: Chebyshev confidence intervals of <span class="math inline">\(\hat{WW}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 over sample replications for various sample sizes (true confidence intervals in red)
</p>
</div>
</div>
<div id="sec:CLT" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Using the Central Limit Theorem</h3>
<p>The main problem with Chebyshev’s upper bound on sampling noise is that it is an upper bound, and thus it overestimates sampling noise and underestimates precision. One alternative to using Chebyshev’s upper bound is to use the Central Limit Theorem (CLT). In econometrics and statistics, the CLT is used to derive approximate values for the sampling noise of estimators. Because these approximations become more and more precise as sample size increases, we call them asymptotic approximations.</p>
<p>Taken to its bare bones, the CLT states that the sum of i.i.d. random variables behaves approximately like a normal distribution when the sample size is large:</p>

<div class="theorem">
<p><span id="thm:CLT" class="theorem"><strong>Theorem 2.4  (Central Limit Theorem)  </strong></span>Let <span class="math inline">\(X_i\)</span> be i.i.d. random variables with <span class="math inline">\(\esp{X_i}=\mu\)</span> and <span class="math inline">\(\var{X_i}=\sigma^2\)</span>, and define <span class="math inline">\(Z_N=\frac{\frac{1}{N}\sum_{i=1}^NX_i-\mu}{\frac{\sigma}{\sqrt{N}}}\)</span>, then, for all <span class="math inline">\(z\)</span> we have:</p>
<span class="math display">\[\begin{align*}
\lim_{N\rightarrow\infty}\Pr(Z_N\leq z) &amp; = \Phi(z),
\end{align*}\]</span>
<p>where <span class="math inline">\(\Phi\)</span> is the cumulative distribution function of the centered standardized normal.</p>
We say that <span class="math inline">\(Z_N\)</span> converges in distribution to a standard normal random variable, and we denote: <span class="math inline">\(Z_N\stackrel{d}{\rightarrow}\mathcal{N}(0,1)\)</span>.
</div>

<p>The CLT is a beautiful result: the distribution of the average of realisations of any random variable that has finite mean and variance can be approximated by a normal when the sample size is large enough. The CLT is somehow limited though because not all estimators are sums. Estimators are generally more or less complex combinations of sums. In order to derive the asymptotic approximation for a lot of estimators that are combinations of sums, econometricians and statisticians complement the CLT with two other extremely powerful tools: Slutsky’s theorem and the Delta method. Slutsky’s theorem states that sums, products and ratios of sums that converge to a normal converge to the sum, product or ratio of these normals. The Delta method states that a function of a sum that converges to a normal converges to a normal whose variance is a quadratic form of the variance of the sum and of the first derivative of the function. Both of these tools are stated more rigorously in the appendix, but you do not need to know them for this class. The idea is for you to be aware of how the main approximations that we are going to use throughout this class have been derived.</p>
<p>Let me now state the main result of this section:</p>

<div class="theorem">
<p><span id="thm:asympnoiseWW" class="theorem"><strong>Theorem 2.5  (Asymptotic Estimate of Sampling Noise of WW)  </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#def:fullrank">2.3</a>, <a href="FPSI.html#def:iid">2.4</a> and <a href="FPSI.html#def:finitevar">2.5</a>, for a given confidence level <span class="math inline">\(\delta\)</span> and sample size <span class="math inline">\(N\)</span>, the sampling noise of <span class="math inline">\(\hat{WW}\)</span> can be approximated as follows:</p>
<span class="math display">\[\begin{align*}
2\epsilon &amp; \approx 2\Phi^{-1}\left(\frac{\delta+1}{2}\right)\frac{1}{\sqrt{N}}\sqrt{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}} \equiv 2\tilde{\epsilon}.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> See in Appendix <a href="proofs.html#proofCLT">A.1.2</a>.
</div>

<p>Let’s write an R function that computes this formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp.noise.ww.CLT &lt;-<span class="st"> </span><span class="cf">function</span>(N,delta,v1,v0,p){
  <span class="kw">return</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>((v1<span class="op">/</span>p<span class="op">+</span>v0<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p))<span class="op">/</span>N))
}</code></pre></div>

<div class="example">
<span id="exm:unnamed-chunk-49" class="example"><strong>Example 2.7  </strong></span>Let’s see how the CLT performs in our example.
</div>
<p> In our sample, for <span class="math inline">\(\delta=\)</span> 0.99, the CLT estimate of sampling noise is <span class="math inline">\(\hat{2\tilde{\epsilon}}=\)</span> 0.35. How does this compare with the true extent of sampling noise when <span class="math inline">\(N=\)</span> 1000? Remember that we have computed an estimate of sampling noise out of our Monte Carlo replications. In Table <a href="FPSI.html#tab:precisionsignal">2.1</a>, we can read that sampling noise is actually equal to 0.39. The CLT approximation is pretty precise: it only underestimates the true extent of sampling noise by 11%.</p>
<p>We can also compute the CLT approximation to sampling noise in all of our samples:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  simuls.ww[[k]]<span class="op">$</span>CLT.noise &lt;-<span class="st"> </span><span class="kw">samp.noise.ww.CLT</span>(N.sample[[k]],delta,simuls.ww[[k]][,<span class="st">&#39;V1&#39;</span>],simuls.ww[[k]][,<span class="st">&#39;V0&#39;</span>],simuls.ww[[k]][,<span class="st">&#39;p&#39;</span>])
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  <span class="kw">hist</span>(simuls.ww[[i]][,<span class="st">&#39;CLT.noise&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">bar</span>(epsilon))),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="kw">min</span>(simuls.ww[[i]][,<span class="st">&#39;CLT.noise&#39;</span>]),<span class="kw">max</span>(simuls.ww[[i]][,<span class="st">&#39;CLT.noise&#39;</span>])))
  <span class="kw">abline</span>(<span class="dt">v=</span>table.noise[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwCLTall"></span>
<img src="STCI_files/figure-html/sampnoisewwCLTall-1.png" alt="Distribution of the CLT approximation of sampling noise over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.10: Distribution of the CLT approximation of sampling noise over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<p>Figure <a href="FPSI.html#fig:sampnoisewwCLTall">2.10</a> shows that the CLT works: CLT-based estimates of sampling noise approximates true sampling noise well. CLT-based approximations of sampling noise are even impressively accurate: they always capture the exact order of magnitude of sampling noise, although there is a slight underestimation when <span class="math inline">\(N=\)</span> 1000 and 10^{4} and a slight overestimation when <span class="math inline">\(N=\)</span> 10^{5}. This success should not come as a surprise as all shocks in our model are normally distributed, meaning that the CLT results are more than an approximation, they are exact. Results might be less spectacular when estimating the effect of the treatment on the outcomes in levels rather than in logs.</p>
<p>As a consequence, the average CLT-based estimates of sampling noise and of confidence intervals are pretty precise, as Figures <a href="FPSI.html#fig:sampnoisewwCLTplot">2.11</a> and <a href="FPSI.html#fig:confintervalCLT">2.12</a> show. Let’s pause for a second at the beauty of what we have achieved using the CLT: by using only information from one sample, we have been able to gauge extremely precisely how the estimator would behave over sampling repetitions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  table.noise<span class="op">$</span>CLT.noise[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(simuls.ww[[k]]<span class="op">$</span>CLT.noise)
}
<span class="kw">ggplot</span>(table.noise, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>TT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>CLT.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>CLT.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwCLTplot"></span>
<img src="STCI_files/figure-html/sampnoisewwCLTplot-1.png" alt="Average CLT-based approximations of sampling noise over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.11: Average CLT-based approximations of sampling noise over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test.CLT &lt;-<span class="st"> </span>simuls.ww[[k]][<span class="kw">sample</span>(N.plot),<span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;CLT.noise&#39;</span>)]
  test.CLT &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.CLT,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta),N.plot)))
  <span class="kw">colnames</span>(test.CLT) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;CLT.noise&#39;</span>,<span class="st">&#39;sampling.noise&#39;</span>)
  test.CLT<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
  plot.test.CLT &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.CLT, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>CLT.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>CLT.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
  plot.list[[k]] &lt;-<span class="st"> </span>plot.test.CLT
}
plot.CI &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confintervalCLT"></span>
<img src="STCI_files/figure-html/confintervalCLT-1.png" alt="CLT-based confidence intervals of $\hat{WW}$ for $\delta=$ 0.99 over sample replications for various sample sizes (true confidence intervals in red)" width="60%" />
<p class="caption">
Figure 2.12: CLT-based confidence intervals of <span class="math inline">\(\hat{WW}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 over sample replications for various sample sizes (true confidence intervals in red)
</p>
</div>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> In proving the main result on the asymptotic distribution of <span class="math inline">\(\hat{WW}\)</span>, we have also proved a very useful result: <span class="math inline">\(\hat{WW}\)</span> is the Ordinary Least Squares (OLS) estimator of <span class="math inline">\(\beta\)</span> in the regression <span class="math inline">\(Y_i=\alpha+\beta D_i + U_i\)</span>. This is pretty cool since we now can use our classical OLS estimator in our statistical package to estimate <span class="math inline">\(\hat{WW}\)</span>. Let’s compute the OLS estimate of <span class="math inline">\(WW\)</span> in our sample:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols.ww &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>Ds)
ww.ols &lt;-<span class="st"> </span>ols.ww<span class="op">$</span>coef[[<span class="dv">2</span>]]</code></pre></div>
<p>We have <span class="math inline">\(\hat{WW}_{OLS}=\)</span> 0.13 <span class="math inline">\(=\)</span> 0.13 <span class="math inline">\(=\hat{WW}\)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Another pretty cool consequence of Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a> and of its proof is that the standard error of the OLS estimator of <span class="math inline">\(\hat{WW}\)</span> (<span class="math inline">\(\sigma_{\beta}\)</span>) is related to the sampling noise of <span class="math inline">\(\hat{WW}\)</span> by the following formula: <span class="math inline">\(2\tilde{\epsilon}=2\Phi^{-1}\left(\frac{\delta+1}{2}\right)\sigma_{\beta}\)</span>.
</div>
<p> This implies that sampling noise is equal to 5 <span class="math inline">\(\sigma_{\beta}\)</span> when <span class="math inline">\(\delta=\)</span> 0.99 and to 4 <span class="math inline">\(\sigma_{\beta}\)</span> when <span class="math inline">\(\delta=\)</span> 0.95. It is thus very easy to move from estimates of the standard error of the <span class="math inline">\(\beta\)</span> coefficient to the extent of sampling noise.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> A last important consequence of Theorem <a href="FPSI.html#thm:asympnoiseWW">2.5</a> and of its proof is that the standard error of the OLS estimator of <span class="math inline">\(\hat{WW}\)</span> (<span class="math inline">\(\sigma_{\beta}\)</span>) that we use is the heteroskedasticity-robust one.
</div>
<p> Using the RCM, we can indeed show that:</p>
<span class="math display">\[\begin{align*}
    \alpha &amp; = \esp{Y_i^0|D_i=0}  \\
    \beta  &amp; =  \Delta^Y_{TT} \\
    U_i    &amp; = Y^0_i-\esp{Y^0_i|D_i=0} + D_i(\Delta^Y_i-\Delta^Y_{TT}),
    \end{align*}\]</span>
<p>Under Assumption <a href="FPCI.html#def:noselb">1.7</a>, we have:</p>
<span class="math display">\[\begin{align*}
    U_i    &amp; = (1-D_i)(Y^0_i-\esp{Y^0_i|D_i=0}) + D_i(Y_i^1-\esp{Y^1_i|D_i=1})
  \end{align*}\]</span>
<p>There is heteroskedasticity because the outcomes of the treated and of the untreated have different variances:</p>
<span class="math display">\[\begin{align*}
    \var{U_i|D_i=d} &amp; = \esp{U_i^2|D_i=d}  \\
                    &amp; = \esp{(Y^d_i-\esp{Y^d_i|D_i=d})^2|D_i=d}  \\
                    &amp; = \var{Y_i^d|D_i=d}
  \end{align*}\]</span>
<p>We do not want to assume homoskedasticity, since it would imply a constant treatment effect. Indeed, <span class="math inline">\(\var{Y_i^1|D_i=1} = \var{Y_i^0|D_i=1}+\var{\alpha_i|D_i=1}\)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> In order to estimate the heteroskedasticity robust standard error from the OLS regression, we can use the sandwich package in R Most available heteroskedasticity robust estimators based on the CLT can be written in the following way:
</div>

<span class="math display">\[\begin{align*}
  \var{\hat{\Theta}_{OLS}} &amp; \approx (X&#39;X)^{-1}X&#39;\hat{\Omega}X(X&#39;X)^{-1},
\end{align*}\]</span>
<p>where <span class="math inline">\(X\)</span> is the matrix of regressors and <span class="math inline">\(\hat{\Omega}=\diag(\hat{\sigma}^2_{U_1},\dots,\hat{\sigma}^2_{U_N})\)</span> is an estimate the covariance matrix of the residuals <span class="math inline">\(U_i\)</span>. Here are various classical estimators for <span class="math inline">\(\hat{\Omega}\)</span>:</p>
<span class="math display">\[\begin{align*}
  \text{HC0:} &amp; &amp; \hat{\sigma_{U_i}}^2 &amp; = \hat{U_i}^2 \\
  \text{HC1:} &amp; &amp; \hat{\sigma_{U_i}}^2 &amp; = \frac{N}{N-K}\hat{U_i}^2 \\
  \text{HC2:} &amp; &amp; \hat{\sigma_{U_i}}^2 &amp; = \frac{\hat{U_i}^2}{1-h_i} \\
  \text{HC3:} &amp; &amp; \hat{\sigma_{U_i}}^2 &amp; = \frac{\hat{U_i}^2}{(1-h_i)^2}, 
\end{align*}\]</span>
<p>where <span class="math inline">\(\hat{U}_i\)</span> is the residual from the OLS regression, <span class="math inline">\(K\)</span> is the number of regressors, <span class="math inline">\(h_i\)</span> is the leverage of observation <span class="math inline">\(i\)</span>, and is the <span class="math inline">\(i^{\text{th}}\)</span> diagonal element of <span class="math inline">\(H=X(X&#39;X)^{-1}X&#39;\)</span>. HC1 is the one reported by Stata when using the ‘robust’ option.</p>

<div class="example">
<span id="exm:unnamed-chunk-54" class="example"><strong>Example 2.8  </strong></span>Using the sandwich package, we can estimate the heteroskedasticity-robust variance-covariance matrix and sampling noise as follows:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols.ww.vcov.HC0 &lt;-<span class="st"> </span><span class="kw">vcovHC</span>(ols.ww, <span class="dt">type =</span> <span class="st">&quot;HC0&quot;</span>)
samp.noise.ww.CLT.ols &lt;-<span class="st"> </span><span class="cf">function</span>(delta,reg,...){
  <span class="kw">return</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg,...)[<span class="dv">2</span>,<span class="dv">2</span>]))
}</code></pre></div>
<p>For <span class="math inline">\(\delta=\)</span> 0.99, sampling noise estimated using the “HC0” option is equal to 0.35. This is exactly the value we have estimated using our CLT-based formula (<span class="math inline">\(\hat{2\tilde{\epsilon}}=\)</span> 0.35). Remember that sampling noise is actually equal to 0.39. Other “HC” options might be better in small samples. For example, with the “HC1” option, we have an estimate for sampling noise of 0.35. What would have happened to our estimate of sampling noise if we had ignored heteroskedasticity? The default OLS standard error estimate yields an estimate for sampling noise of 0.36.</p>
</div>
<div id="sec:resamp" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Using resampling methods</h3>
<p>The main intuition behind resampling methods is to use the sample as a population, to draw samples from it and compute our estimator on each of these samples in order to gauge its variability over sampling repetitions. There are three main methods of resampling that work that way: bootstrapping, radomization inference and subsampling. Bootstrapping draws samples with replacement, so that each sample has the same size as the original sample. Subsampling draws samples without replacement, thereby the samples are of a smaller size than the original one. Randomization inference keeps the same sample in all repetitions, but changes the allocation of the treatment.</p>
<p>Why would we use resampling methods instead of CLT-based standard errors? There are several possible reasons:</p>
<ol style="list-style-type: decimal">
<li>Asymptotic refinements: sometimes, resampling methods are more precise in small samples than the CLT-based asymptotic approaches. In that case, we say that resampling methods offer asymptotic refinements.</li>
<li>Ease of computation: for some estimators, the CLT-based estimates of sampling noise are complex or cumbersome to compute, whereas resampling methods are only computationally intensive.</li>
<li>Inexistence of CLT-based estimates of sampling noise: some estimators do not have any CLT-based estimates of sampling noise yet. That was the case for the Nearest-Neighbour Matching estimator (NNM) for a long time for example. It still is the case for the Synthetic Control Method estimator. Beware though that the bootstrap is not valid for all estimators. For example, it is possible to show that the bootstrap is invalid for NNM. Subsampling is valid for NNM though (see Abadie and Imbens, 2006).</li>
</ol>
<div id="bootstrap" class="section level4">
<h4><span class="header-section-number">2.2.4.1</span> Bootstrap</h4>
<p>The basic idea of the bootstrap is to use Monte Carlo replications to draw samples from the original sample with replacement. Then, at each replication, we compute the value of our estimator <span class="math inline">\(\hat{E}\)</span> on the new sample. Let’s call this new value <span class="math inline">\(\hat{E}^*_k\)</span> for bootstrap replication <span class="math inline">\(k\)</span>. Under certain conditions, the distribution of <span class="math inline">\(\hat{E}^*_k\)</span> approximates the distribution of <span class="math inline">\(\hat{E}\)</span> over sample repetitions very well, and all the more so as the sample size gets large.</p>
<p>What are the conditions under which the bootstrap is going to provide an accurate estimation of the distribution of <span class="math inline">\(\hat{E}\)</span>? Horowitz (2001) reports on a very nice result by Mammen that makes these conditions clear:</p>

<div class="theorem">
<span id="thm:mammen92" class="theorem"><strong>Theorem 2.6  (Mammen (1992))  </strong></span>Let <span class="math inline">\(\left\{X_i:i=1,\dots,N\right\}\)</span> be a random sample from a population. For a sequence of functions <span class="math inline">\(g_N\)</span> and sequences of numbers <span class="math inline">\(t_N\)</span> and <span class="math inline">\(\sigma_N\)</span>, define <span class="math inline">\(\bar{g}_N=\frac{1}{N}\sum_{i=1}^Ng_N(X_i)\)</span> and <span class="math inline">\(T_N=(\bar{g}_N-t_N)/\sigma_N\)</span>. For the bootstrap sample <span class="math inline">\(\left\{X^*_i:i=1,\dots,N\right\}\)</span>, define <span class="math inline">\(\bar{g}^*_N=\frac{1}{N}\sum_{i=1}^Ng_N(X^*_i)\)</span> and <span class="math inline">\(T^*_N=(\bar{g}^*_N-\bar{g}_N)/\sigma_N\)</span>. Let <span class="math inline">\(G_N(\tau)=\Pr(T_N\leq\tau)\)</span> and <span class="math inline">\(G^*_N(\tau)=\Pr(T^*_N\leq\tau)\)</span>, where this last probability distribution is taken over bootstrap sampling replications. Then <span class="math inline">\(G^*_N\)</span> consistently estimates <span class="math inline">\(G_N\)</span> if and only if <span class="math inline">\(T_N\stackrel{d}{\rightarrow}\mathcal{N}(0,1)\)</span>.
</div>

<p>Theorem <a href="FPSI.html#thm:mammen92">2.6</a> states that the bootstrap will offer a consistent estimation of the distribution of a given estimator if and only if this estimator is asymptotically normally distributed. It means that we could theoretically use the CLT-based asymptotic distribution to compute sampling noise. So, and it demands to be stronlgy emphasized, .</p>
<p>How do we estimate sampling noise with the bootstrap? There are several ways to do so, but I am going to emphasize the most widespread here, that is known as the percentile method. Let’s define <span class="math inline">\(E^*_{\frac{1-\delta}{2}}\)</span> and <span class="math inline">\(E^*_{\frac{1+\delta}{2}}\)</span> as the corresponding quantiles of the bootstrap distribution of <span class="math inline">\(\hat{E}^*_k\)</span> over a large number <span class="math inline">\(K\)</span> of replications. The bootstrapped sampling noise using the percentile method is simply the distance between these two quantities.</p>

<div class="theorem">
<p><span id="thm:bootnoiseWW" class="theorem"><strong>Theorem 2.7  (Bootstrapped Estimate of Sampling Noise of WW)  </strong></span>Under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#def:fullrank">2.3</a>, <a href="FPSI.html#def:iid">2.4</a> and <a href="FPSI.html#def:finitevar">2.5</a>, for a given confidence level <span class="math inline">\(\delta\)</span> and sample size <span class="math inline">\(N\)</span>, the sampling noise of <span class="math inline">\(\hat{WW}\)</span> can be approximated as follows:</p>
<span class="math display">\[\begin{align*}
2\epsilon &amp; \approx E^*_{\frac{1+\delta}{2}}-E^*_{\frac{1-\delta}{2}} \equiv 2\tilde{\epsilon}^b.
\end{align*}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The <span class="math inline">\(WW\)</span> estimator can be written as a sum:</p>
<span class="math display">\[\begin{align*}
\hat{\Delta^Y_{WW}} &amp; = \frac{1}{N}\sum_{i=1}^N\frac{\left(Y_i-\frac{1}{N}\sum_{i=1}^NY_i\right)\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)}{\frac{1}{N}\sum_{i=1}^N\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)^2}.
\end{align*}\]</span>
Using Lemma <a href="proofs.html#lem:asymWW">A.5</a>, we know that the <span class="math inline">\(WW\)</span> estimator is asymptotically normal under Assumptions <a href="FPCI.html#def:noselb">1.7</a>, <a href="FPSI.html#def:fullrank">2.3</a>, <a href="FPSI.html#def:iid">2.4</a> and <a href="FPSI.html#def:finitevar">2.5</a>. Using Theorem <a href="FPSI.html#thm:mammen92">2.6</a> proves the result.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> With the bootstrap, we are not going to define the confidence interval using Theorem <a href="FPSI.html#thm:confinter">2.1</a> but directly using <span class="math inline">\(\left\{E^*_{\frac{1-\delta}{2}};E^*_{\frac{1+\delta}{2}}\right\}\)</span>. Indeed, we have defined the bootstrapped estimator of sampling noise by using the asymetric confidence interval. We could have used the equivalent of Definition <a href="FPSI.html#def:sampnoise">2.1</a> on the bootstrapped samples to compute sampling noise using the symmetric confidence interval. Both are feasible and similar in large samples, since the asymptotic distribution is symmetric. One advantage of asymetric confidence intervals is that they might capture deviations from the normal distribution in small samples. These advantages are part of what we call asymptotic refinements. Rigorously, though, asymptotic refinements have not been proved to exist for the percentile method but only for the method bootstrapping asymptotically pivotal quantities.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> We say that a method brings asymptotic refinements if it increases the precision when estimating sampling noise and confidence intervals relative to the asymptotic CLT-based approximation. The bootstrap has been shown rigorously to bring asymptotic refinements when used to estimate the distribution of asymptotically pivotal statistic. An asymptotically pivotal statistic is a statistic that can be computed from the sample but that, asymptotically, converges to a quantity that does not depend on the sample, like for example a standard normal. Using Lemma <a href="proofs.html#lem:asymWW">A.5</a>, we know for example that the following statistic is asymptotically normal:
</div>

<span class="math display">\[\begin{align*}
  T_N^{WW} &amp; = \frac{\hat{\Delta^Y_{WW}}-\Delta^Y_{TT}}{\sqrt{\frac{\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}{N}}} \stackrel{d}{\rightarrow}\mathcal{N}\left(0,1\right).
\end{align*}\]</span>
<p>To build a confidence interval bootstrapping <span class="math inline">\(T_N^{WW}\)</span>, compute an estimator of <span class="math inline">\(T_N^{WW}\)</span> for each bootstrapped sample, say <span class="math inline">\(\hat{T}_{N,k}^{WW*}\)</span>. You can for example use the OLS estimator in the bootstrapped sample, with a heteroskedasticity-robust standard error estimator. Or you can compute the <span class="math inline">\(WW\)</span> estimator by hand in the sample along with an estimator of its variance using the variance of the outcomes in the treated and control groups. You can then estimate the confidence interval as follows: <span class="math inline">\(\left\{\hat{\Delta^Y_{WW}}-\hat{\sigma_{WW}}\hat{T}^{WW*}_{N,\frac{1-\delta}{2}};\hat{\Delta^Y_{WW}}+\hat{\sigma_{WW}}\hat{T}^{WW*}_{N,\frac{1+\delta}{2}}\right\}\)</span>, where <span class="math inline">\(\hat{T}^{WW*}_{N,q}\)</span> iq the <span class="math inline">\(q^{\text{th}}\)</span> quantile of the distribution of <span class="math inline">\(\hat{T}_{N,k}^{WW*}\)</span> over sampling replications and <span class="math inline">\(\hat{\sigma_{WW}}\)</span> is an estimate of the variance of <span class="math inline">\(\hat{\Delta^Y_{WW}}\)</span> (either the CLT-based approximation of the bootstrapped one, see below).</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> One last possibility to develop an estimator for sampling noise and confidence interval is to use the bootstrap in order to estimate the variance of the estimator <span class="math inline">\(\hat{E}\)</span>, <span class="math inline">\(\hat{\sigma^2_{E}}\)</span>, and then use it to compute sampling noise. If <span class="math inline">\(\hat{E}\)</span> is asymptotically normally distributed, we have that sampling noise is equal to <span class="math inline">\(2\Phi^{-1}\left(\frac{\delta+1}{2}\right)\hat{\sigma_{E}}\)</span>. You can use the usual formula from Theore <a href="FPSI.html#thm:confinter">2.1</a> to compute the confidence interval. The bootstrapped variance of <span class="math inline">\(\hat{E}\)</span>, <span class="math inline">\(\hat{\sigma^2_{E}}\)</span>, is simply the variance of <span class="math inline">\(\hat{E}^*_k\)</span> over bootstrap replications.
</div>


<div class="example">
<span id="exm:unnamed-chunk-59" class="example"><strong>Example 2.9  </strong></span>In the numerical example, I am going to derive the bootstrapped confidence intervals and sampling noise for the percentile method. Let’s first put the dataset from our example in a nice data frame format so that resampling is made easier. We then define a function taking a number of bootstrapped replications and spitting out sampling noise and confidence intervals.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(y,Ds,yB))
boot.fun.ww.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="cf">function</span>(seed,data){
  <span class="kw">set.seed</span>(seed,<span class="dt">kind=</span><span class="st">&quot;Wichmann-Hill&quot;</span>)
  data &lt;-<span class="st"> </span>data[<span class="kw">sample</span>(<span class="kw">nrow</span>(data),<span class="dt">replace =</span> <span class="ot">TRUE</span>),]
  ols.ww &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>Ds,<span class="dt">data=</span>data)
  ww &lt;-<span class="st"> </span>ols.ww<span class="op">$</span>coef[[<span class="dv">2</span>]]
  <span class="kw">return</span>(ww)
}

boot.fun.ww &lt;-<span class="st"> </span><span class="cf">function</span>(Nboot,data){
  <span class="co">#sfInit(parallel=TRUE,cpus=8)</span>
  boot &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nboot,boot.fun.ww.<span class="dv">1</span>,<span class="dt">data=</span>data)
  <span class="co">#sfStop()</span>
  <span class="kw">return</span>(<span class="kw">unlist</span>(boot))
}

boot.CI.ww &lt;-<span class="st"> </span><span class="cf">function</span>(boot,delta){
  <span class="kw">return</span>(<span class="kw">c</span>(<span class="kw">quantile</span>(boot,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">-</span>delta)<span class="op">/</span><span class="dv">2</span>),<span class="kw">quantile</span>(boot,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">+</span>delta)<span class="op">/</span><span class="dv">2</span>)))
}

boot.samp.noise.ww &lt;-<span class="st"> </span><span class="cf">function</span>(boot,delta){
  <span class="kw">return</span>(<span class="kw">quantile</span>(boot,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">+</span>delta)<span class="op">/</span><span class="dv">2</span>)<span class="op">-</span><span class="kw">quantile</span>(boot,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">-</span>delta)<span class="op">/</span><span class="dv">2</span>))
}

Nboot &lt;-<span class="st"> </span><span class="dv">500</span>
ww.boot &lt;-<span class="st"> </span><span class="kw">boot.fun.ww</span>(Nboot,data)
ww.CI.boot &lt;-<span class="st"> </span><span class="kw">boot.CI.ww</span>(ww.boot,delta)
ww.samp.noise.boot &lt;-<span class="st"> </span><span class="kw">boot.samp.noise.ww</span>(ww.boot,delta)</code></pre></div>
<p>Over 500 replications, the 99% bootstrapped confidence interval using the percentile method is <span class="math inline">\(\left\{-0.023;0.316\right\}\)</span>. As a consequence, the bootstrapped estimate of 99% sampling noise is of 0.339. Remember that, with <span class="math inline">\(N=\)</span> 1000, sampling noise is actually equal to 0.39.</p>
<p>In order to assess the global precision of bootstrapping, we are going to resort to Monte Carlo simulations. For each Monte Carlo sample, we are going to estimate sampling noise and confidence intervals using the bootstrap. As you can imagine, this is going to prove rather computationally intensive. I cannot use parallelization twice: I have to choose whether to parallelize the Monte Carlo simulations or the bootstrap simulations. I have choosen to parallelize the outer loop, so that a given job takes longer on each cluster.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.ww.boot &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param,Nboot,delta){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
  Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  data &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(y,Ds,yB))
  ww.boot &lt;-<span class="st"> </span><span class="kw">boot.fun.ww</span>(Nboot,data)
  ww.CI.boot &lt;-<span class="st"> </span><span class="kw">boot.CI.ww</span>(ww.boot,delta)
  ww.samp.noise.boot &lt;-<span class="st"> </span><span class="kw">boot.samp.noise.ww</span>(ww.boot,delta)
  <span class="kw">return</span>(<span class="kw">c</span>((<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>Ds)<span class="op">-</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">-</span>Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)),<span class="kw">var</span>(y[Ds<span class="op">==</span><span class="dv">1</span>]),<span class="kw">var</span>(y[Ds<span class="op">==</span><span class="dv">0</span>]),<span class="kw">mean</span>(Ds),ww.CI.boot[[<span class="dv">1</span>]],ww.CI.boot[[<span class="dv">2</span>]],ww.samp.noise.boot))
}

sf.simuls.ww.N.boot &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,Nboot,delta,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">2</span><span class="op">*</span>ncpus)
  <span class="kw">sfExport</span>(<span class="st">&quot;boot.fun.ww&quot;</span>,<span class="st">&quot;boot.CI.ww&quot;</span>,<span class="st">&quot;boot.samp.noise.ww&quot;</span>,<span class="st">&quot;boot.fun.ww.1&quot;</span>)
  sim &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.ww.boot,<span class="dt">N=</span>N,<span class="dt">Nboot=</span>Nboot,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">7</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;V1&#39;</span>,<span class="st">&#39;V0&#39;</span>,<span class="st">&#39;p&#39;</span>,<span class="st">&#39;boot.lCI&#39;</span>,<span class="st">&#39;boot.uCI&#39;</span>,<span class="st">&#39;boot.samp.noise&#39;</span>)
  <span class="kw">return</span>(sim)
}

simuls.ww.boot &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.ww.N.boot,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param,<span class="dt">Nboot=</span>Nboot,<span class="dt">delta=</span>delta)</code></pre></div>
<p>We can now graph our bootstrapped estimate of sampling noise in all of our samples, the average bootstrapped estimates of sampling noise and of confidence intervals, in Figures <a href="FPSI.html#fig:sampnoisewwbootall">2.13</a>, <a href="FPSI.html#fig:sampnoisewwbootplot">2.14</a> and <a href="FPSI.html#fig:confintervalboot">2.15</a> show.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  <span class="kw">hist</span>(simuls.ww.boot[[i]][,<span class="st">&#39;boot.samp.noise&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">bar</span>(epsilon))),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="kw">min</span>(simuls.ww.boot[[i]][,<span class="st">&#39;boot.samp.noise&#39;</span>]),<span class="kw">max</span>(simuls.ww.boot[[i]][,<span class="st">&#39;boot.samp.noise&#39;</span>])))
  <span class="kw">abline</span>(<span class="dt">v=</span>table.noise[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwbootall"></span>
<img src="STCI_files/figure-html/sampnoisewwbootall-1.png" alt="Distribution of the bootstrapped approximation of sampling noise over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.13: Distribution of the bootstrapped approximation of sampling noise over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  table.noise<span class="op">$</span>boot.noise[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(simuls.ww.boot[[k]]<span class="op">$</span>boot.samp.noise)
}
<span class="kw">ggplot</span>(table.noise, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>TT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>boot.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>boot.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwbootplot"></span>
<img src="STCI_files/figure-html/sampnoisewwbootplot-1.png" alt="Average bootstrapped approximations of sampling noise over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.14: Average bootstrapped approximations of sampling noise over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
 <span class="kw">set.seed</span>(<span class="dv">1234</span>)
 test.boot &lt;-<span class="st"> </span>simuls.ww.boot[[k]][<span class="kw">sample</span>(N.plot),<span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;boot.lCI&#39;</span>,<span class="st">&#39;boot.uCI&#39;</span>)]
 test.boot &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.boot,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww.boot[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta),N.plot)))
 <span class="kw">colnames</span>(test.boot) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;boot.lCI&#39;</span>,<span class="st">&#39;boot.uCI&#39;</span>,<span class="st">&#39;sampling.noise&#39;</span>)
 test.boot<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
 plot.test.boot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.boot, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>boot.lCI, <span class="dt">ymax=</span>boot.uCI), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">     </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">     </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">     </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
 plot.list[[k]] &lt;-<span class="st"> </span>plot.test.boot
}
plot.CI &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confintervalboot"></span>
<img src="STCI_files/figure-html/confintervalboot-1.png" alt="Bootstrapped confidence intervals of $\hat{WW}$ for $\delta=$ 0.99 over sample replications for various sample sizes (true confidence intervals in red)" width="60%" />
<p class="caption">
Figure 2.15: Bootstrapped confidence intervals of <span class="math inline">\(\hat{WW}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 over sample replications for various sample sizes (true confidence intervals in red)
</p>
</div>
<p><strong>TO DO: COMMENT AND USE PIVOTAL TEST STATISTIC</strong></p>
</div>
<div id="randomization-inference" class="section level4">
<h4><span class="header-section-number">2.2.4.2</span> Randomization inference</h4>
<p>Randomization inference (a.k.a. Fisher’s permutation approach) tries to mimick the sampling noise due to the random allocation of the treatment vector, as we have seen in Section <a href="FPSI.html#sec:illusnoisesamp">2.1.3</a>. In practice, the idea is simply to look at how the treatment effect that we estimate varies when we visit all the possible allocations of the treament dummy in the sample. For each new allocation, we are going to compute the with/without estimator using the observed outcomes and the newly allocated treatment dummy. It means that some actually treated observations are going to enter into the computation of the control group mean, while some actually untreated observations are going to enter into the computation of the treatment group mean. As a consequence, the resulting distribution will be centered at zero. Under the assumption of a constant treatment effect, the distribution of the parameter obtained using randomization inference will be an exact estimation of sampling noise for the sample treatment effect.</p>
<p>Notice how beautilful the result is: randomization inference yields an  measure of sampling noise. The resulting estimate of sampling noise is not an approximation that is going to become better as sample size increases. No, it is the  value of sampling noise in the sample.</p>
<p>There are two ways to compute a confidence interval using Fisher’s permutation approach. One is to form symmetric intervals using our estimate of sampling noise as presented in Section <a href="FPSI.html#sec:confinterv">2.1.4</a>. Another approach is to directly use the quantiles of the distribution of the parameter centered around the estimated treatment effect, in the same spirit as bootstrapped confidence intervals using the percentile approach. This last approach accomodates possible asymetries in the finite sample distribution of the treatment effect.</p>
<p>Computing the value of the treatment effect for all possible treatment allocations can take a lot of time with large samples. That’s why we in general compute the test statistic for a reasonably large number of random allocations.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Fisher’s original approach is slightly different from the one I delineate here. Fisher wanted to derive a test statistic for whether the treatment effect was zero, not to estimate sampling noise. Under the null that the treatment has absolutely no effect whatsoever on any unit, any test statistic whose value should be zero if the two distributions where identical can be computed on the actual sample and its distribution can be derived using Fisher’s permutation approach. The test statistic can be the difference in means, standard deviations, medians, ranks, the T-stat, the Kolmogorov-Smirnov test statistic or any other test statistic that you might want to compute. Comparing the actual value of the test statistic to its distribution under the null gives a p-value for the validity of the null.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Imbens and Rubin propose a more complex procedure to derive the confidence interval for the treatment effect using randomization inference. They propose to compute Fisher’s p-value for different values of the treatment effect, and to set the confidence interval as the values of the treatment effect under and above which the p-value is smaller than <span class="math inline">\(\delta\)</span>. When using the with/without estimator as the test statistic, the two approches should be equivalent. Is is possible that the estimates using statistics less influenced by outliers are more precise though.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Note that we pay two prices for having an exact estimation of sampling noise:
</div>

<ol style="list-style-type: decimal">
<li>We have to assume that the treatment effect is constant, e.g. we have to assume homoskedasticity. This is in general not the case. Whether this is in general a big issue depends on how large the difference is between homoskedastic and heteroskedastic standard errors. One way around this issue would be to add a small amount of noise to the observations that are in the group with the lowest variance. Whether this would work in practice is still to be demonstrated.</li>
<li>We have to be interested only in the sampling noise of the sample treatment effect. The sampling noise of the population treatment effect is not estimated using Fisher’s permutation approach. As we have seen in Section <a href="FPSI.html#sec:illusnoisesamp">2.1.3</a>, there is no practical difference between these two sampling noises in our example. Whether this is the case in general deserves further investigation.</li>
</ol>

<div class="example">
<span id="exm:unnamed-chunk-63" class="example"><strong>Example 2.10  </strong></span>In practice, randomization inference is very close to a bootstrap procedure, except that instead of resampling with replacement from the original sample, we only change the vector of treatment allocation at each replication.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fisher.fun.ww.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="cf">function</span>(seed,data){
  <span class="kw">set.seed</span>(seed,<span class="dt">kind=</span><span class="st">&quot;Wichmann-Hill&quot;</span>)
  data<span class="op">$</span>D &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">nrow</span>(data),<span class="dv">1</span>,<span class="kw">mean</span>(data<span class="op">$</span>Ds))
  ols.ww &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>D,<span class="dt">data=</span>data)
  ww &lt;-<span class="st"> </span>ols.ww<span class="op">$</span>coef[[<span class="dv">2</span>]]
  <span class="kw">return</span>(ww)
}

fisher.fun.ww &lt;-<span class="st"> </span><span class="cf">function</span>(Nfisher,data,delta){
  fisher &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nfisher,fisher.fun.ww.<span class="dv">1</span>,<span class="dt">data=</span>data))
  ols.ww &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>Ds,<span class="dt">data=</span>data)
  ww &lt;-<span class="st"> </span>ols.ww<span class="op">$</span>coef[[<span class="dv">2</span>]]
  fisher &lt;-<span class="st"> </span>fisher<span class="op">+</span><span class="st"> </span>ww
  fisher.CI.ww &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">quantile</span>(fisher,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">-</span>delta)<span class="op">/</span><span class="dv">2</span>),<span class="kw">quantile</span>(fisher,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">+</span>delta)<span class="op">/</span><span class="dv">2</span>))
  fisher.samp.noise.ww &lt;-<span class="st"> </span><span class="kw">quantile</span>(fisher,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">+</span>delta)<span class="op">/</span><span class="dv">2</span>)<span class="op">-</span><span class="kw">quantile</span>(fisher,<span class="dt">prob=</span>(<span class="dv">1</span><span class="op">-</span>delta)<span class="op">/</span><span class="dv">2</span>)
  <span class="kw">return</span>(<span class="kw">list</span>(fisher,fisher.CI.ww,fisher.samp.noise.ww))
}

Nfisher &lt;-<span class="st"> </span><span class="dv">500</span>
ww.fisher &lt;-<span class="st"> </span><span class="kw">fisher.fun.ww</span>(Nfisher,data,delta)</code></pre></div>
<p>Over 500 replications, the 99% confidence interval based on Fisher’s permutation approach is <span class="math inline">\(\left\{-0.052;0.3\right\}\)</span>. As a consequence, the bootstrapped estimate of 99% sampling noise is of 0.352. Remember that, with <span class="math inline">\(N=\)</span> 1000, sampling noise is actually equal to 0.39.</p>
<p>In order to assess the global precision of Fisher’s permutation method, we are going to resort to Monte Carlo simulations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.ww.fisher &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param,Nfisher,delta){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))
  Ds[V<span class="op">&lt;=</span><span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])] &lt;-<span class="st"> </span><span class="dv">1</span> 
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  data &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(y,Ds,yB))
  ww.fisher &lt;-<span class="st"> </span><span class="kw">fisher.fun.ww</span>(Nfisher,data,delta)
  <span class="kw">return</span>(<span class="kw">c</span>((<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>Ds)<span class="op">-</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">-</span>Ds))<span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)),<span class="kw">var</span>(y[Ds<span class="op">==</span><span class="dv">1</span>]),<span class="kw">var</span>(y[Ds<span class="op">==</span><span class="dv">0</span>]),<span class="kw">mean</span>(Ds),ww.fisher[[<span class="dv">2</span>]][<span class="dv">1</span>],ww.fisher[[<span class="dv">2</span>]][<span class="dv">2</span>],ww.fisher[[<span class="dv">3</span>]]))
}

sf.simuls.ww.N.fisher &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,Nfisher,delta,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">2</span><span class="op">*</span>ncpus)
  <span class="kw">sfExport</span>(<span class="st">&quot;fisher.fun.ww&quot;</span>,<span class="st">&quot;fisher.fun.ww.1&quot;</span>)
  sim &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.ww.fisher,<span class="dt">N=</span>N,<span class="dt">Nfisher=</span>Nfisher,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">7</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;V1&#39;</span>,<span class="st">&#39;V0&#39;</span>,<span class="st">&#39;p&#39;</span>,<span class="st">&#39;fisher.lCI&#39;</span>,<span class="st">&#39;fisher.uCI&#39;</span>,<span class="st">&#39;fisher.samp.noise&#39;</span>)
  <span class="kw">return</span>(sim)
}

simuls.ww.fisher &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.ww.N.fisher,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param,<span class="dt">Nfisher=</span>Nfisher,<span class="dt">delta=</span>delta)</code></pre></div>
<p>We can now graph our bootstrapped estimate of sampling noise in all of our samples, the average bootstrapped estimates of sampling noise and of confidence intervals, as Figures <a href="FPSI.html#fig:sampnoisewwfisherall">2.16</a>, <a href="FPSI.html#fig:sampnoisewwfisherplot">2.17</a> and <a href="FPSI.html#fig:confintervalfisher">2.18</a> show. The results are pretty good. On average, estimates of sampling noise using Randomization Inference are pretty accurate, as Figure <a href="FPSI.html#fig:sampnoisewwfisherplot">2.17</a> shows. It seems that sampling noise is underestimated by Randomization Inference when <span class="math inline">\(N=\)</span> 1000, without any clear reason why.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
  <span class="kw">hist</span>(simuls.ww.fisher[[i]][,<span class="st">&#39;fisher.samp.noise&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">bar</span>(epsilon))),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="kw">min</span>(simuls.ww.fisher[[i]][,<span class="st">&#39;fisher.samp.noise&#39;</span>]),<span class="kw">max</span>(simuls.ww.fisher[[i]][,<span class="st">&#39;fisher.samp.noise&#39;</span>])))
  <span class="kw">abline</span>(<span class="dt">v=</span>table.noise[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwfisherall"></span>
<img src="STCI_files/figure-html/sampnoisewwfisherall-1.png" alt="Distribution of the estimates of sampling noise using Randomization Inference over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.16: Distribution of the estimates of sampling noise using Randomization Inference over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  table.noise<span class="op">$</span>fisher.noise[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(simuls.ww.fisher[[k]]<span class="op">$</span>fisher.samp.noise)
}
<span class="kw">ggplot</span>(table.noise, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>TT)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>TT<span class="op">-</span>fisher.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>TT<span class="op">+</span>fisher.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwfisherplot"></span>
<img src="STCI_files/figure-html/sampnoisewwfisherplot-1.png" alt="Average estimates of sampling noise using Randomization Inference over replications of samples of different sizes (true sampling noise in red)" width="60%" />
<p class="caption">
Figure 2.17: Average estimates of sampling noise using Randomization Inference over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
 <span class="kw">set.seed</span>(<span class="dv">1234</span>)
 test.fisher &lt;-<span class="st"> </span>simuls.ww.fisher[[k]][<span class="kw">sample</span>(N.plot),<span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;fisher.lCI&#39;</span>,<span class="st">&#39;fisher.uCI&#39;</span>)]
 test.fisher &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.fisher,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.ww.fisher[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta),N.plot)))
 <span class="kw">colnames</span>(test.fisher) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;fisher.lCI&#39;</span>,<span class="st">&#39;fisher.uCI&#39;</span>,<span class="st">&#39;sampling.noise&#39;</span>)
 test.fisher<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
 plot.test.fisher &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.fisher, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>fisher.lCI, <span class="dt">ymax=</span>fisher.uCI), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">     </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">     </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">     </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">     </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
 plot.list[[k]] &lt;-<span class="st"> </span>plot.test.fisher
}
plot.CI &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confintervalfisher"></span>
<img src="STCI_files/figure-html/confintervalfisher-1.png" alt="Confidence intervals of $\hat{WW}$ for $\delta=$ 0.99 estimated using Randomization Inference over sample replications for various sample sizes (true confidence intervals in red)" width="60%" />
<p class="caption">
Figure 2.18: Confidence intervals of <span class="math inline">\(\hat{WW}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 estimated using Randomization Inference over sample replications for various sample sizes (true confidence intervals in red)
</p>
</div>
<p><strong>TO DO: ALTERNATIVE APPROACH USING p-VALUES</strong></p>
</div>
<div id="subsampling" class="section level4">
<h4><span class="header-section-number">2.2.4.3</span> Subsampling</h4>
<p><strong>TO DO: ALL</strong></p>

</div>
</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="FPCI.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="RCT.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/chabefer/STCI/blob/master/02_FPSI.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["STCI.pdf"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 1
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
