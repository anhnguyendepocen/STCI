<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 3 Randomized Controlled Trials | Statistical Tools for Causal Inference</title>
  <meta name="description" content="This is an open source collaborative book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 3 Randomized Controlled Trials | Statistical Tools for Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an open source collaborative book." />
  <meta name="github-repo" content="chabefer/STCI" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Randomized Controlled Trials | Statistical Tools for Causal Inference" />
  
  <meta name="twitter:description" content="This is an open source collaborative book." />
  

<meta name="author" content="The SKY Community">


<meta name="date" content="2019-10-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="FPSI.html">
<link rel="next" href="NE.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







$$
\newcommand{\uns}[1]{\mathbf{1}[#1]}
\newcommand{\esp}[1]{\mathbf{E}[#1]}
\newcommand{\Ind}{\perp\kern-5pt\perp}
\newcommand{\var}[1]{\mathbf{V}[ #1 ]}
\newcommand{\cov}[1]{\mathbf{C}[ #1 ]}
\newcommand{\plim}[1]{\text{plim}_{ #1 \rightarrow \infty}}
\newcommand{\plims}{\text{plim}}
\newcommand{\partder}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\diag}{diag}
$$


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Tools for Causal Inference</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>I Fundamental Problems of Inference</b></span></li>
<li class="chapter" data-level="" data-path="introduction-the-two-fundamental-problems-of-inference.html"><a href="introduction-the-two-fundamental-problems-of-inference.html"><i class="fa fa-check"></i>Introduction: the Two Fundamental Problems of Inference</a></li>
<li class="chapter" data-level="1" data-path="FPCI.html"><a href="FPCI.html"><i class="fa fa-check"></i><b>1</b> Fundamental Problem of Causal Inference</a><ul>
<li class="chapter" data-level="1.1" data-path="FPCI.html"><a href="FPCI.html#rubin-causal-model"><i class="fa fa-check"></i><b>1.1</b> Rubin Causal Model</a><ul>
<li class="chapter" data-level="1.1.1" data-path="FPCI.html"><a href="FPCI.html#treatment-allocation-rule"><i class="fa fa-check"></i><b>1.1.1</b> Treatment allocation rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="FPCI.html"><a href="FPCI.html#potential-outcomes"><i class="fa fa-check"></i><b>1.1.2</b> Potential outcomes</a></li>
<li class="chapter" data-level="1.1.3" data-path="FPCI.html"><a href="FPCI.html#switching-equation"><i class="fa fa-check"></i><b>1.1.3</b> Switching equation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="FPCI.html"><a href="FPCI.html#treatment-effects"><i class="fa fa-check"></i><b>1.2</b> Treatment effects</a><ul>
<li class="chapter" data-level="1.2.1" data-path="FPCI.html"><a href="FPCI.html#individual-level-treatment-effects"><i class="fa fa-check"></i><b>1.2.1</b> Individual level treatment effects</a></li>
<li class="chapter" data-level="1.2.2" data-path="FPCI.html"><a href="FPCI.html#average-treatment-effect-on-the-treated"><i class="fa fa-check"></i><b>1.2.2</b> Average treatment effect on the treated</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="FPCI.html"><a href="FPCI.html#fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>1.3</b> Fundamental problem of causal inference</a></li>
<li class="chapter" data-level="1.4" data-path="FPCI.html"><a href="FPCI.html#intuitive-estimators-confounding-factors-and-selection-bias"><i class="fa fa-check"></i><b>1.4</b> Intuitive estimators, confounding factors and selection bias</a><ul>
<li class="chapter" data-level="1.4.1" data-path="FPCI.html"><a href="FPCI.html#withwithout-comparison-selection-bias-and-cross-sectional-confounders"><i class="fa fa-check"></i><b>1.4.1</b> With/Without comparison, selection bias and cross-sectional confounders</a></li>
<li class="chapter" data-level="1.4.2" data-path="FPCI.html"><a href="FPCI.html#the-beforeafter-comparison-temporal-confounders-and-time-trend-bias"><i class="fa fa-check"></i><b>1.4.2</b> The before/after comparison, temporal confounders and time trend bias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="FPSI.html"><a href="FPSI.html"><i class="fa fa-check"></i><b>2</b> Fundamental Problem of Statistical Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="FPSI.html"><a href="FPSI.html#sec:sampnoise"><i class="fa fa-check"></i><b>2.1</b> What is sampling noise? Definition and illustration</a><ul>
<li class="chapter" data-level="2.1.1" data-path="FPSI.html"><a href="FPSI.html#sec:definitionnoise"><i class="fa fa-check"></i><b>2.1.1</b> Sampling noise, a definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisepop"><i class="fa fa-check"></i><b>2.1.2</b> Sampling noise for the population treatment effect</a></li>
<li class="chapter" data-level="2.1.3" data-path="FPSI.html"><a href="FPSI.html#sec:illusnoisesamp"><i class="fa fa-check"></i><b>2.1.3</b> Sampling noise for the sample treatment effect</a></li>
<li class="chapter" data-level="2.1.4" data-path="FPSI.html"><a href="FPSI.html#sec:confinterv"><i class="fa fa-check"></i><b>2.1.4</b> Building confidence intervals from estimates of sampling noise</a></li>
<li class="chapter" data-level="2.1.5" data-path="FPSI.html"><a href="FPSI.html#reporting-sampling-noise-a-proposal"><i class="fa fa-check"></i><b>2.1.5</b> Reporting sampling noise: a proposal</a></li>
<li class="chapter" data-level="2.1.6" data-path="FPSI.html"><a href="FPSI.html#sec:effectsize"><i class="fa fa-check"></i><b>2.1.6</b> Using effect sizes to normalize the reporting of treatment effects and their precision</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="FPSI.html"><a href="FPSI.html#sec:estimsampnoise"><i class="fa fa-check"></i><b>2.2</b> Estimating sampling noise</a><ul>
<li class="chapter" data-level="2.2.1" data-path="FPSI.html"><a href="FPSI.html#sec:assumptions"><i class="fa fa-check"></i><b>2.2.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.2.2" data-path="FPSI.html"><a href="FPSI.html#sec:cheb"><i class="fa fa-check"></i><b>2.2.2</b> Using Chebyshev’s inequality</a></li>
<li class="chapter" data-level="2.2.3" data-path="FPSI.html"><a href="FPSI.html#sec:CLT"><i class="fa fa-check"></i><b>2.2.3</b> Using the Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.4" data-path="FPSI.html"><a href="FPSI.html#sec:resamp"><i class="fa fa-check"></i><b>2.2.4</b> Using resampling methods</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Methods of Causal Inference</b></span></li>
<li class="chapter" data-level="3" data-path="RCT.html"><a href="RCT.html"><i class="fa fa-check"></i><b>3</b> Randomized Controlled Trials</a><ul>
<li class="chapter" data-level="3.1" data-path="RCT.html"><a href="RCT.html#sec:design1"><i class="fa fa-check"></i><b>3.1</b> Brute Force Design</a><ul>
<li class="chapter" data-level="3.1.1" data-path="RCT.html"><a href="RCT.html#identification"><i class="fa fa-check"></i><b>3.1.1</b> Identification</a></li>
<li class="chapter" data-level="3.1.2" data-path="RCT.html"><a href="RCT.html#estimating-ate"><i class="fa fa-check"></i><b>3.1.2</b> Estimating ATE</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="RCT.html"><a href="RCT.html#sec:design2"><i class="fa fa-check"></i><b>3.2</b> Randomization After Self-Selection</a><ul>
<li class="chapter" data-level="3.2.1" data-path="RCT.html"><a href="RCT.html#identification-1"><i class="fa fa-check"></i><b>3.2.1</b> Identification</a></li>
<li class="chapter" data-level="3.2.2" data-path="RCT.html"><a href="RCT.html#estimating-tt"><i class="fa fa-check"></i><b>3.2.2</b> Estimating TT</a></li>
<li class="chapter" data-level="3.2.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-1"><i class="fa fa-check"></i><b>3.2.3</b> Estimating Sampling Noise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="RCT.html"><a href="RCT.html#sec:design3"><i class="fa fa-check"></i><b>3.3</b> Randomization After Eligibility</a><ul>
<li class="chapter" data-level="3.3.1" data-path="RCT.html"><a href="RCT.html#identification-2"><i class="fa fa-check"></i><b>3.3.1</b> Identification</a></li>
<li class="chapter" data-level="3.3.2" data-path="RCT.html"><a href="RCT.html#estimating-the-ite-and-the-tt"><i class="fa fa-check"></i><b>3.3.2</b> Estimating the ITE and the TT</a></li>
<li class="chapter" data-level="3.3.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-2"><i class="fa fa-check"></i><b>3.3.3</b> Estimating sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="RCT.html"><a href="RCT.html#sec:design4"><i class="fa fa-check"></i><b>3.4</b> Encouragement Design</a><ul>
<li class="chapter" data-level="3.4.1" data-path="RCT.html"><a href="RCT.html#identification-3"><i class="fa fa-check"></i><b>3.4.1</b> Identification</a></li>
<li class="chapter" data-level="3.4.2" data-path="RCT.html"><a href="RCT.html#estimating-the-local-average-treatment-effect-and-the-intention-to-treat-effect"><i class="fa fa-check"></i><b>3.4.2</b> Estimating the Local Average Treatment Effect and the Intention to Treat Effect</a></li>
<li class="chapter" data-level="3.4.3" data-path="RCT.html"><a href="RCT.html#estimating-sampling-noise-3"><i class="fa fa-check"></i><b>3.4.3</b> Estimating sampling noise</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="RCT.html"><a href="RCT.html#sec:threats"><i class="fa fa-check"></i><b>3.5</b> Threats to the validity of RCTs</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="NE.html"><a href="NE.html"><i class="fa fa-check"></i><b>4</b> Natural Experiments</a></li>
<li class="chapter" data-level="5" data-path="sec-OM.html"><a href="sec-OM.html"><i class="fa fa-check"></i><b>5</b> Observational Methods</a></li>
<li class="part"><span><b>III Additional Topics</b></span></li>
<li class="chapter" data-level="6" data-path="Power.html"><a href="Power.html"><i class="fa fa-check"></i><b>6</b> Power Analysis</a></li>
<li class="chapter" data-level="7" data-path="Placebo.html"><a href="Placebo.html"><i class="fa fa-check"></i><b>7</b> Placebo Tests</a></li>
<li class="chapter" data-level="8" data-path="cluster.html"><a href="cluster.html"><i class="fa fa-check"></i><b>8</b> Clustering</a></li>
<li class="chapter" data-level="9" data-path="LaLonde.html"><a href="LaLonde.html"><i class="fa fa-check"></i><b>9</b> LaLonde Tests</a></li>
<li class="chapter" data-level="10" data-path="Diffusion.html"><a href="Diffusion.html"><i class="fa fa-check"></i><b>10</b> Diffusion effects</a></li>
<li class="chapter" data-level="11" data-path="Distribution.html"><a href="Distribution.html"><i class="fa fa-check"></i><b>11</b> Distributional effects</a></li>
<li class="chapter" data-level="12" data-path="sec-meta.html"><a href="sec-meta.html"><i class="fa fa-check"></i><b>12</b> Meta-analysis and Publication Bias</a><ul>
<li class="chapter" data-level="12.1" data-path="sec-meta.html"><a href="sec-meta.html#meta-analysis"><i class="fa fa-check"></i><b>12.1</b> Meta-analysis</a><ul>
<li class="chapter" data-level="12.1.1" data-path="sec-meta.html"><a href="sec-meta.html#basic-setting"><i class="fa fa-check"></i><b>12.1.1</b> Basic setting</a></li>
<li class="chapter" data-level="12.1.2" data-path="sec-meta.html"><a href="sec-meta.html#MetaWA"><i class="fa fa-check"></i><b>12.1.2</b> Meta-analysis as a weighted average</a></li>
<li class="chapter" data-level="12.1.3" data-path="sec-meta.html"><a href="sec-meta.html#constantly-updated-meta-analysis"><i class="fa fa-check"></i><b>12.1.3</b> Constantly updated meta-analysis</a></li>
<li class="chapter" data-level="12.1.4" data-path="sec-meta.html"><a href="sec-meta.html#testing-for-the-homogeneity-of-treatment-effects"><i class="fa fa-check"></i><b>12.1.4</b> Testing for the homogeneity of treatment effects</a></li>
<li class="chapter" data-level="12.1.5" data-path="sec-meta.html"><a href="sec-meta.html#meta-analysis-when-treatment-effects-are-heterogeneous"><i class="fa fa-check"></i><b>12.1.5</b> Meta-analysis when treatment effects are heterogeneous</a></li>
<li class="chapter" data-level="12.1.6" data-path="sec-meta.html"><a href="sec-meta.html#meta-regression"><i class="fa fa-check"></i><b>12.1.6</b> Meta-regression</a></li>
<li class="chapter" data-level="12.1.7" data-path="sec-meta.html"><a href="sec-meta.html#why-vote-counting-does-not-work"><i class="fa fa-check"></i><b>12.1.7</b> Why vote-counting does not work</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="sec-meta.html"><a href="sec-meta.html#publication-bias"><i class="fa fa-check"></i><b>12.2</b> Publication bias</a><ul>
<li class="chapter" data-level="12.2.1" data-path="sec-meta.html"><a href="sec-meta.html#sources-of-publication-bias"><i class="fa fa-check"></i><b>12.2.1</b> Sources of publication bias</a></li>
<li class="chapter" data-level="12.2.2" data-path="sec-meta.html"><a href="sec-meta.html#detecting-and-correcting-for-publication-bias"><i class="fa fa-check"></i><b>12.2.2</b> Detecting and correcting for publication bias</a></li>
<li class="chapter" data-level="12.2.3" data-path="sec-meta.html"><a href="sec-meta.html#vote-counting-and-publication-bias"><i class="fa fa-check"></i><b>12.2.3</b> Vote counting and publication bias</a></li>
<li class="chapter" data-level="12.2.4" data-path="sec-meta.html"><a href="sec-meta.html#the-value-of-a-statistically-significant-result"><i class="fa fa-check"></i><b>12.2.4</b> The value of a statistically significant result</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Bounds.html"><a href="Bounds.html"><i class="fa fa-check"></i><b>13</b> Bounds</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="proofs.html"><a href="proofs.html"><i class="fa fa-check"></i><b>A</b> Proofs</a><ul>
<li class="chapter" data-level="A.1" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-reffpsi"><i class="fa fa-check"></i><b>A.1</b> Proofs of results in Chapter @ref(FPSI)</a><ul>
<li class="chapter" data-level="A.1.1" data-path="proofs.html"><a href="proofs.html#proofcheb"><i class="fa fa-check"></i><b>A.1.1</b> Proof of Theorem @ref(thm:uppsampnoise)</a></li>
<li class="chapter" data-level="A.1.2" data-path="proofs.html"><a href="proofs.html#proofCLT"><i class="fa fa-check"></i><b>A.1.2</b> Proof of Theorem @ref(thm:asympnoiseWW)</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="proofs.html"><a href="proofs.html#proofs-of-results-in-chapter-refrct"><i class="fa fa-check"></i><b>A.2</b> Proofs of results in Chapter @ref(RCT)</a><ul>
<li class="chapter" data-level="A.2.1" data-path="proofs.html"><a href="proofs.html#proofIdentLATE"><i class="fa fa-check"></i><b>A.2.1</b> Proof of Theorem @ref(thm:IdentLATE)</a></li>
<li class="chapter" data-level="A.2.2" data-path="proofs.html"><a href="proofs.html#proofWaldIV"><i class="fa fa-check"></i><b>A.2.2</b> Proof of Theorem @ref(thm:WaldIV)</a></li>
<li class="chapter" data-level="A.2.3" data-path="proofs.html"><a href="proofs.html#ProofAsymWald"><i class="fa fa-check"></i><b>A.2.3</b> Proof of Theorem @ref(thm:asymWald)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Tools for Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="RCT" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Randomized Controlled Trials</h1>
<p>The most robust and rigorous method that has been devised by social scientists to estimate the effect of an intervention on an outcome is the Randomized Controlled Trial (RCT). RCTs are used extensively in the field to evaluate a wide array of programs, from development, labor and education interventions to environmental nudges to website and search engine features.</p>
<p>The key feature of an RCT is the introduction by the researcher of randomness in the allocation of the treatment. Individuals with <span class="math inline">\(R_i=1\)</span>, where <span class="math inline">\(R_i\)</span> denotes the outcome of a random event, such as a coin toss, have a higher probability of receiving the treatment. Potential outcomes have the same distribution in both <span class="math inline">\(R_i=1\)</span> and <span class="math inline">\(R_i=0\)</span> groups. If we observe different outcomes between the treatment and control group, it has to be because of the causal effect of the treatment, since both groups only differ by the proportion of treated and controls.</p>
<p>The most attractive feature of RCTs is that researchers enforce the main identification assumption (we do not have to assume that it holds, we can make sure that it does). This property of RCTs distinguishes them from all the other methods that we are going to learn in this class.</p>
<p>In this lecture, we are going to study how to estimate the effect of an intervention on an outcome using RCTs. We are especially going to study the various types of designs and what can be recovered from them using which technique. For each design, we are going to detail which treatment effect it enables us to identify, how to obtain a sample estimate of this treatment effect and how to estimate the associated sampling noise. The main substantial difference between these four designs are the types of treatment effect parameters that they enable us to recover. Sections <a href="RCT.html#sec:design1">3.1</a> to <a href="RCT.html#sec:design4">3.4</a> of this lecture introduces the four designs and how to analyze them.</p>
<p>Unfortunately, RCTs are not bullet proof. They suffer from problems that might make their estimates of causal effects badly biased. Section <a href="RCT.html#sec:threats">3.5</a> surveys the various threats and what we can do to try to minimize them.</p>
<div id="sec:design1" class="section level2">
<h2><span class="header-section-number">3.1</span> Brute Force Design</h2>
<p>In the Brute Force Design, eligible individuals are randomly assigned to the treatment irrespective of their willingness to accept it and have to comply with the assignment. This is a rather dumb procedure but it is very easy to analyze and that is why I start with it. With the Brute Force Design, you can recover the average effect of the treatment on the whole population. This parameter is generally called the Average Treatment Effect (ATE).</p>
<p>In this section, I am going to detail the assumptions required for the Brute Force Design to identify the ATE, how to form an estimator of the ATE and how to estimate its sampling noise.</p>
<div id="identification" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Identification</h3>
<p>In the Brute Force Design, we need two assumptions for the ATE to be identified in the population: Independence and Brute Force Validity.</p>

<div class="definition">
<span id="def:independence" class="definition"><strong>Definition 3.1  (Independence)  </strong></span>We assume that the allocation of the program is independent of potential outcomes:
<span class="math display">\[\begin{align*}
  R_i\Ind(Y_i^0,Y_i^1).
\end{align*}\]</span>
</div>

<p>Here, <span class="math inline">\(\Ind\)</span> codes for independence or random variables. Independence can be enforced by the randomized allocation.</p>
<p>We need a second assumption for the Brute Force Design to work:</p>

<div class="definition">
<span id="def:BF" class="definition"><strong>Definition 3.2  (Brute Force Validity)  </strong></span>We assume that the randomized allocation of the program is mandatory and does not interfere with how potential outcomes are generated:
<span class="math display">\[\begin{align*}
Y_i &amp; = 
  \begin{cases}
    Y_i^1 &amp; \text{ if } R_i=1  \\
    Y_i^0 &amp; \text{ if } R_i=0      
  \end{cases}
\end{align*}\]</span>
with <span class="math inline">\(Y_i^1\)</span> and <span class="math inline">\(Y_i^0\)</span> the same potential outcomes as defined in Lecture~0 with a routine allocation of the treatment.
</div>

<p>Under both Idependence and Brute Force Validity, we have the follwing result:</p>

<div class="theorem">
<span id="thm:BFATE" class="theorem"><strong>Theorem 3.1  (Identification in the Brute Force Design)  </strong></span>Under Assumptions <a href="RCT.html#def:independence">3.1</a> and <a href="RCT.html#def:BF">3.2</a>, the WW estimator identifies the Average Effect of the Treatment (ATE):
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW} &amp; = \Delta^Y_{ATE},
\end{align*}\]</span>
</div>

with:
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW} &amp; = \esp{Y_i|R_i=1} - \esp{Y_i|R_i=0} \\
  \Delta^Y_{ATE} &amp; = \esp{Y_i^1-Y_i^0}. 
\end{align*}\]</span>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW} &amp; = \esp{Y_i|R_i=1} - \esp{Y_i|R_i=0} \\
                &amp; = \esp{Y^1_i|R_i=1} - \esp{Y^0_i|R_i=0} \\
                &amp; = \esp{Y_i^1}-\esp{Y_i^0}\\
                &amp; = \esp{Y_i^1-Y_i^0},
\end{align*}\]</span>
where the first equality uses Assumption <a href="RCT.html#def:BF">3.2</a>, the second equality Assumption <a href="RCT.html#def:independence">3.1</a> and the last equality the linearity of the expectation operator.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> As you can see from Theorem <a href="RCT.html#thm:BFATE">3.1</a>, ATE is the average effect of the treatment on the whole population, those who would be eligible for it and those who would not. ATE differs from TT because the effect of the treatment might be correlated with treatment intake. It is possible that the treatment has a bigger (resp. smaller) effect on treated individuals. In that case, ATE is higher (resp. smaller) than TT.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Another related design is the Brute Force Design among Eligibles. In this design, you impose the treatment status only among eligibles, irrespective of whether they want the treatment or not. It can be operationalized using the selection rule used in Section <a href="RCT.html#sec:design2">3.2</a>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-67" class="example"><strong>Example 3.1  </strong></span>Let’s use the example to illustrate the concept of ATE. Let’s generate data with our usual parameter values without allocating the treatment yet:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">param &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">8</span>,.<span class="dv">5</span>,.<span class="dv">28</span>,<span class="dv">1500</span>,<span class="fl">0.9</span>,<span class="fl">0.01</span>,<span class="fl">0.05</span>,<span class="fl">0.05</span>,<span class="fl">0.05</span>,<span class="fl">0.1</span>)
<span class="kw">names</span>(param) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;barmu&quot;</span>,<span class="st">&quot;sigma2mu&quot;</span>,<span class="st">&quot;sigma2U&quot;</span>,<span class="st">&quot;barY&quot;</span>,<span class="st">&quot;rho&quot;</span>,<span class="st">&quot;theta&quot;</span>,<span class="st">&quot;sigma2epsilon&quot;</span>,<span class="st">&quot;sigma2eta&quot;</span>,<span class="st">&quot;delta&quot;</span>,<span class="st">&quot;baralpha&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
Ds[YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>]] &lt;-<span class="st"> </span><span class="dv">1</span> 
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)</code></pre></div>
<p>In the sample, the ATE is the average difference between <span class="math inline">\(y_i^1\)</span> and <span class="math inline">\(y_i^0\)</span>, or – the expectation operator being linear – the difference between average <span class="math inline">\(y_i^1\)</span> and average <span class="math inline">\(y_i^0\)</span>. In our sample, the former is equal to 0.179 and the latter to 0.179.</p>
<p>In the population, the ATE is equal to:</p>
<span class="math display">\[\begin{align*}
\Delta^y_{ATE} &amp; = \esp{Y_i^1-Y_i^0} \\
              &amp; = \esp{\alpha_i} \\
              &amp; = \bar{\alpha}+\theta\bar{\mu}.
\end{align*}\]</span>
<p>Let’s write a function to compute the value of the ATE and of TT (we derived the formula for TT in the previous lecture):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta.y.ate &lt;-<span class="st"> </span><span class="cf">function</span>(param){
  <span class="kw">return</span>(param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>])
}
delta.y.tt &lt;-<span class="st"> </span><span class="cf">function</span>(param){
  <span class="kw">return</span>(param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>((param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">*</span><span class="kw">dnorm</span>((<span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])<span class="op">-</span>param[<span class="st">&quot;barmu&quot;</span>])<span class="op">/</span>(<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))))<span class="op">/</span>(<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>])<span class="op">*</span><span class="kw">pnorm</span>((<span class="kw">log</span>(param[<span class="st">&quot;barY&quot;</span>])<span class="op">-</span>param[<span class="st">&quot;barmu&quot;</span>])<span class="op">/</span>(<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;sigma2U&quot;</span>]))))))
}</code></pre></div>
<p>In the population, with our parameter values, <span class="math inline">\(\Delta^y_{ATE}=\)</span> 0.18 and <span class="math inline">\(\Delta^y_{TT}=\)</span> 0.172. In our case, selection into the treatment is correlated with lower outcomes, so that <span class="math inline">\(TT\leq ATE\)</span>.</p>
<p>In order to implement the Brute Force Design in practice in a sample, we simply either draw a coin repeatedly for each member of the sample, assigning for example, all “heads” to the treatment and all “tails” to the control. Because it can be a little cumbersome, it is possible to replace the coin toss by a pseudo-Random Number Generator (RNG), which is is an algorithm that tries to mimic the properties of random draws. When generating the samples in the numerical exmples, I actually use a pseudo-RNG. For example, we can draw from a uniform distribution on <span class="math inline">\([0,1]\)</span> and allocate to the treatment all the individuals whose draw is smaller than 0.5:</p>
<span class="math display">\[\begin{align*}
  R_i^* &amp; \sim \mathcal{U}[0,1]\\
  R_i &amp; = 
  \begin{cases}
    1 &amp; \text{ if } R_i^*\leq .5 \\
    0 &amp; \text{ if } R_i^*&gt; .5 
  \end{cases}
\end{align*}\]</span>
<p>The advantage of using a uniform law is that you can set up proportions of treated and controls easily.</p>

<div class="example">
<span id="exm:unnamed-chunk-68" class="example"><strong>Example 3.2  </strong></span>In our numerical example, the following R code generates two random groups, one treated and one control, and imposes the Assumption of Brute Force Validity:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># randomized allocation of 50% of individuals</span>
Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>R<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>R<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)</code></pre></div>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> It is interesting to stop for one minute to think about how the Brute Force Design solves the FPCI. First, with the ATE, the counterfactual problem is more severe than in the case of the TT. In the routine mode of the program, where only eligible individuals receive the treatment, both parts of the ATE are unobserved:
</div>

<ul>
<li><span class="math inline">\(\esp{Y_i^1}\)</span> is unobserved since we only observe the expected value of outcomes for the treated <span class="math inline">\(\esp{Y_i^1|D_i=1}\)</span>, and they do not have to be the same.</li>
<li><span class="math inline">\(\esp{Y_i^0}\)</span> is unobserved since we only observe the expected value of outcomes for the untreated <span class="math inline">\(\esp{Y_i^0|D_i=0}\)</span>, and they do not have to be the same.</li>
</ul>
<p>What the Brute Force Design does, is that it allocates randomly one part of the sample to the treatment, so that we see <span class="math inline">\(\esp{Y_i^1|R_i=1}=\esp{Y_i^1}\)</span> and one part to the control so that we see <span class="math inline">\(\esp{Y_i^0|R_i=0}=\esp{Y_i^0}\)</span>.</p>
</div>
<div id="estimating-ate" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Estimating ATE</h3>
<div id="using-the-ww-estimator" class="section level4">
<h4><span class="header-section-number">3.1.2.1</span> Using the WW estimator</h4>
In order to estimate ATE in a sample where the treatment has been randomized using a Brute Force Design, we simply use the sample equivalent of the With/Without estimator:
<span class="math display">\[\begin{align*}
  \hat{\Delta}^Y_{WW} &amp; = \frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N Y_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N Y_i(1-R_i).
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-70" class="example"><strong>Example 3.3  </strong></span>In our numerical example, the WW estimator can be computed as follows in the sample:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta.y.ww &lt;-<span class="st"> </span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">0</span>])</code></pre></div>
<p>The WW estimator of the ATE in the sample is equal to 0.156. Let’s recall that the true value of ATE is 0.18 in the population and 0.179 in the sample.</p>
<p>We can also see in our example how the Brute Force Design approximates the counterfactual expectation <span class="math inline">\(\esp{y_i^1}\)</span> and its sample equivalent mean <span class="math inline">\(\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^1_i\)</span> by the observed mean in the treated sample <span class="math inline">\(\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N y_iR_i\)</span>. In our example, the sample value of the counterfactual mean potential outcome <span class="math inline">\(\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^1_i\)</span> is equal to 8.222 and the sample value of its observed counterpart is 8.209. Similarly, the sample value of the counterfactual mean potential outcome <span class="math inline">\(\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^0_i\)</span> is equal to 8.043 and the sample value of its observed counterpart is 8.054.</p>
</div>
<div id="using-ols" class="section level4">
<h4><span class="header-section-number">3.1.2.2</span> Using OLS</h4>
As we have seen in Lecture 0, the WW estimator is numerically identical to the OLS estimator of a linear regression of outcomes on treatment: The OLS coefficient <span class="math inline">\(\beta\)</span> in the following regression:
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta R_i + U_i
    \end{align*}\]</span>
<p>is the WW estimator.</p>

<div class="example">
<span id="exm:unnamed-chunk-71" class="example"><strong>Example 3.4  </strong></span>In our numerical example, we can run the OLS regression as follows:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.ols &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>R)</code></pre></div>
<p><span class="math inline">\(\hat{\Delta}^y_{OLS}=\)</span> 0.156 which is exactly equal, as expected, to the WW estimator: 0.156.</p>
</div>
<div id="using-ols-conditioning-on-covariates" class="section level4">
<h4><span class="header-section-number">3.1.2.3</span> Using OLS conditioning on covariates</h4>
The advantage of using OLS other the direct WW comparison is that it gives you a direct estimate of sampling noise (see next section) but also that it enables you to condition on additional covariates in the regression: The OLS coefficient <span class="math inline">\(\beta\)</span> in the following regression:
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta R_i + \gamma&#39; X_i + U_i
    \end{align*}\]</span>
<p>is a consistent (and even unbiased) estimate of the ATE.</p>
<p><strong><span style="font-variant: small-caps;">proof needed, especially assumption of linearity. Also, is interaction between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(R_i\)</span> needed?</span></strong></p>

<div class="example">
<span id="exm:unnamed-chunk-72" class="example"><strong>Example 3.5  </strong></span>In our numerical example, we can run the OLS regression conditioning on <span class="math inline">\(y_i^B\)</span> as follows:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.ols.yB &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>R <span class="op">+</span><span class="st"> </span>yB)</code></pre></div>
<p><span class="math inline">\(\hat{\Delta}^y_{OLSX}=\)</span> 0.177. Note that <span class="math inline">\(\hat{\Delta}^y_{OLSX}\neq\hat{\Delta}^y_{WW}\)</span>. There is no numerical equivalence between the two estimators.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Why would you want to condition on covariates in an RCT? Indeed, covariates should be balanced by randomization and thus there does not seem to be a rationale for conditioning on potential confounders, since there should be none. The main reason why we condition on covariates is to decrease sampling noise. Remember that sampling noise is due to imbalances between confounders in the treatment and control group. Since these imbalances are not systematic, the WW estimator is unbiased. We can also make the bias due to these unbalances as small as we want by choosing an adequate sample size (the WW estimator is consistent). But for a given sample size, these imbalances generate sampling noise around the true ATE. Conditioning on covariates helps decrease sampling noise by accounting for imbalances due to observed covariates. If observed covariates explain a large part of the variation in outcomes, conditioning on them is going to prevent a lot of sampling noise from occuring.
</div>


<div class="example">
<span id="exm:unnamed-chunk-74" class="example"><strong>Example 3.6  </strong></span>In order to make the gains in precision from conditioning on covariates apparent, let’s use Monte Carlo simulations of our numerical example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.brute.force.ww &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  Ds[YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>]] &lt;-<span class="st"> </span><span class="dv">1</span> 
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  <span class="co"># randomized allocation of 50% of individuals</span>
  Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
  R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>R<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>R<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  reg.y.R.ols &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>R)
  <span class="kw">return</span>(<span class="kw">c</span>(reg.y.R.ols<span class="op">$</span>coef[<span class="dv">2</span>],<span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.ols,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])))
}

simuls.brute.force.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.brute.force.ww &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.brute.force.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">colnames</span>(simuls.brute.force.ww) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;se&#39;</span>)
  <span class="kw">return</span>(simuls.brute.force.ww)
}

sf.simuls.brute.force.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">2</span><span class="op">*</span>ncpus)
  <span class="kw">sfLibrary</span>(sandwich)
  sim &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.brute.force.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;se&#39;</span>)
  <span class="kw">return</span>(sim)
}

Nsim &lt;-<span class="st"> </span><span class="dv">1000</span>
<span class="co">#Nsim &lt;- 10</span>
N.sample &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">1000</span>,<span class="dv">10000</span>,<span class="dv">100000</span>)
<span class="co">#N.sample &lt;- c(100,1000,10000)</span>
<span class="co">#N.sample &lt;- c(100,1000)</span>
<span class="co">#N.sample &lt;- c(100)</span>

simuls.brute.force.ww &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.brute.force.ww.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)
<span class="kw">names</span>(simuls.brute.force.ww) &lt;-<span class="st"> </span>N.sample</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.brute.force.ww.yB &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  Ds &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)
  Ds[YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>]] &lt;-<span class="st"> </span><span class="dv">1</span> 
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  <span class="co"># randomized allocation of 50% of individuals</span>
  Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
  R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>R<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>R<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  reg.y.R.yB.ols &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>R <span class="op">+</span><span class="st"> </span>yB)
  <span class="kw">return</span>(<span class="kw">c</span>(reg.y.R.yB.ols<span class="op">$</span>coef[<span class="dv">2</span>],<span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.yB.ols,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])))
}

simuls.brute.force.ww.yB.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.brute.force.ww.yB &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.brute.force.ww.yB,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">colnames</span>(simuls.brute.force.ww.yB) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;se&#39;</span>)
  <span class="kw">return</span>(simuls.brute.force.ww.yB)
}

sf.simuls.brute.force.ww.yB.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">2</span><span class="op">*</span>ncpus)
  <span class="kw">sfLibrary</span>(sandwich)
  sim &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.brute.force.ww.yB,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>))
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;se&#39;</span>)
  <span class="kw">return</span>(sim)
}

Nsim &lt;-<span class="st"> </span><span class="dv">1000</span>
<span class="co">#Nsim &lt;- 10</span>
N.sample &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">1000</span>,<span class="dv">10000</span>,<span class="dv">100000</span>)
<span class="co">#N.sample &lt;- c(100,1000,10000)</span>
<span class="co">#N.sample &lt;- c(100,1000)</span>
<span class="co">#N.sample &lt;- c(100)</span>

simuls.brute.force.ww.yB &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.brute.force.ww.yB.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)
<span class="kw">names</span>(simuls.brute.force.ww.yB) &lt;-<span class="st"> </span>N.sample</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:montecarlohistbruteforcewwyB"></span>
<img src="STCI_files/figure-html/montecarlohistbruteforcewwyB-1.png" alt="Distribution of the $WW$ and $OLSX$ estimators in a Brute Force design over replications of samples of different sizes" width="50%" /><img src="STCI_files/figure-html/montecarlohistbruteforcewwyB-2.png" alt="Distribution of the $WW$ and $OLSX$ estimators in a Brute Force design over replications of samples of different sizes" width="50%" />
<p class="caption">
Figure 3.1: Distribution of the <span class="math inline">\(WW\)</span> and <span class="math inline">\(OLSX\)</span> estimators in a Brute Force design over replications of samples of different sizes
</p>
</div>
<p>Figure <a href="RCT.html#fig:montecarlohistbruteforcewwyB">3.1</a> shows that the gains in precision from conditioning on <span class="math inline">\(y_i^B\)</span> are spectacular in our numerical example. They basically correspond to a gain in one order of magnitude of sample size: the precision of the <span class="math inline">\(OLSX\)</span> estimator conditioning on <span class="math inline">\(y_i^B\)</span> with a sample size of 100 is similar to the precision of the <span class="math inline">\(OLS\)</span> estimator not conditioning on <span class="math inline">\(y_i^B\)</span> with a sample size of <span class="math inline">\(1000\)</span>. This large gain in precision is largely due to the fact that <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_i^B\)</span> are highly correlated. Not all covariates perform so well in actual samples in the social sciences.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> The ability to condition on covariates in order to decrease sampling noise is a blessing but can also be a curse when combined with significance testing. Indeed, you can now see that you can run a lot of regressions (with and without some covariates, interactions, etc) and maybe report only the statistically significant ones. This is a bad practice that will lead to publication bias and inflated treatment effects. Several possibilities in order to avoid that:
</div>

<ol style="list-style-type: decimal">
<li>Pre-register your analysis and explain which covariates you are going to use (with which interactions, etc) so that you cannot cherry pick your favorite results ex-post.</li>
<li>Use a stratified design for your RCT (more on this in Lecture 6) so that the important covariates are already balanced between treated and controls.</li>
<li>If unable to do all of the above, report results from regressions without controls and with various sets of controls. We do not expect the various treatment effect estimates to be the same (they cannot be, otherwise, they would have similar sampling noise), but we expect the following pattern: conditioning should systematically decrease sampling noise, not increase the treatment effect estimate. If conditioning on covariates makes a treatment effect significant, pay attention to why: is it because of a decrease in sampling noise (expected and OK) or because of an increase in treatment effect (beware specification search).</li>
</ol>
<p><strong><span style="font-variant: small-caps;">Revise that especially in light of Chapter <a href="sec-meta.html#sec:meta">12</a></span></strong></p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> You might not be happy with the assumption of linearity needed to use OLS to control for covariates. I have read somewhere (forgot where) that this should not be much of a problem since covariates are well balanced between groups by randomization, and thus a linear first approximation to the function relating <span class="math inline">\(X_i\)</span> to <span class="math inline">\(Y_i\)</span> should be fine. I tend not to buy that argument much. I have to run simulations with a non linear relation between outcomes and controls and see how linear OLS performs. If you do not like the linearity assumption, you can always use any of the nonparametric observational methods presented in Chapter <a href="sec-OM.html#sec:OM">5</a>.
</div>

</div>
<div id="estimating-sampling-noise" class="section level4">
<h4><span class="header-section-number">3.1.2.4</span> Estimating Sampling Noise</h4>
<p>In order to estimate sampling noise, you can either use the CLT-based approach or resampling, either using the bootstrap or randomization inference. In Section <a href="FPSI.html#sec:estimsampnoise">2.2</a>, we have already discussed how to estimate sampling noise when using the WW estimator that we are using here. We are going to use the default and heteroskedasticity-robust standard errors from OLS, which are both CLT-based. Only the heteroskedasticity-robust standard errors are valid under the assumptions that we have made so far. Homoskedasticity would require constant treatment effects. Heteroskedasticity being small in our numerical example, that should not matter much, but it could in other applications.</p>

<div class="example">
<span id="exm:unnamed-chunk-77" class="example"><strong>Example 3.7  </strong></span>Let us first estimate sampling noise for the simple WW estimator without control variables, using the OLS estimator.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.BF.simuls &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.brute.force.ww[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;WW&#39;</span>]<span class="op">-</span><span class="kw">delta.y.ate</span>(param)),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.BF.OLS.hetero &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.ols,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.BF.OLS.homo &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.ols)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>The true value of the 99% sampling noise with a sample size of 1000 and no control variables is stemming from the simulations is 0.274. The 99% sampling noise estimated using heteroskedasticity robust OLS standard errors is 0.295. The 99% sampling noise estimated using default OLS standard errors is 0.294.</p>
<p>Let us now estimate sampling noise for the simple WW estimator conditioning on <span class="math inline">\(y_i^B\)</span>, using the OLS estimator.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.BF.simuls.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.brute.force.ww.yB[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;WW&#39;</span>]<span class="op">-</span><span class="kw">delta.y.ate</span>(param)),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.BF.OLS.hetero.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.ols.yB,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.BF.OLS.homo.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.ols.yB)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>The true value of the 99% sampling noise with a sample size of 1000 and no control variables is stemming from the simulations is 0.088. The 99% sampling noise estimated using heteroskedasticity robust OLS standard errors is 0.092. The 99% sampling noise estimated using default OLS standard errors is 0.091.</p>
<p>Let’s see how all of this works on average. Figure <a href="RCT.html#fig:sampnoisewwBFCLTplot">3.2</a> shows that overall the sampling nois eis much lower with <span class="math inline">\(OLSX\)</span> than with <span class="math inline">\(WW\)</span>, as expected from Figure <a href="RCT.html#fig:montecarlohistbruteforcewwyB">3.1</a>. The CLT-based estimator of sampling noise accounting for heteroskedasticity (in blue) recovers true sampling noise (in red) pretty well. Figure <a href="RCT.html#fig:sampnoisewwBFCLTall">3.3</a> shows that the CLT-based estimates of sampling noise are on point, except for <span class="math inline">\(N=10000\)</span>, where the CLT slightly overestimates true sampling noise. Figure <a href="RCT.html#fig:confintervalCLTBF">3.4</a> shows what happens when conditioning on <span class="math inline">\(Y^B\)</span> in a selection of 40 samples. The reduction in samplong noise is pretty drastic here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  simuls.brute.force.ww[[k]]<span class="op">$</span>CLT.noise &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span>simuls.brute.force.ww[[k]][,<span class="st">&#39;se&#39;</span>]
  simuls.brute.force.ww.yB[[k]]<span class="op">$</span>CLT.noise &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((delta<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span>simuls.brute.force.ww.yB[[k]][,<span class="st">&#39;se&#39;</span>]
}

samp.noise.ww.BF &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.brute.force.ww,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),samp.noise,<span class="dt">delta=</span>delta)
precision.ww.BF &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.brute.force.ww,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),precision,<span class="dt">delta=</span>delta)
<span class="kw">names</span>(precision.ww.BF) &lt;-<span class="st"> </span>N.sample
signal.to.noise.ww.BF &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.brute.force.ww,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),signal.to.noise,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)
<span class="kw">names</span>(signal.to.noise.ww.BF) &lt;-<span class="st"> </span>N.sample
table.noise.BF &lt;-<span class="st"> </span><span class="kw">cbind</span>(samp.noise.ww.BF,precision.ww.BF,signal.to.noise.ww.BF)
<span class="kw">colnames</span>(table.noise.BF) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;sampling.noise&#39;</span>, <span class="st">&#39;precision&#39;</span>, <span class="st">&#39;signal.to.noise&#39;</span>)
table.noise.BF &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(table.noise.BF)
table.noise.BF<span class="op">$</span>N &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(N.sample)
table.noise.BF<span class="op">$</span>ATE &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">delta.y.ate</span>(param),<span class="kw">nrow</span>(table.noise.BF))
<span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  table.noise.BF<span class="op">$</span>CLT.noise[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(simuls.brute.force.ww[[k]]<span class="op">$</span>CLT.noise)
}
table.noise.BF<span class="op">$</span>Method &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;WW&quot;</span>,<span class="kw">nrow</span>(table.noise.BF))

samp.noise.ww.BF.yB &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.brute.force.ww.yB,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),samp.noise,<span class="dt">delta=</span>delta)
precision.ww.BF.yB &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.brute.force.ww.yB,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),precision,<span class="dt">delta=</span>delta)
<span class="kw">names</span>(precision.ww.BF.yB) &lt;-<span class="st"> </span>N.sample
signal.to.noise.ww.BF.yB &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">lapply</span>(simuls.brute.force.ww.yB,<span class="st">`</span><span class="dt">[</span><span class="st">`</span>,,<span class="dv">1</span>),signal.to.noise,<span class="dt">delta=</span>delta,<span class="dt">param=</span>param)
<span class="kw">names</span>(signal.to.noise.ww.BF.yB) &lt;-<span class="st"> </span>N.sample
table.noise.BF.yB &lt;-<span class="st"> </span><span class="kw">cbind</span>(samp.noise.ww.BF.yB,precision.ww.BF.yB,signal.to.noise.ww.BF.yB)
<span class="kw">colnames</span>(table.noise.BF.yB) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;sampling.noise&#39;</span>, <span class="st">&#39;precision&#39;</span>, <span class="st">&#39;signal.to.noise&#39;</span>)
table.noise.BF.yB &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(table.noise.BF.yB)
table.noise.BF.yB<span class="op">$</span>N &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(N.sample)
table.noise.BF.yB<span class="op">$</span>ATE &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">delta.y.ate</span>(param),<span class="kw">nrow</span>(table.noise.BF.yB))
<span class="cf">for</span> (k <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample))){
  table.noise.BF.yB<span class="op">$</span>CLT.noise[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(simuls.brute.force.ww.yB[[k]]<span class="op">$</span>CLT.noise)
}
table.noise.BF.yB<span class="op">$</span>Method &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;OLSX&quot;</span>,<span class="kw">nrow</span>(table.noise.BF))

table.noise.BF.tot &lt;-<span class="st"> </span><span class="kw">rbind</span>(table.noise.BF,table.noise.BF.yB)
table.noise.BF.tot<span class="op">$</span>Method &lt;-<span class="st"> </span><span class="kw">factor</span>(table.noise.BF.tot<span class="op">$</span>Method,<span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;WW&quot;</span>,<span class="st">&quot;OLSX&quot;</span>))

<span class="kw">ggplot</span>(table.noise.BF.tot, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(N), <span class="dt">y=</span>ATE,<span class="dt">fill=</span>Method)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>ATE<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>ATE<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>ATE<span class="op">-</span>CLT.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>ATE<span class="op">+</span>CLT.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Sample Size&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="kw">c</span>(<span class="fl">0.85</span>,<span class="fl">0.88</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwBFCLTplot"></span>
<img src="STCI_files/figure-html/sampnoisewwBFCLTplot-1.png" alt="Average CLT-based approximations of sampling noise in the Brute Force design for $WW$ and $OLSX$ over replications of samples of different sizes (true sampling noise in red)" width="50%" />
<p class="caption">
Figure 3.2: Average CLT-based approximations of sampling noise in the Brute Force design for <span class="math inline">\(WW\)</span> and <span class="math inline">\(OLSX\)</span> over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(simuls.brute.force.ww)){
  <span class="kw">hist</span>(simuls.brute.force.ww[[i]][,<span class="st">&#39;CLT.noise&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">bar</span>(epsilon))[WW]),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="kw">min</span>(table.noise.BF[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="kw">min</span>(simuls.brute.force.ww[[i]][,<span class="st">&#39;CLT.noise&#39;</span>])),<span class="kw">max</span>(table.noise.BF[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="kw">max</span>(simuls.brute.force.ww[[i]][,<span class="st">&#39;CLT.noise&#39;</span>]))))
  <span class="kw">abline</span>(<span class="dt">v=</span>table.noise.BF[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(simuls.brute.force.ww.yB)){
  <span class="kw">hist</span>(simuls.brute.force.ww.yB[[i]][,<span class="st">&#39;CLT.noise&#39;</span>],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(<span class="dv">2</span><span class="op">*</span><span class="kw">bar</span>(epsilon))[OLSX]),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="kw">min</span>(table.noise.BF.yB[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="kw">min</span>(simuls.brute.force.ww.yB[[i]][,<span class="st">&#39;CLT.noise&#39;</span>])),<span class="kw">max</span>(table.noise.BF.yB[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="kw">max</span>(simuls.brute.force.ww.yB[[i]][,<span class="st">&#39;CLT.noise&#39;</span>]))))
  <span class="kw">abline</span>(<span class="dt">v=</span>table.noise.BF.yB[i,<span class="kw">colnames</span>(table.noise)<span class="op">==</span><span class="st">&#39;sampling.noise&#39;</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:sampnoisewwBFCLTall"></span>
<img src="STCI_files/figure-html/sampnoisewwBFCLTall-1.png" alt="Distribution of the CLT approximation of sampling noise in the Brute Force design for $WW$ and $OLSX$ over replications of samples of different sizes (true sampling noise in red)" width="50%" /><img src="STCI_files/figure-html/sampnoisewwBFCLTall-2.png" alt="Distribution of the CLT approximation of sampling noise in the Brute Force design for $WW$ and $OLSX$ over replications of samples of different sizes (true sampling noise in red)" width="50%" />
<p class="caption">
Figure 3.3: Distribution of the CLT approximation of sampling noise in the Brute Force design for <span class="math inline">\(WW\)</span> and <span class="math inline">\(OLSX\)</span> over replications of samples of different sizes (true sampling noise in red)
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N.plot &lt;-<span class="st"> </span><span class="dv">40</span>
plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()
limx &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="op">-</span><span class="fl">0.65</span>,<span class="fl">1.25</span>),<span class="kw">c</span>(<span class="op">-</span><span class="fl">0.1</span>,<span class="fl">0.5</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.30</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.25</span>))

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test.CLT.BF &lt;-<span class="st"> </span>simuls.brute.force.ww[[k]][<span class="kw">sample</span>(N.plot),<span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;CLT.noise&#39;</span>)]
  test.CLT.BF &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.CLT.BF,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.brute.force.ww[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta),N.plot)))
  <span class="kw">colnames</span>(test.CLT.BF) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;CLT.noise&#39;</span>,<span class="st">&#39;sampling.noise&#39;</span>)
  test.CLT.BF<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
  plot.test.CLT.BF &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.CLT.BF, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>CLT.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>CLT.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">ylim</span>(limx[[k]][<span class="dv">1</span>],limx[[k]][<span class="dv">2</span>])<span class="op">+</span>
<span class="st">      </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
  plot.list[[k]] &lt;-<span class="st"> </span>plot.test.CLT.BF
}
plot.CI.BF &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI.BF)

plot.list &lt;-<span class="st"> </span><span class="kw">list</span>()
<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(N.sample)){
  <span class="kw">set.seed</span>(<span class="dv">1234</span>)
  test.CLT.BF.yB &lt;-<span class="st"> </span>simuls.brute.force.ww.yB[[k]][<span class="kw">sample</span>(N.plot),<span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;CLT.noise&#39;</span>)]
  test.CLT.BF.yB &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(test.CLT.BF.yB,<span class="kw">rep</span>(<span class="kw">samp.noise</span>(simuls.brute.force.ww.yB[[k]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">delta=</span>delta),N.plot)))
  <span class="kw">colnames</span>(test.CLT.BF.yB) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>,<span class="st">&#39;CLT.noise&#39;</span>,<span class="st">&#39;sampling.noise&#39;</span>)
  test.CLT.BF.yB<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N.plot
  plot.test.CLT.BF.yB &lt;-<span class="st"> </span><span class="kw">ggplot</span>(test.CLT.BF.yB, <span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.factor</span>(id), <span class="dt">y=</span>WW)) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_bar</span>(<span class="dt">position=</span><span class="kw">position_dodge</span>(), <span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>, <span class="dt">colour=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>sampling.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>sampling.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin=</span>WW<span class="op">-</span>CLT.noise<span class="op">/</span><span class="dv">2</span>, <span class="dt">ymax=</span>WW<span class="op">+</span>CLT.noise<span class="op">/</span><span class="dv">2</span>), <span class="dt">width=</span>.<span class="dv">2</span>,<span class="dt">position=</span><span class="kw">position_dodge</span>(.<span class="dv">9</span>),<span class="dt">color=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_hline</span>(<span class="kw">aes</span>(<span class="dt">yintercept=</span><span class="kw">delta.y.ate</span>(param)), <span class="dt">colour=</span><span class="st">&quot;#990000&quot;</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">ylim</span>(limx[[k]][<span class="dv">1</span>],limx[[k]][<span class="dv">2</span>])<span class="op">+</span>
<span class="st">      </span><span class="kw">xlab</span>(<span class="st">&quot;Sample id&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">ylab</span>(<span class="st">&quot;OLSX&quot;</span>)<span class="op">+</span>
<span class="st">      </span><span class="kw">theme_bw</span>()<span class="op">+</span>
<span class="st">      </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;N=&quot;</span>,N.sample[k]))
  plot.list[[k]] &lt;-<span class="st"> </span>plot.test.CLT.BF.yB
}
plot.CI.BF.yB &lt;-<span class="st"> </span><span class="kw">plot_grid</span>(plot.list[[<span class="dv">1</span>]],plot.list[[<span class="dv">2</span>]],plot.list[[<span class="dv">3</span>]],plot.list[[<span class="dv">4</span>]],<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">nrow=</span><span class="kw">length</span>(N.sample))
<span class="kw">print</span>(plot.CI.BF.yB)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:confintervalCLTBF"></span>
<img src="STCI_files/figure-html/confintervalCLTBF-1.png" alt="CLT-based confidence intervals of $\hat{WW}$ and $\hat{OLSX}$ for $\delta=$ 0.99 over sample replications for various sample sizes (true confidence intervals in red)" width="50%" /><img src="STCI_files/figure-html/confintervalCLTBF-2.png" alt="CLT-based confidence intervals of $\hat{WW}$ and $\hat{OLSX}$ for $\delta=$ 0.99 over sample replications for various sample sizes (true confidence intervals in red)" width="50%" />
<p class="caption">
Figure 3.4: CLT-based confidence intervals of <span class="math inline">\(\hat{WW}\)</span> and <span class="math inline">\(\hat{OLSX}\)</span> for <span class="math inline">\(\delta=\)</span> 0.99 over sample replications for various sample sizes (true confidence intervals in red)
</p>
</div>
</div>
</div>
</div>
<div id="sec:design2" class="section level2">
<h2><span class="header-section-number">3.2</span> Randomization After Self-Selection</h2>
<p>In Randomization After Self-Selection, individuals are randomly assigned to the treatment after having expressed their willingness to receive it. This design is able to recover the average effect of the Treatment on the Treated (TT).</p>
<p>In order to explain this design clearly, and especially to make it clear how it differs from the following one (randomization after eligibility), I have to introduce a slightly more complex selection rule that we have seen so far, one that includes self-selection,  take-up decisions by agents. We are going to assume that there are two steps in agents’ participation process:</p>
<ul>
<li>Eligibility: agents’ eligibility is assessed first, giving rise to a group of eligible individuals (<span class="math inline">\(E_i=1\)</span>) and a group of non eligible individuals (<span class="math inline">\(E_i=0\)</span>).</li>
<li>Self-selection: eligible agents can then decide whether they want to take-up the proposed treatment or not. <span class="math inline">\(D_i=1\)</span> for those who do. <span class="math inline">\(D_i=0\)</span> for those who do not. By convention, ineligibles have <span class="math inline">\(D_i=0\)</span>.</li>
</ul>

<div class="example">
<span id="exm:unnamed-chunk-78" class="example"><strong>Example 3.8  </strong></span>In our numerical example, here are the equations operationalizing these notions:
</div>

<span class="math display">\[\begin{align*}
E_i &amp; = \uns{y_i^B\leq\bar{y}} \\
D_i &amp; = \uns{\underbrace{\bar{\alpha}+\theta\bar{\mu}-C_i}_{D_i^*}\geq0 \land E_i=1} \\
C_i &amp; = \bar{c} + \gamma \mu_i + V_i\\
V_i &amp; \sim \mathcal{N}(0,\sigma^2_V)
\end{align*}\]</span>
<p>Eligibility is still decided based on pre-treatment outcomes being smaller than a threshold level <span class="math inline">\(\bar{y}\)</span>. Self-selection among eligibles is decided by the net utility of the treatment <span class="math inline">\(D_i^*\)</span> being positive. Here, the net utility is composed of the average gain from the treatment (assuming agents cannot foresee their idiosyncratic gain from the treatment) <span class="math inline">\(\bar{\alpha}+\theta\bar{\mu}\)</span> minus the cost of participation <span class="math inline">\(C_i\)</span>. The cost of participation in turn depends on a constant, on <span class="math inline">\(\mu_i\)</span> and on a random shock orthogonal to everything else <span class="math inline">\(V_i\)</span>. This cost might represent the administrative cost of applying for the treatment and the opportunity cost of participating into the treatment (foregone earnings and/or cost of time). Conditional on eligiblity, self-selection is endogenous in this model since both the gains and the cost of participation depend on <span class="math inline">\(\mu_i\)</span>. Costs depend on <span class="math inline">\(\mu_i\)</span> since most productive people may face lower administrative costs but a higher opportunity cost of time.</p>
<p>Let’s choose some values for the new parameters:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">param &lt;-<span class="st"> </span><span class="kw">c</span>(param,<span class="op">-</span><span class="fl">6.25</span>,<span class="fl">0.9</span>,<span class="fl">0.5</span>)
<span class="kw">names</span>(param) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;barmu&quot;</span>,<span class="st">&quot;sigma2mu&quot;</span>,<span class="st">&quot;sigma2U&quot;</span>,<span class="st">&quot;barY&quot;</span>,<span class="st">&quot;rho&quot;</span>,<span class="st">&quot;theta&quot;</span>,<span class="st">&quot;sigma2epsilon&quot;</span>,<span class="st">&quot;sigma2eta&quot;</span>,<span class="st">&quot;delta&quot;</span>,<span class="st">&quot;baralpha&quot;</span>,<span class="st">&quot;barc&quot;</span>,<span class="st">&quot;gamma&quot;</span>,<span class="st">&quot;sigma2V&quot;</span>)</code></pre></div>
<p>and let’s generate a new dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
Dstar &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dstar<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)</code></pre></div>
<p>Let’s compute the value of the TT parameter in this new model:</p>
<span class="math display">\[\begin{align*}
  \Delta^y_{TT} &amp; = \bar{\alpha}+ \theta\esp{\mu_i|\mu_i+U_i^B\leq\bar{y} \land \bar{\alpha}+\theta\bar{\mu}-\bar{c}-\gamma\mu_i-V_i\geq0}
\end{align*}\]</span>
<p>To compute the expectation of a doubly censored normal, I use the package <code>tmvtnorm</code>.</p>
<span class="math display">\[\begin{align*}
  (\mu_i,y_i^B,D_i^*) &amp; = \mathcal{N}\left(\bar{\mu},\bar{\mu},\bar{\alpha}+(\theta-\gamma)\bar{\mu}-\bar{c},
                                        \left(\begin{array}{ccc}
                                              \sigma^2_{\mu} &amp; \sigma^2_{\mu} &amp; -\gamma\sigma^2_{\mu} \\
                                              \sigma^2_{\mu} &amp; \sigma^2_{\mu} + \sigma^2_{U} &amp; -\gamma\sigma^2_{\mu} \\
                                              -\gamma\sigma^2_{\mu} &amp; -\gamma\sigma^2_{\mu} &amp; \gamma^2\sigma^2_{\mu}+\sigma^2_{V}
                                              \end{array}
                                        \right)
                                      \right)
\end{align*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean.mu.yB.Dstar &lt;-<span class="st"> </span><span class="kw">c</span>(param[<span class="st">&#39;barmu&#39;</span>],param[<span class="st">&#39;barmu&#39;</span>],param[<span class="st">&#39;baralpha&#39;</span>]<span class="op">-</span><span class="st"> </span>param[<span class="st">&#39;barc&#39;</span>]<span class="op">+</span>(param[<span class="st">&#39;theta&#39;</span>]<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>])<span class="op">*</span>param[<span class="st">&#39;barmu&#39;</span>])
cov.mu.yB.Dstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(param[<span class="st">&#39;sigma2mu&#39;</span>],param[<span class="st">&quot;sigma2mu&quot;</span>],<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],
                            param[<span class="st">&quot;sigma2mu&quot;</span>],param[<span class="st">&#39;sigma2mu&#39;</span>]<span class="op">+</span>param[<span class="st">&#39;sigma2U&#39;</span>],<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],
                            <span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">*</span>(param[<span class="st">&#39;gamma&#39;</span>])<span class="op">^</span><span class="dv">2</span><span class="op">+</span>param[<span class="st">&#39;sigma2V&#39;</span>]),<span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
lower.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span><span class="ot">Inf</span>,<span class="dv">0</span>)
upper.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">Inf</span>,<span class="kw">log</span>(param[<span class="st">&#39;barY&#39;</span>]),<span class="ot">Inf</span>)
moments.cut &lt;-<span class="st"> </span><span class="kw">mtmvnorm</span>(<span class="dt">mean=</span>mean.mu.yB.Dstar,<span class="dt">sigma=</span>cov.mu.yB.Dstar,<span class="dt">lower=</span>lower.cut,<span class="dt">upper=</span>upper.cut)
delta.y.tt &lt;-<span class="st"> </span>param[<span class="st">&#39;baralpha&#39;</span>]<span class="op">+</span><span class="st"> </span>param[<span class="st">&#39;theta&#39;</span>]<span class="op">*</span>moments.cut<span class="op">$</span>tmean[<span class="dv">1</span>]
delta.y.ww.self.select &lt;-<span class="st"> </span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>The value of <span class="math inline">\(\Delta^y_{TT}\)</span> in our illustration is now 0.17.</p>
<div id="identification-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Identification</h3>
<p>With Randomization After Self-Selection, identification requires two assumptions:</p>

<div class="definition">
<p><span id="def:indepss" class="definition"><strong>Definition 3.3  (Independence Among Self-Selected)  </strong></span>We assume that the randomized allocation of the program among applicants is well done:</p>
<span class="math display">\[\begin{align*}
  R_i\Ind(Y_i^0,Y_i^1)|D_i=1.
\end{align*}\]</span>
</div>

<p>Independence can be enforced by the randomized allocation of the treatment among the eligible applicants.</p>
<p>We need a second assumption:</p>

<div class="definition">
<p><span id="def:rassval" class="definition"><strong>Definition 3.4  (Randomization After Self-Selection Validity)  </strong></span>We assume that the randomized allocation of the program does not interfere with how potential outcomes and self-selection are generated:</p>
<span class="math display">\[\begin{align*}
Y_i &amp; = 
  \begin{cases}
    Y_i^1 &amp; \text{ if } (R_i=1 \text{ and } D_i=1)   \\
    Y_i^0 &amp; \text{ if } (R_i=0 \text{ and } D_i=1) \text{ or } D_i=0
  \end{cases}
\end{align*}\]</span>
</div>

<p>with <span class="math inline">\(Y_i^1\)</span>, <span class="math inline">\(Y_i^0\)</span> and <span class="math inline">\(D_i\)</span> the same potential outcomes and self-selection decisions as in a routine allocation of the treatment.</p>
<p>Under these assumptions, we have the following result:</p>

<div class="theorem">
<p><span id="thm:idrass" class="theorem"><strong>Theorem 3.2  (Identification With Randomization After Self-Selection)  </strong></span>Under Assumptions <a href="RCT.html#def:indepss">3.3</a> and <a href="RCT.html#def:rassval">3.4</a>, the WW estimator among the self-selected identifies TT:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW|D=1} &amp; = \Delta^Y_{TT},
\end{align*}\]</span>
</div>

<p>with:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW|D=1} &amp; = \esp{Y_i|R_i=1,D_i=1} - \esp{Y_i|R_i=0,D_i=1}.
\end{align*}\]</span>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW|D=1} &amp; = \esp{Y_i|R_i=1,D_i=1} - \esp{Y_i|R_i=0,D_i=1} \\
                    &amp; = \esp{Y^1_i|R_i=1,D_i=1} - \esp{Y^0_i|R_i=0,D_i=1} \\
                    &amp; = \esp{Y_i^1|D_i=1}-\esp{Y_i^0|D_i=1}\\
                    &amp; = \esp{Y_i^1-Y_i^0|D_i=1},
\end{align*}\]</span>
where the second equality uses Randomization After Self-Selection Validity, the third equality Independence Among Self-Selected and the last equality the linearity of the expectation operator.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> The key intuitions for how Randomization After Self-Selection solves the FPCI are:
</div>

<ul>
<li>By allowing for eligibilty and self-selection, we identify the agents that would benefit from the treatment in routine mode (the treated).</li>
<li>By randomly denying the treatment to some of the treated, we can estimate the counterfactual outcome of the treated by looking at the counterfactual outcome of the denied applicants: <span class="math inline">\(\esp{Y_i^0|D_i=1}=\esp{Y_i|R_i=0,D_i=1}\)</span>.</li>
</ul>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> In practice, we use a pseudo-RNG to generate a random allocation among applicants:
</div>

<span class="math display">\[\begin{align*}
  R_i^* &amp; \sim \mathcal{U}[0,1]\\
  R_i &amp; = 
  \begin{cases}
    1 &amp; \text{ if } R_i^*\leq .5 \land D_i=1\\
    0 &amp; \text{ if } R_i^*&gt; .5 \land D_i=1
  \end{cases}
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-82" class="example"><strong>Example 3.9  </strong></span>In our numerical example, the following R code generates two random groups, one treated and one control, and imposes the Assumption of Randomization After Self-Selection Validity:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#random allocation among self-selected</span>
Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>R<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>R<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)</code></pre></div>
</div>
<div id="estimating-tt" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Estimating TT</h3>
<div id="using-the-ww-estimator-1" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Using the WW Estimator</h4>
<p>As in the case of the Brute Force Design, we can use the WW estimator to estimate the effect of the program with Randomization After Self-Selection, except that this time the WW estimator is applied among applicant to the program only:</p>
<span class="math display">\[\begin{align*}
  \hat{\Delta}^Y_{WW|D=1} &amp; = \frac{1}{\sum_{i=1}^N D_iR_i}\sum_{i=1}^N Y_iD_iR_i-\frac{1}{\sum_{i=1}^N D_i(1-R_i)}\sum_{i=1}^N D_iY_i(1-R_i).
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-83" class="example"><strong>Example 3.10  </strong></span>In our numerical example, we can form the WW estimator among applicants:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta.y.ww.self.select &lt;-<span class="st"> </span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>WW among applicants is equal to 0.085. It is actually rather far from the true value of 0.17, which reminds us that unbiasedness does not mean that a given sample will not suffer from a large bias. We just drew a bad sample where confounders are not very well balanced.</p>
</div>
<div id="using-ols-1" class="section level4">
<h4><span class="header-section-number">3.2.2.2</span> Using OLS</h4>
<p>As in the Brute Force Design with the ATE, we can estimate the TT parameter with Randomization After Self-Selection using the OLS estimator. In the following regression run among applicants only (with <span class="math inline">\(D_i=1\)</span>), <span class="math inline">\(\beta\)</span> estimates TT:</p>
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta R_i + U_i.
    \end{align*}\]</span>
<p>As a matter of fact, the OLS estimator without control variables is numerically equivalent to the WW estimator.</p>

<div class="example">
<span id="exm:unnamed-chunk-84" class="example"><strong>Example 3.11  </strong></span>In our numerical example, here is the OLS regression:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.ols.self.select &lt;-<span class="st"> </span><span class="kw">lm</span>(y[Ds<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>R[Ds<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>The value of the OLS estimator is 0.085, which is identical to the WW estimator among applicants.</p>
</div>
<div id="using-ols-conditioning-on-covariates-1" class="section level4">
<h4><span class="header-section-number">3.2.2.3</span> Using OLS Conditioning on Covariates</h4>
<p>We might want to condition on covariates in order to reduce the amount of sampling noise. Parametrically, we can run the following OLS regression among applicants (with <span class="math inline">\(D_i=1\)</span>):</p>
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta R_i + \gamma&#39; X_i + U_i.
\end{align*}\]</span>
<p><span class="math inline">\(\beta\)</span> estimates the TT.</p>
<p><strong><span style="font-variant: small-caps;">Needed: proof. Especially check whether we need to center covariates at the mean of the treatment group. I think so.</span></strong></p>
<p>We can also use Matching to obtain a nonparametric estimator.</p>

<div class="example">
<span id="exm:unnamed-chunk-85" class="example"><strong>Example 3.12  </strong></span>Let us first compute the OLS estimator conditioning on <span class="math inline">\(y_i^B\)</span>:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.yB.ols.self.select &lt;-<span class="st"> </span><span class="kw">lm</span>(y[Ds<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>R[Ds<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[Ds<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>Our estimate of TT after conditioning on <span class="math inline">\(y_i^B\)</span> is 0.145. Conditioning on <span class="math inline">\(y_i^B\)</span> has been able to solve part of the bias of the WW problem estimator.</p>
<p>Let’s now check whether conditioning on OLS has brought an improvement in terms of decreased sampling noise.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.self.select.ww &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
  Dstar &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
  Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dstar<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  
  <span class="co">#random allocation among self-selected</span>
  Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
  R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>R<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>R<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  <span class="kw">return</span>(<span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>]))
}

simuls.self.select.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.self.select.ww &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.self.select.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">colnames</span>(simuls.self.select.ww) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>)
  <span class="kw">return</span>(simuls.self.select.ww)
}

sf.simuls.self.select.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">8</span>)
  sim &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.self.select.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>)
  <span class="kw">return</span>(sim)
}

Nsim &lt;-<span class="st"> </span><span class="dv">1000</span>
<span class="co">#Nsim &lt;- 10</span>
N.sample &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">1000</span>,<span class="dv">10000</span>,<span class="dv">100000</span>)
<span class="co">#N.sample &lt;- c(100,1000,10000)</span>
<span class="co">#N.sample &lt;- c(100,1000)</span>
<span class="co">#N.sample &lt;- c(100)</span>

simuls.self.select.ww &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.self.select.ww.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)
<span class="kw">names</span>(simuls.self.select.ww) &lt;-<span class="st"> </span>N.sample</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.self.select.yB.ww &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
  Dstar &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
  Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dstar<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  
  <span class="co">#random allocation among self-selected</span>
  Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
  R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>Ds<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>R<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>R<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>R)
  reg.y.R.yB.ols.self.select &lt;-<span class="st"> </span><span class="kw">lm</span>(y[Ds<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>R[Ds<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[Ds<span class="op">==</span><span class="dv">1</span>])
  <span class="kw">return</span>(reg.y.R.yB.ols.self.select<span class="op">$</span>coef[<span class="dv">2</span>])
}

simuls.self.select.yB.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.self.select.yB.ww &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.self.select.yB.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">colnames</span>(simuls.self.select.yB.ww) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>)
  <span class="kw">return</span>(simuls.self.select.yB.ww)
}

sf.simuls.self.select.yB.ww.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">8</span>)
  sim &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.self.select.yB.ww,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;WW&#39;</span>)
  <span class="kw">return</span>(sim)
}

Nsim &lt;-<span class="st"> </span><span class="dv">1000</span>
<span class="co">#Nsim &lt;- 10</span>
N.sample &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">1000</span>,<span class="dv">10000</span>,<span class="dv">100000</span>)
<span class="co">#N.sample &lt;- c(100,1000,10000)</span>
<span class="co">#N.sample &lt;- c(100,1000)</span>
<span class="co">#N.sample &lt;- c(100)</span>

simuls.self.select.yB.ww &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.self.select.yB.ww.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)
<span class="kw">names</span>(simuls.self.select.yB.ww) &lt;-<span class="st"> </span>N.sample</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(simuls.self.select.ww)){
  <span class="kw">hist</span>(simuls.self.select.ww[[i]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(Delta<span class="op">^</span>yWW)),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.55</span>))
  <span class="kw">abline</span>(<span class="dt">v=</span>delta.y.tt,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(simuls.self.select.yB.ww)){
  <span class="kw">hist</span>(simuls.self.select.yB.ww[[i]][,<span class="st">&#39;WW&#39;</span>],<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(Delta<span class="op">^</span>yWW)),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.55</span>))
  <span class="kw">abline</span>(<span class="dt">v=</span>delta.y.tt,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:montecarlohistselfselectww"></span>
<img src="STCI_files/figure-html/montecarlohistselfselectww-1.png" alt="Distribution of the $WW$ and $OLSX$ estimator with randomization after self-selection over replications of samples of different sizes" width="50%" /><img src="STCI_files/figure-html/montecarlohistselfselectww-2.png" alt="Distribution of the $WW$ and $OLSX$ estimator with randomization after self-selection over replications of samples of different sizes" width="50%" />
<p class="caption">
Figure 3.5: Distribution of the <span class="math inline">\(WW\)</span> and <span class="math inline">\(OLSX\)</span> estimator with randomization after self-selection over replications of samples of different sizes
</p>
</div>
<p>Figure <a href="RCT.html#fig:montecarlohistselfselectww">3.5</a> shows that, in our example, conditioning on covariates improves precision by the same amount as an increase in sample size by almost one order of magnitude.</p>
</div>
</div>
<div id="estimating-sampling-noise-1" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Estimating Sampling Noise</h3>
<p>In order to estimate precision, we can either use the CLT, deriving sampling noise from the heteroskedasticity-robust standard error OLS estimates, or we can use some form of resampling as the bootstrap or randomization inference.</p>

<div class="example">
<span id="exm:unnamed-chunk-86" class="example"><strong>Example 3.13  </strong></span>Let us derive the CLT-based estimates of sampling noise using the OLS standard errors without conditioning on covariates first. I’m using the sample size with <span class="math inline">\(N=1000\)</span> as an example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.RASS.simuls &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.self.select.ww[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;WW&#39;</span>]<span class="op">-</span>delta.y.tt),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.RASS.OLS.homo &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.ols.self.select)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.RASS.OLS.hetero &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.ols.self.select,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>True 99% sampling noise (from the simulations) is 0.548. 99% sampling noise estimated using default OLS standard errors is 0.578. 99% sampling noise estimated using heteroskedasticity robust OLS standard errors is 0.58.</p>
<p>Conditioning on covariates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.RASS.simuls.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.self.select.yB.ww[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;WW&#39;</span>]<span class="op">-</span>delta.y.tt),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.RASS.OLS.homo.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.yB.ols.self.select)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.RASS.OLS.hetero.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.yB.ols.self.select,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>True 99% sampling noise (from the simulations) is 0.295. 99% sampling noise estimated using default OLS standard errors is 0.294. 99% sampling noise estimated using heteroskedasticity robust OLS standard errors is 0.299.</p>
</div>
</div>
<div id="sec:design3" class="section level2">
<h2><span class="header-section-number">3.3</span> Randomization After Eligibility</h2>
<p>In Randomization After Eligiblity, we randomly select two groups among the eligibles. Members of the treated group are informed that they are eligible to the program and are free to self-select into it. Members of the control group are not enformed that they are eligible and cannot enroll into the program. With Randomization After Eligiblity, we can still recover the TT despite the fact that we have not randomized access to the programs among the applicants. This is the magic of instrumental variables. Let us detail the mechanics of this beautiful result.</p>
<div id="identification-2" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Identification</h3>
<p>In order to state the identification results in the Randomization After Eligibility design rigorously, I need to define new potential outcomes:</p>
<ul>
<li><span class="math inline">\(Y_i^{d,r}\)</span> is the value of the outcome <span class="math inline">\(Y\)</span> when individual <span class="math inline">\(i\)</span> belongs to the program group <span class="math inline">\(d\)</span> (<span class="math inline">\(d\in\left\{0,1\right\}\)</span>) and has been randomized in group <span class="math inline">\(r\)</span> (<span class="math inline">\(r\in\left\{0,1\right\}\)</span>).</li>
<li><span class="math inline">\(D_i^r\)</span> is the value of the program participation decision when individual <span class="math inline">\(i\)</span> has been assigned randomly to group <span class="math inline">\(r\)</span>.</li>
</ul>
<div id="identification-of-tt" class="section level4">
<h4><span class="header-section-number">3.3.1.1</span> Identification of TT</h4>
<p>In a Randomization After Eligiblity design, we need three assumptions to ensure identification of the TT:</p>

<div class="definition">
<p><span id="def:indelig" class="definition"><strong>Definition 3.5  (Independence Among Eligibles)  </strong></span>We assume that the randomized allocation of the program among eligibles is well done:</p>
<span class="math display">\[\begin{align*}
  R_i\Ind(Y_i^{0,0},Y_i^{0,1},Y_i^{1,0},Y_i^{1,1},D_i^1,D_i^0)|E_i=1.
\end{align*}\]</span>
</div>

<p>Independence can be enforced by the randomized allocation of information about eligibility among the eligibles.</p>
<p>We need a second assumption:</p>

<div class="definition">
<p><span id="def:randeligvalid" class="definition"><strong>Definition 3.6  (Randomization After Eligibility Validity)  </strong></span>We assume that no eligibles that has been randomized out can take the treatment and that the randomized allocation of the program does not interfere with how potential outcomes and self-selection are generated:</p>
<span class="math display">\[\begin{align*}
D_i^0 &amp; = 0\text{, } \forall i, \\
D_i &amp; = D_i^1R_i+(1-R_i)D_i^0 \\
Y_i &amp; = 
  \begin{cases}
    Y_i^{1,1} &amp; \text{ if } (R_i=1 \text{ and } D_i=1)   \\
    Y_i^{0,1} &amp; \text{ if } (R_i=1 \text{ and } D_i=0)    \\
    Y_i^{0,0} &amp; \text{ if } R_i=0
  \end{cases}
\end{align*}\]</span>
</div>
<p> with <span class="math inline">\(Y_i^{1,1}\)</span>, <span class="math inline">\(Y_i^{0,1}\)</span>, <span class="math inline">\(Y_i^{0,0}\)</span>, <span class="math inline">\(D_i^1\)</span> and <span class="math inline">\(D^0_i\)</span> the same potential outcomes and self-selection decisions as in a routine allocation of the treatment.</p>
<p>We need a third assumption:</p>

<div class="definition">
<p><span id="def:exclrestric" class="definition"><strong>Definition 3.7  (Exclusion Restriction of Eligibility)  </strong></span>We assume that there is no direct effect of being informed about eligibliity to the program on outcomes:</p>
<span class="math display">\[\begin{align*}
  Y_i^{1,1} &amp; = Y_i^{1,0}= Y_i^1\\
  Y_i^{0,1} &amp; = Y_i^{0,0}= Y_i^0.
\end{align*}\]</span>
</div>

<p>Under these assumptions, we have the following result:</p>

<div class="theorem">
<p><span id="thm:identTTRAE" class="theorem"><strong>Theorem 3.3  (Identification of TT With Randomization After Eligibility)  </strong></span>Under Assumptions <a href="RCT.html#def:indelig">3.5</a>, <a href="RCT.html#def:randeligvalid">3.6</a> and <a href="RCT.html#def:exclrestric">3.7</a>, the Bloom estimator among eligibles identifies TT:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{Bloom|E=1} &amp; = \Delta^Y_{TT},
\end{align*}\]</span>
</div>
<p> with:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{Bloom|E=1} &amp; = \frac{\Delta^Y_{WW|E=1}}{\Pr(D_i=1|R_i=1,E_i=1)} \\
  \Delta^Y_{WW|E=1} &amp; =\esp{Y_i|R_i=1,E_i=1}-\esp{Y_i|R_i=0,E_i=1}.
\end{align*}\]</span>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> I keep the conditioning on <span class="math inline">\(E_i=1\)</span> implicit all along to save notation.</p>
<span class="math display">\[\begin{align*}
  \esp{Y_i|R_i=1} &amp; = \esp{Y_i^{1,1}D_i+Y_i^{0,1}(1-D_i)|R_i=1} \\
                  &amp; = \esp{Y_i^0+D_i(Y_i^1-Y_i^0)|R_i=1} \\
                  &amp; = \esp{Y_i^0|R_i=1}+\esp{Y_i^1-Y_i^0|D_i=1,R_i=1}\Pr(D_i=1|R_i=1)\\
                  &amp; = \esp{Y_i^0}+\esp{Y_i^1-Y_i^0|D_i=1}\Pr(D_i=1|R_i=1),
\end{align*}\]</span>
<p>where the first equality uses Assumption <a href="RCT.html#def:randeligvalid">3.6</a>, the second equality Assumption <a href="RCT.html#def:exclrestric">3.7</a> and the last equality Assumption <a href="RCT.html#def:indelig">3.5</a> and the fact that <span class="math inline">\(D_i=1\Rightarrow R_i=1\)</span>. Using the same reasoning, we also have:</p>
<span class="math display">\[\begin{align*}
  \esp{Y_i|R_i=0} &amp; = \esp{Y_i^{1,0}D_i+Y_i^{0,0}(1-D_i)|R_i=0} \\
                  &amp; = \esp{Y_i^0|R_i=0} \\
                  &amp; = \esp{Y_i^0}.
\end{align*}\]</span>
A direct application of the formula for the Bloom estimator proves the result.
</div>

</div>
<div id="identification-of-ite" class="section level4">
<h4><span class="header-section-number">3.3.1.2</span> Identification of ITE</h4>
<p>The previous proof does not give a lot of intuition of how TT is identified in the Randomization After Eligibility design. In order to gain more insight, we are going to decompose the Bloom estimator, and have a look at its numerator. The numerator of the Bloom estimator is a With/Without comparison, and it identifies, under fairly light conditions, another causal effect, the Intention to Treat Effect (ITE).</p>
<p>Let me first define the ITE:</p>

<div class="definition">
<p><span id="def:ITE" class="definition"><strong>Definition 3.8  (Intention to Treat Effect)  </strong></span>In a Randomization After Eligibility design, the Intention to Treat Effect (ITE) is the effect of receiving information about eligiblity among eligibles:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|E_i=1}.
\end{align*}\]</span>
</div>

<p>Receiving information about eligibility has two impacts, in the general framework that we have delineated so far: first, it triggers some individuals into the treatment (those for which <span class="math inline">\(D_i^1\neq0\)</span>); second, it might have a direct effect on outcomes (<span class="math inline">\(Y_i^{d,1}\neq Y_i^{d,0}\)</span>). This second effect is the effect of annoucing eligiblity that does not goes through participation into the program. For example, it is possible that announcing eligibility to a retirement program makes me save more for retirement, even if I end up not taking up the proposed program.</p>
<p>The two causal channels that are at work within the ITE can be seen more clearly after some manipulations:</p>
<span class="math display" id="eq:ITE2">\[\begin{align}
  \Delta^Y_{ITE} &amp; = \esp{Y_i^{1,1}D^1_i+Y_i^{0,1}(1-D_i^1)-(Y_i^{1,0}D^0_i+Y_i^{0,0}(1-D_i^0))|E_i=1}\nonumber \\
                &amp; = \esp{Y_i^{1,1}D^1_i+Y_i^{0,1}(1-D_i^1)-(Y_i^{0,0}(D_i^1+1-D_i^1))|E_i=1}\nonumber \\
                &amp; = \esp{(Y_i^{1,1}-Y_i^{0,0})D^1_i+(Y_i^{0,1}-Y_i^{0,0})(1-D_i^1)|E_i=1}\nonumber \\
                &amp; = \esp{Y_i^{1,1}-Y_i^{0,0}|D^1_i=1,E_i=1}\Pr(D^1_i=1|E_i=1)\nonumber \\
                &amp; \phantom{=}+\esp{Y_i^{0,1}-Y_i^{0,0}|D_i^1=0,E_i=1}\Pr(D_i^1=0|E_i=1),\tag{3.1}
\end{align}\]</span>
<p>where the first equality follows from Assumption <a href="RCT.html#def:randeligvalid">3.6</a> and the second equality uses the fact that <span class="math inline">\(D_i^0=0\)</span>, <span class="math inline">\(\forall i\)</span>.</p>
<p>We can now see that the ITE is composed of two terms: the first term captures the effect of announcing eligibility on those who decide to participate into the program; the second term captures the effect of announcing eligibility on those who do not participate into the program. Both of these effects are weighted by the respective proportions of those reacting to the eligibility annoucement by participating and by not participating respectively.</p>
<p>Now, in order to see how the ITE “contains” the TT, we can use the following theorem:</p>

<div class="theorem">
<p><span id="thm:ITETT" class="theorem"><strong>Theorem 3.4  (From ITE to TT)  </strong></span>Under Assumptions <a href="RCT.html#def:indelig">3.5</a>, <a href="RCT.html#def:randeligvalid">3.6</a> and <a href="RCT.html#def:exclrestric">3.7</a>, ITE is equal to TT multiplied by the proportion of individuals taking up the treatment after eligibility has been announced:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \Delta^Y_{TT}\Pr(D^1_i=1|E_i=1).
\end{align*}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Under Assumption <a href="RCT.html#def:exclrestric">3.7</a>, Equation <a href="RCT.html#eq:ITE2">(3.1)</a> becomes:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \esp{Y_i^{1}-Y_i^{0}|D^1_i=1,E_i=1}\Pr(D^1_i=1|E_i=1) \\
                &amp; \phantom{=}+\esp{Y_i^{0}-Y_i^{0}|D_i^1=0,E_i=1}\Pr(D_i^1=0|E_i=1)\\
                &amp; = \esp{D_i^1(Y_i^{1}-Y_i^{0})|R_i=1,E_i=1} \\
                &amp; = \esp{Y_i^{1}-Y_i^{0}|D_i^1=1,R_i=1,E_i=1}\Pr(D^1_i=1|R_i=1,E_i=1) \\
                &amp; = \esp{Y_i^{1}-Y_i^{0}|D_i=1,E_i=1}\Pr(D^1_i=1|E_i=1),
\end{align*}\]</span>
where the first equality follows from Assumption <a href="RCT.html#def:exclrestric">3.7</a>, the second from Bayes’ rule and Assumptions <a href="RCT.html#def:indelig">3.5</a>, the third from Bayes’ rule and the last from the fact that <span class="math inline">\(D_i^1=1,R_i=1\Leftrightarrow D_i=1\)</span>.
</div>

<p>The previous theorem shows that Assumption <a href="RCT.html#def:exclrestric">3.7</a> shuts down any direct effect of the announcement of eligibility on outcomes. As a consequence of this assumption, the only impact that an eligibility annoucement has on outcomes is through participation into the program. Hence, the ITE is equal to TT multiplied by the proportion of people taking up the treatment when eligibility is announced.</p>
<p>In order to move from the link between TT and ITE to the mechanics of the Bloom estimator, we need two additional identification results. The first result shows that ITE can be identified under fairly light conditions by a WW estimator. The second result shows that the proportion of people taking up the treatment when eligiblity is announced is also easily estimated from the data.</p>

<div class="theorem">
<p><span id="thm:ITERAE" class="theorem"><strong>Theorem 3.5  (Identification of ITE with Randomization After Eligibility)  </strong></span>Under Assumptions <a href="RCT.html#def:indelig">3.5</a> and <a href="RCT.html#def:randeligvalid">3.6</a>, ITE is identified by the With/Without comparison among eligibles:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \Delta^Y_{WW|E=1}.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{align*}
 \Delta^Y_{WW|E=1} &amp; =\esp{Y_i|R_i=1,E_i=1}-\esp{Y_i|R_i=0,E_i=1} \\
                   &amp; = \esp{Y_i^{D_i^1,1}|R_i=1,E_i=1}-\esp{Y_i^{D_i^0,0}|R_i=0,E_i=1} \\
                    &amp; = \esp{Y_i^{D_i^1,1}|E_i=1}-\esp{Y_i^{D_i^0,0}|E_i=1},
\end{align*}\]</span>
where the second equality follows from Assumption <a href="RCT.html#def:randeligvalid">3.6</a> and the third from Assumption <a href="RCT.html#def:indelig">3.5</a>.
</div>


<div class="theorem">
<p><span id="thm:prRAE" class="theorem"><strong>Theorem 3.6  (Identification of <span class="math inline">\(\Pr(D^1_i=1|E_i=1)\)</span>)  </strong></span>Under Assumptions <a href="RCT.html#def:indelig">3.5</a> and <a href="RCT.html#def:randeligvalid">3.6</a>, <span class="math inline">\(\Pr(D^1_i=1|E_i=1)\)</span> is identified by the proportion of people taking up the offered treatment when informed about their eligibility status:</p>
<span class="math display">\[\begin{align*}
  \Pr(D^1_i=1|E_i=1) &amp; = \Pr(D_i=1|R_i=1,E_i=1).
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{align*}
 \Pr(D_i=1|R_i=1,E_i=1) &amp; =\Pr(D^1_i=1|R_i=1,E_i=1) \\
                        &amp; = \Pr(D^1_i=1|E_i=1),
\end{align*}\]</span>
where the first equality follows from Assumption <a href="RCT.html#def:randeligvalid">3.6</a> and the second from Assumption <a href="RCT.html#def:indelig">3.5</a>.
</div>


<div class="corollary">
<p><span id="cor:RAEBloom" class="corollary"><strong>Corollary 3.1  (Bloom estimator and ITE)  </strong></span>It follows from Theorems <a href="RCT.html#thm:ITERAE">3.5</a> and <a href="RCT.html#thm:prRAE">3.6</a> that, under Assumptions <a href="RCT.html#def:indelig">3.5</a> and <a href="RCT.html#def:randeligvalid">3.6</a>, the Bloom estimator is equal to the ITE divided by the propotion of agents taking up the program when eligible:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{Bloom|E=1} &amp; = \frac{\Delta^Y_{ITE}}{\Pr(D^1_i=1|E_i=1)}.
\end{align*}\]</span>
</div>

<p>As a consequence of Corollary <a href="RCT.html#cor:RAEBloom">3.1</a>, we see that the Bloom estimator reweights the ITE, the effect of receiving information about eligibility, by the proportion of people reacting to the eligibility by participating in the program. From Theorem <a href="RCT.html#thm:ITETT">3.4</a>, we know that this ratio will be equal to TT if the Assumption <a href="RCT.html#def:exclrestric">3.7</a> also holds, so that all the impact of the eligibility annoucement stems from entering the program. The eligibility annoucement serves as an instrument for program participation.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> The design using Randomization After Eligibility seems like magic. You do not assign randomly the program, but information about the eligiblity status, but you can recover the effect of the program anyway. How does this magic work? Randomization After Eligibility is also less intrusive than Randomization After Self-Selection. With the latter design, you have to actively send away individuals that have expressed an interest for entering the program. This is harsh. With Randomization After Eligibility, you do not have to send away people expressing interest after being informed. And it seems that you are not paying a price for that, since you are able to recover the same TT parameter. Well, actually, you are going to pay a price in terms of larger sampling noise.
</div>
<p> The intuition for all that can be delineated using the very same apparatus that we have developed so far. So here goes. Under the assumptions made so far, it is easy to show that (omitting the conditioning on <span class="math inline">\(E_i=1\)</span> for simplicity):</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW|E=1} &amp; = \esp{Y_i^{1,1}|D_i^1=1,R_i=1}\Pr(D^1_i=1|R_i=1)\\
                    &amp; \phantom{=}-\esp{Y_i^{0,0}|D_i^1=1,R_i=0}\Pr(D^1_i=1|R_i=0) \\
                    &amp; \phantom{=}+ \esp{Y_i^{0,1}|D_i^1=0,R_i=1}\Pr(D^1_i=0|R_i=1)\\
                    &amp; \phantom{=}-\esp{Y_i^{0,0}|D_i^1=0,R_i=0}\Pr(D^1_i=0|R_i=0).  
\end{align*}\]</span>
<p>The first part of the equation is due to the difference in outcomes between the two treatment arms for people that take up the program when eligibility is announced. The second part is due to the difference in outcomes between the two treatment arms for people that do not take up the program when eligibility is announced. This second part cancels out under Assumption <a href="RCT.html#def:indelig">3.5</a> and <a href="RCT.html#def:exclrestric">3.7</a>.</p>
<p>But this cancelling out only happens in the population. In a given sample, the sample equivalents to the two members of the second part of the equation do not have to be equal, and thus they do not cancel out, generating additional sampling noise compared to the Randomization After Self-Selection design. Indeed, in the Randomization After Self-Selection design, you observe the population with <span class="math inline">\(D_i^1=1\)</span> in both the treatment and control arms (you actually observe this population before randomizing the treatment within it), and you can enforce that the effect on <span class="math inline">\(D_i^1=0\)</span> should be zero, under your assumptions. In the Randomization After Eligiblity design, you do not observe the population with <span class="math inline">\(D_i^1=1\)</span> in the control arm, and you cannot enforce the equality of the outcomes for those with <span class="math inline">\(D_i^1=0\)</span> present in both arms. You have to rely on the sampling estimates to make this cancellation, and that generates sampling noise.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> In practice, we use a pseudo-RNG to allocate the randomized annoucement of the eligibility status:
</div>

<span class="math display">\[\begin{align*}
  R_i^* &amp; \sim \mathcal{U}[0,1]\\
  R_i &amp; = 
  \begin{cases}
    1 &amp; \text{ if } R_i^*\leq .5 \land E_i=1\\
    0 &amp; \text{ if } R_i^*&gt; .5 \land E_i=1
  \end{cases} \\
  D_i &amp; = \uns{\bar{\alpha}+\theta\bar{\mu}-C_i\geq0 \land E_i=1 \land R_i=1}
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-93" class="example"><strong>Example 3.14  </strong></span>In our numerical example, we can actually use the same sample as we did for Randomization After Self-Selection. I have to generate it again, though, since I am going to allocate <span class="math inline">\(R_i\)</span> differently.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
Dindex &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
Dstar &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)</code></pre></div>
<p>The value of TT in our example is the same as the one in the Randomization After Self-Selection case. TT in the population is equal to 0.17.</p>
<p>Let’s now compute the value of ITE in the population. In our model, exclusion restriction holds, so that we can use the fact that <span class="math inline">\(ITE=TT\Pr(D^1_i=1|E_i=1)\)</span>. We thus only need to compute <span class="math inline">\(\Pr(D^1_i=1|E_i=1)\)</span>:</p>
<span class="math display">\[\begin{align*}
  \Pr(D^1_i=1|E_i=1) &amp; = \Pr(D_i^*\geq0|y_i^B\leq\bar{y}).
\end{align*}\]</span>
<p>I can again use the package <code>tmvtnorm</code> to compute that probability. It is indeed equal to <span class="math inline">\(1-\Pr(D_i^*&lt;0|y_i^B\leq\bar{y})\)</span>, where <span class="math inline">\(\Pr(D_i^*&lt;0|y_i^B\leq\bar{y})\)</span> is the cumulative density of <span class="math inline">\(D_i^*\)</span> conditional on <span class="math inline">\(y_i^B\leq\bar{y}\)</span>,  the marginal cumulative of the third variable of the truncated trivariate normal <span class="math inline">\((\mu_i,y_i^B,D_i^*)\)</span> where the first variable is not truncated and the second one is truncated at <span class="math inline">\(\bar{y}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lower.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span><span class="ot">Inf</span>)
upper.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">Inf</span>,<span class="kw">log</span>(param[<span class="st">&#39;barY&#39;</span>]),<span class="ot">Inf</span>)
prD1.elig &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">-</span><span class="kw">ptmvnorm.marginal</span>(<span class="dt">xn=</span><span class="dv">0</span>,<span class="dt">n=</span><span class="dv">3</span>,<span class="dt">mean=</span>mean.mu.yB.Dstar,<span class="dt">sigma=</span>cov.mu.yB.Dstar,<span class="dt">lower=</span>lower.cut,<span class="dt">upper=</span>upper.cut)
delta.y.ite &lt;-<span class="st"> </span>delta.y.tt<span class="op">*</span>prD1.elig</code></pre></div>
<p><span class="math inline">\(\Pr(D^1_i=1|E_i=1)=\)</span> 0.459. As a consequence, ITE in the population is equal to 0.17 * 0.459 <span class="math inline">\(\approx\)</span> 0.078. In the sample, the value of ITE and TT are equal to:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta.y.tt.sample &lt;-<span class="st"> </span><span class="kw">mean</span>(y1[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>Dstar<span class="op">==</span><span class="dv">1</span>]<span class="op">-</span>y0[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>Dstar<span class="op">==</span><span class="dv">1</span>])
delta.y.ite.sample &lt;-<span class="st"> </span>delta.y.tt.sample<span class="op">*</span><span class="kw">mean</span>(Dstar[E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p><span class="math inline">\(\Delta^y_{ITE_s}=\)</span> 0.068 and <span class="math inline">\(\Delta^y_{TT_s}=\)</span> 0.187.</p>
<p>Now, we can allocate the randomized treatment and let potential outcomes be realized:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#random allocation among eligibles</span>
Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)</code></pre></div>
</div>
</div>
<div id="estimating-the-ite-and-the-tt" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Estimating the ITE and the TT</h3>
<p>In general, we start the analysis of Randomization After Eligibility by estimating the ITE. Then, we provide the TT by dividing the ITE by the proportion of participants among the eligibles.</p>
<p>Actually, this procedure is akin to an instrumental variables estimator and we will see that the Bloom estimator is actually an IV estimator. The ITE estimation step corresponds to the reduced form in a classical IV approach. Estimation of the proportion of participants is the first stage in a IV approach. Estimation of the TT corresponds to the structural equation step of an IV procedure.</p>
<div id="estimating-the-ite" class="section level4">
<h4><span class="header-section-number">3.3.2.1</span> Estimating the ITE</h4>
<p>Estimation of the ITE relies on the WW estimator, in general implemented using OLS. It is similar to the estimation of ATE and TT in the Brute Force and Randomization After Self-Selection designs.</p>
<div id="using-the-ww-estimator-2" class="section level5">
<h5><span class="header-section-number">3.3.2.1.1</span> Using the WW estimator</h5>
<p>Estimation of the ITE can be based on the WW estimator among eligibles.</p>
<span class="math display">\[\begin{align*}
  \hat{\Delta}^Y_{WW|E=1} &amp; = \frac{1}{\sum_{i=1}^N E_iR_i}\sum_{i=1}^N Y_iE_iR_i-\frac{1}{\sum_{i=1}^N E_i(1-R_i)}\sum_{i=1}^N E_iY_i(1-R_i).
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-94" class="example"><strong>Example 3.15  </strong></span>In our numerical example, we can form the WW estimator among eligibles:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta.y.ww.elig &lt;-<span class="st"> </span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>WW among eligibles is equal to 0.069.</p>
</div>
<div id="using-ols-2" class="section level5">
<h5><span class="header-section-number">3.3.2.1.2</span> Using OLS</h5>
<p>As we have already seen before, the WW estimator is equivalent to OLS with one constant and no control variables. As a consequence, we can estimate the ITE using the OLS estimate of <span class="math inline">\(\beta\)</span> in the following regression run on the sample with <span class="math inline">\(E_i=1\)</span>:</p>
<span class="math display">\[\begin{align*}
  Y_i &amp; = \alpha + \beta R_i + U_i.
\end{align*}\]</span>
<p>By construction, <span class="math inline">\(\hat{\beta}_{OLSR|E=1}=\hat{\Delta}^Y_{WW|E=1}\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-95" class="example"><strong>Example 3.16  </strong></span>In our numerical example, we can form the WW estimator among eligibles:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.ols.elig &lt;-<span class="st"> </span><span class="kw">lm</span>(y[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>R[E<span class="op">==</span><span class="dv">1</span>])
delta.y.ols.elig &lt;-<span class="st"> </span>reg.y.ols.elig<span class="op">$</span>coef[<span class="dv">2</span>]</code></pre></div>
<p><span class="math inline">\(\hat{\beta}_{OLSR|E=1}\)</span> is equal to 0.069. Remember that ITE in the population is equal to 0.078.</p>
</div>
<div id="using-ols-conditioning-on-covariates-2" class="section level5">
<h5><span class="header-section-number">3.3.2.1.3</span> Using OLS conditioning on covariates</h5>
<p>Again, as in the previous designs, we can compute ITE by using OLS conditional on covariates. Parametrically, we can run the following OLS regression among eligibles (with <span class="math inline">\(E_i=1\)</span>):</p>
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta R_i + \gamma&#39; X_i + U_i.
  \end{align*}\]</span>
<p>The OLS estimate of <span class="math inline">\(\beta\)</span> estimates the ITE.</p>
<p><strong><span style="font-variant: small-caps;">Again: Needed: proof. Especially check whether we need to center covariates at the mean of the treatment group. I think so.</span></strong></p>
<p>We can also use Matching to obtain a nonparametric estimator.</p>

<div class="example">
<span id="exm:unnamed-chunk-96" class="example"><strong>Example 3.17  </strong></span>Let us compute the OLS estimator conditioning on <span class="math inline">\(y_i^B\)</span>:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.yB.ols.elig &lt;-<span class="st"> </span><span class="kw">lm</span>(y[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>Our estimate of ITE after conditioning on <span class="math inline">\(y_i^B\)</span> is 0.065. I do not have time to run the simulations, but it is highly likely that the sampling noise is lower after conditioning on <span class="math inline">\(y_i^B\)</span>.</p>
<p><strong><span style="font-variant: small-caps;">I do not have time to run the simulations, but it is highly likely that the sampling noise is lower after conditioning on <span class="math inline">\(y_i^B\)</span>.</span></strong></p>
</div>
</div>
<div id="estimating-tt-1" class="section level4">
<h4><span class="header-section-number">3.3.2.2</span> Estimating TT</h4>
<p>We can estimate TT either using the Bloom estimator, or using the IV estimator, which is equivalent to a Bloom estimator in the Eligibility design.</p>
<div id="using-the-bloom-estimator" class="section level5">
<h5><span class="header-section-number">3.3.2.2.1</span> Using the Bloom estimator</h5>
<p>Using the Bloom estimator, we simply compute the numerator of the Bloom estimator and divide it by the estimated proportion of eligible individuals with <span class="math inline">\(R_i=1\)</span> that have chosen to take the program.</p>
<span class="math display">\[\begin{align*}
  \hat{\Delta}^Y_{WW|D=1} &amp; = \frac{\frac{1}{\sum_{i=1}^N E_iR_i}\sum_{i=1}^N Y_iE_iR_i-\frac{1}{\sum_{i=1}^N E_i(1-R_i)}\sum_{i=1}^N E_iY_i(1-R_i)}{\frac{1}{\sum_{i=1}^N E_iR_i}\sum_{i=1}^N D_iE_iR_i}.
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-97" class="example"><strong>Example 3.18  </strong></span>Let’s see how the Boom estimator works in our example.
</div>
<p> The numerator of the Bloom estimator is the ITE that we have just computed: 0.069. The denominator of the Bloom estimator is equal to the proportion of eligible individuals with <span class="math inline">\(R_i=1\)</span> that have chosen to take the program: 0.342.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delta.y.R.bloom.elig &lt;-<span class="st"> </span>(<span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>]))<span class="op">/</span><span class="kw">mean</span>(Ds[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>The resulting estimate of TT is 0.203. It is rather far from the population or sample estimates: 0.17 and 0.187 respectively. What happened? The error seems to come from noise in the denominator of the Bloom estimator. In the ITE estimation, the true ITEs in the population and sample are 0.078 and 0.068 respectively and our estimate is equal to 0.069, so that’s fine. In the denominator, the proportion of randomized eligibles that take the program is equal to 0.342 while the true proportions in the population and in the sample are 0.459 and 0.364 respectively. So we do not have enough invited eligibles getting into the program, and the ones who do have unusually large outcomes. These two sampling errors combine to blow up the estimate of TT.</p>
</div>
<div id="using-iv" class="section level5">
<h5><span class="header-section-number">3.3.2.2.2</span> Using IV</h5>
<p>There is a very useful results, similar to the one stating that the WW estimator is equivalent to an OLS estimator: in the Eligiblity design, the Bloom estimator is equivalent to an IV estimator:</p>

<div class="theorem">
<p><span id="thm:BloomIV" class="theorem"><strong>Theorem 3.7  (Bloom is IV)  </strong></span>Under the assumption that there is at least one individual with <span class="math inline">\(R_i=1\)</span> and with <span class="math inline">\(D_i=1\)</span>, the coefficient <span class="math inline">\(\beta\)</span> in the following regression estimated among eligibles using <span class="math inline">\(R_i\)</span> as an IV</p>
<span class="math display">\[\begin{align*}
        Y_i &amp;  = \alpha + \beta D_i + U_i
    \end{align*}\]</span>
<p>is the Bloom estimator in the Eligibility Design:</p>
<span class="math display">\[\begin{align*}
\hat{\beta}_{IV} &amp; = \frac{\frac{1}{\sum_{i=1}^N E_i}\sum_{i=1}^NE_i\left(Y_i-\frac{1}{\sum_{i=1}^N E_i}\sum_{i=1}^NE_iY_i\right)\left(R_i-\frac{1}{\sum_{i=1}^N E_i}\sum_{i=1}^NE_iR_i\right)}{\frac{1}{\sum_{i=1}^N E_i}\sum_{i=1}^NE_i\left(D_i-\frac{1}{\sum_{i=1}^N E_i}\sum_{i=1}^NE_iD_i\right)\left(R_i-\frac{1}{\sum_{i=1}^N E_i}\sum_{i=1}^NE_iR_i\right)} \\
                                &amp; = \frac{\frac{1}{\sum_{i=1}^N E_iR_i}\sum_{i=1}^N Y_iR_iE_i-\frac{1}{\sum_{i=1}^N (1-R_i)E_i}\sum_{i=1}^N Y_i(1-R_i)E_i}{\frac{1}{\sum_{i=1}^N E_iR_i}\sum_{i=1}^N D_iR_iE_i}.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> The proof is straightforward using Theorem <a href="RCT.html#thm:WaldIV">3.15</a> below and setting <span class="math inline">\(D_i=0\)</span> when <span class="math inline">\(R_i=0\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-99" class="example"><strong>Example 3.19  </strong></span>In our numerical example, we have:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.2sls.elig &lt;-<span class="st"> </span><span class="kw">ivreg</span>(y[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>Ds[E<span class="op">==</span><span class="dv">1</span>]<span class="op">|</span>R[E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p><span class="math inline">\(\hat{\beta}_{IV}=\)</span> 0.203 which is indeed equal to the Bloom estimator (<span class="math inline">\(\hat{\Delta}^y_{Bloom}=\)</span> 0.203).</p>
</div>
<div id="using-iv-conditional-on-covariates" class="section level5">
<h5><span class="header-section-number">3.3.2.2.3</span> Using IV conditional on covariates</h5>
<p>We can improve on the precision of our 2SLS estimator by conditioning on observed covariates. Parametrically estimating the following equation with <span class="math inline">\(R_i\)</span> and <span class="math inline">\(X_i\)</span> as instruments on the sample with <span class="math inline">\(E_i=1\)</span>:</p>
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta D_i + \gamma&#39; X_i + U_i.
\end{align*}\]</span>
<p><strong><span style="font-variant: small-caps;">Proof? Do we need to center covariates to their mean in the treatment group?</span></strong></p>
<p><strong><span style="font-variant: small-caps;">Nonparametric estimation using Frolich’s Wald matching estimator.</span></strong>}</p>

<div class="example">
<span id="exm:unnamed-chunk-100" class="example"><strong>Example 3.20  </strong></span>In our numerical example, we have:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.yB.2sls.elig &lt;-<span class="st"> </span><span class="kw">ivreg</span>(y[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Ds[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>] <span class="op">|</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p>As a consequence, <span class="math inline">\(\hat{\Delta}^y_{Bloom(X)}=\)</span> 0.191.</p>
<p>Does conditioning on covariates improve precision? Let’s run some Monte-Carlo somulations in order to check for that.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.elig &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
  Dindex &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
  Dstar &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  
  <span class="co">#random allocation among self-selected</span>
  Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
  R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  reg.y.R.2sls.elig &lt;-<span class="st"> </span><span class="kw">ivreg</span>(y[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>Ds[E<span class="op">==</span><span class="dv">1</span>]<span class="op">|</span>R[E<span class="op">==</span><span class="dv">1</span>])
  <span class="kw">return</span>(reg.y.R.2sls.elig<span class="op">$</span>coef[<span class="dv">2</span>])
}

simuls.elig.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.elig &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.elig,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">colnames</span>(simuls.elig) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Bloom&#39;</span>)
  <span class="kw">return</span>(simuls.elig)
}

sf.simuls.elig.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">8</span>)
  <span class="kw">sfLibrary</span>(AER)
  sim &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.elig,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Bloom&#39;</span>)
  <span class="kw">return</span>(sim)
}

Nsim &lt;-<span class="st"> </span><span class="dv">1000</span>
<span class="co">#Nsim &lt;- 10</span>
<span class="co">#N.sample &lt;- c(100,1000,10000,100000)</span>
N.sample &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1000</span>,<span class="dv">10000</span>,<span class="dv">100000</span>)
<span class="co">#N.sample &lt;- c(100,1000)</span>
<span class="co">#N.sample &lt;- c(100)</span>

simuls.elig &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.elig.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)
<span class="kw">names</span>(simuls.elig) &lt;-<span class="st"> </span>N.sample</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">monte.carlo.elig.yB &lt;-<span class="st"> </span><span class="cf">function</span>(s,N,param){
  <span class="kw">set.seed</span>(s)
  mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
  UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
  yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
  YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
  E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
  V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
  Dindex &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
  Dstar &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
  eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
  U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
  y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
  alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
  y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
  Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
  Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)
  
  <span class="co">#random allocation among self-selected</span>
  Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
  R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
  y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
  reg.y.R.yB.2sls.elig &lt;-<span class="st"> </span><span class="kw">ivreg</span>(y[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Ds[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>] <span class="op">|</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>])
  <span class="kw">return</span>(reg.y.R.yB.2sls.elig<span class="op">$</span>coef[<span class="dv">2</span>])
}

simuls.elig.yB.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  simuls.elig.yB &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.elig.yB,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">colnames</span>(simuls.elig.yB) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Bloom&#39;</span>)
  <span class="kw">return</span>(simuls.elig.yB)
}

sf.simuls.elig.yB.N &lt;-<span class="st"> </span><span class="cf">function</span>(N,Nsim,param){
  <span class="kw">sfInit</span>(<span class="dt">parallel=</span><span class="ot">TRUE</span>,<span class="dt">cpus=</span><span class="dv">8</span>)
  <span class="kw">sfLibrary</span>(AER)
  sim &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">unlist</span>(<span class="kw">sfLapply</span>(<span class="dv">1</span><span class="op">:</span>Nsim,monte.carlo.elig.yB,<span class="dt">N=</span>N,<span class="dt">param=</span>param)),<span class="dt">nrow=</span>Nsim,<span class="dt">ncol=</span><span class="dv">1</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
  <span class="kw">sfStop</span>()
  <span class="kw">colnames</span>(sim) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Bloom&#39;</span>)
  <span class="kw">return</span>(sim)
}

Nsim &lt;-<span class="st"> </span><span class="dv">1000</span>
<span class="co">#Nsim &lt;- 10</span>
<span class="co">#N.sample &lt;- c(100,1000,10000,100000)</span>
N.sample &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1000</span>,<span class="dv">10000</span>,<span class="dv">100000</span>)
<span class="co">#N.sample &lt;- c(100,1000)</span>
<span class="co">#N.sample &lt;- c(100)</span>

simuls.elig.yB &lt;-<span class="st"> </span><span class="kw">lapply</span>(N.sample,sf.simuls.elig.yB.N,<span class="dt">Nsim=</span>Nsim,<span class="dt">param=</span>param)
<span class="kw">names</span>(simuls.elig.yB) &lt;-<span class="st"> </span>N.sample</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(simuls.elig)){
  <span class="kw">hist</span>(simuls.elig[[i]][,<span class="st">&#39;Bloom&#39;</span>],<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(Delta<span class="op">^</span>yBloom)),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.55</span>))
  <span class="kw">abline</span>(<span class="dt">v=</span>delta.y.tt,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(simuls.elig.yB)){
  <span class="kw">hist</span>(simuls.elig.yB[[i]][,<span class="st">&#39;Bloom&#39;</span>],<span class="dt">breaks=</span><span class="dv">30</span>,<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&#39;N=&#39;</span>,<span class="kw">as.character</span>(N.sample[i])),<span class="dt">xlab=</span><span class="kw">expression</span>(<span class="kw">hat</span>(Delta<span class="op">^</span>yBloom)),<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.15</span>,<span class="fl">0.55</span>))
  <span class="kw">abline</span>(<span class="dt">v=</span>delta.y.tt,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
}</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:montecarlohisteligbloom"></span>
<img src="STCI_files/figure-html/montecarlohisteligbloom-1.png" alt="Distribution of the $Bloom$ and $Bloom(X)$ estimators with randomization after eligibility over replications of samples of different sizes" width="50%" /><img src="STCI_files/figure-html/montecarlohisteligbloom-2.png" alt="Distribution of the $Bloom$ and $Bloom(X)$ estimators with randomization after eligibility over replications of samples of different sizes" width="50%" />
<p class="caption">
Figure 3.6: Distribution of the <span class="math inline">\(Bloom\)</span> and <span class="math inline">\(Bloom(X)\)</span> estimators with randomization after eligibility over replications of samples of different sizes
</p>
</div>
<p>We can take three things from Figure <a href="RCT.html#fig:montecarlohisteligbloom">3.6</a>:</p>
<ol style="list-style-type: decimal">
<li>Problems with the IV estimator appear with <span class="math inline">\(N=100\)</span> (probably because there are some samples where no one is treated).</li>
<li>Sampling noise from randomization after eligibility is indeed larger than sampling noise from randomization after self-selection.</li>
<li>Conditioning on covariates helps.</li>
</ol>
</div>
</div>
</div>
<div id="estimating-sampling-noise-2" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Estimating sampling noise</h3>
<p>As always, we can estimate samling noise either using the CLT or resampling methods. Using the CLT, we can derive the following formula for the distribution of the Bloom estimator:</p>

<div class="theorem">
<p><span id="thm:asymBloom" class="theorem"><strong>Theorem 3.8  (Asymptotic Distribution of <span class="math inline">\(\hat{\Delta}^Y_{Bloom}\)</span>)  </strong></span>Under Assumptions <a href="RCT.html#def:indelig">3.5</a>, <a href="RCT.html#def:randeligvalid">3.6</a> and <a href="RCT.html#def:exclrestric">3.7</a> and assuming that there is at least one individual with <span class="math inline">\(R_i=1\)</span> and one individual with <span class="math inline">\(D_i=1\)</span>, we have (keeping the conditioning on <span class="math inline">\(E_i=1\)</span> implicit):</p>
<span class="math display">\[\begin{align*}
  \sqrt{N}(\hat{\Delta}^Y_{Bloom}-\Delta^Y_{TT}) &amp;  \stackrel{d}{\rightarrow}
  \mathcal{N}\left(0,\frac{1}{(p^{D}_1)^2}\left[\left(\frac{p^D}{p^R}\right)^2\frac{\var{Y_i|R_i=0}}{1-p^R}+\left(\frac{1-p^D}{1-p^R}\right)^2\frac{\var{Y_i|R_i=1}}{p^R}\right]\right),
\end{align*}\]</span>
</div>

<p>with <span class="math inline">\(p^D=\Pr(D_i=1)\)</span>, <span class="math inline">\(p^R=\Pr(R_i=1)\)</span> and <span class="math inline">\((p^{D}_1=\Pr(D_i=1|R_i=1)\)</span>.</p>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> The proof is immediate using Theorem <a href="RCT.html#thm:asymWald">3.16</a>, setting <span class="math inline">\(p^{AT}=0\)</span>.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Theorem <a href="RCT.html#thm:asymBloom">3.8</a> shows that there is a price to pay for not randomizing after self-selection. This price is a decrease in precision. The variance of the estimator is weighted by <span class="math inline">\(\frac{1}{(\Pr(D_i=1|R_i=1))^2}\)</span>. This means that the effective sample size is equal to the number of individuals that take up the treatment when offered. We generaly call these individuals “compliers,” since they comply with the treatment assignment. Sampling noise is of the same order of magnitude as the number of compliers You might have very low precision despite a very large sample size if you have a very small proportion of compliers.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> In order to compute an estimate of the sampling noise of hte Bloom estimator, we can either use the plug-in formula from Theorem <a href="RCT.html#thm:asymBloom">3.8</a> or use the IV standard errors robust to heteroskedasticity. Here is a simple function in order to compute the plug-in estimator:
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var.RAE.plugin &lt;-<span class="st"> </span><span class="cf">function</span>(pD1,pD,pR,V0,V1,N){
  <span class="kw">return</span>(((pD<span class="op">/</span>pR)<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(V0<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>pR))<span class="op">+</span>((<span class="dv">1</span><span class="op">-</span>pD)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>pR))<span class="op">^</span><span class="dv">2</span><span class="op">*</span>(V1<span class="op">/</span>pR))<span class="op">/</span>(N<span class="op">*</span>pD1<span class="op">^</span><span class="dv">2</span>))
}</code></pre></div>

<div class="example">
<span id="exm:unnamed-chunk-104" class="example"><strong>Example 3.21  </strong></span>Let us derive the CLT-based estimates of sampling noise using both the plug-in estimator and the IV standard errors without conditioning on covariates first. For the sake of the example, I’m working with a sample of size <span class="math inline">\(N=1000\)</span>.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.RAE.simuls &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.elig[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;Bloom&#39;</span>]<span class="op">-</span>delta.y.tt),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.RAE.IV.plugin &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">var.RAE.plugin</span>(<span class="dt">pD1=</span><span class="kw">mean</span>(Ds[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>]),<span class="dt">pD=</span><span class="kw">mean</span>(Ds[E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">pR=</span><span class="kw">mean</span>(R[E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">V0=</span><span class="kw">var</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">V1=</span><span class="kw">var</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">N=</span><span class="kw">length</span>(y[E<span class="op">==</span><span class="dv">1</span>])))
sn.RAE.IV.homo &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.2sls.elig)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.RAE.IV.hetero &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.2sls.elig,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>True 99% sampling noise (from the simulations) is 0.757. 99% sampling noise estimated using the plug-in estimator is 0.921. 99% sampling noise estimated using default IV standard errors is 1.069. 99% sampling noise estimated using heteroskedasticity robust IV standard errors is 0.92.</p>
<p>Conditioning on covariates:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.RAE.simuls.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.elig.yB[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;Bloom&#39;</span>]<span class="op">-</span>delta.y.tt),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.RAE.IV.homo.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.yB.2sls.elig)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.RAE.IV.hetero.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.yB.2sls.elig,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>True 99% sampling noise (from the simulations) is 0.393. 99% sampling noise estimated using default IV standard errors is 0.457. 99% sampling noise estimated using heteroskedasticity robust IV standard errors is 0.454.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Sampling noise in the Randomization After Eligibility design seems larger than sampling noise in the Randomization After Self-Selection design.
</div>
<p> In the Randomization After Self-Selection design, sampling noise with <span class="math inline">\(N=1000\)</span> is equal to 0.55. In the Randomization After Eligibility design, sampling noise with <span class="math inline">\(N=1000\)</span> is equal to 0.76. Why such a difference? Both designs have the same effective sample size.</p>
<p>In the Randomization After Self-Selection design, the effective sample size <span class="math inline">\(N^{RASS}_e\)</span> is the number of eligible individuals that apply to take up the program: <span class="math inline">\(N^{RASS}_e=N\Pr(D_i=1|E_i=1)\Pr(E_i=1)\)</span>. In our example, <span class="math inline">\(N^{RASS}_e=1000 *\)</span> 0.459 <span class="math inline">\(*\)</span> 0.218 <span class="math inline">\(=\)</span> 100.</p>
<p>In the Randomization After Eligibility design, the sample size on which the regressions are performed is <span class="math inline">\(N^{RAE}\)</span>, the number of eligible individuals: <span class="math inline">\(N^{RAE}=N\Pr(E_i=1)\)</span>. In our example, <span class="math inline">\(N^{RAE}=1000 *\)</span> 0.459 <span class="math inline">\(=\)</span> 459. But the effective sample size for the Randomization After Eligibility design is actually equal to the one in the Randomization After Self-Selection design because only compliers matter for the precision of the Bloom estimator, as Theorem <a href="RCT.html#thm:asymBloom">3.8</a> shows. Thus <span class="math inline">\(N^{RASS}_e=N^{RAE}_e\)</span>.</p>
<p>Why then is sampling noise much larger in the Randomization After Eligibility design? Probably because the Bloom estimator cannot enforce the fact that the impact of the program on non compliers is zero. It has to estimate the average outcome of non compliers in both treatment arms and hope that they cancel. In real samples, they won’t, increasing the size of sampling noise.</p>
</div>
</div>
<div id="sec:design4" class="section level2">
<h2><span class="header-section-number">3.4</span> Encouragement Design</h2>
<p>In an Encouragement Design, we randomly select two groups among the eligibles, as in Randomization After Eligibility. Treated individuals randomly receive an encouragement to participate in the program and decide whether they want to comply with the encouragement and join the program. Individuals in the control group do not receive an encouragement, but they can still decide to self-select in the program. The Encouragement design differs from the Randomization After Eligibility design mainly by not barring entry into the programs to individuals in the control group. If successful, the encouragement generates a higher level of take up of the program in the treatment group than in the control group. Examples of encouragements are additional reminders that the program exists, help in subscribing the program, financial incentives for subscribing the program, etc.</p>
<p>In an Encouragement Design, we can recover the causal effect of the treatment not on all the treated but on the treated whose participation into the program has been triggered by the encouragement. The individuals reacting to the encouragement by participating in the program are usually called compliers. The effect of the treatment on the compliers is called the Local Average Treatment Effect. The main identification result for Encouragement designs is that a Wald ratio (an IV estimator) recovers the LATE. It is due to Imbens and Angrist (1994). A key assumption for this result is exclusion restriction: there has to be zero impact of the encouragement on the outcome, except through participation in the treatment. A second key assumption is that no one individual is driven away from participating in the treatment because of the encouragement. This assumption is called monotonicity.</p>
<p>Let’s detail these assumptions, the identification result and the estimation strategy.</p>
<div id="identification-3" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Identification</h3>
<div id="identification-of-the-local-average-treatment-effect" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> Identification of the Local Average Treatment Effect</h4>
<p>Before stating the identification results, let’s go through some definitions and assumptions. We are going to denote <span class="math inline">\(R_i=1\)</span> when individual <span class="math inline">\(i\)</span> receives the encouragement and <span class="math inline">\(R_i=0\)</span> when she does not. As in Section <a href="RCT.html#sec:design3">3.3</a>, we have four potential outcomes for <span class="math inline">\(Y_i\)</span>: <span class="math inline">\(Y_i^{d,r}\)</span>, <span class="math inline">\((d,r)\in\left\{0,1\right\}^2\)</span>, where <span class="math inline">\(d\)</span> denotes receiving the treatment and <span class="math inline">\(r\)</span> receiving the encouragement. We also have two potential outcomes for <span class="math inline">\(D_i\)</span>: <span class="math inline">\(D_i^{r}\)</span>, <span class="math inline">\(r\in\left\{0,1\right\}\)</span>. <span class="math inline">\(D_i^{1}\)</span> indicates whether individual <span class="math inline">\(i\)</span> takes the treatment when receving the encouragement and <span class="math inline">\(D_i^{0}\)</span> whether she takes the treatment when not receiving the encouragement. These potential outcomes define four possible types of individuals, that I’m going to denote with the random variable <span class="math inline">\(T_i\)</span>:</p>
<ul>
<li><strong>Always takers</strong>, who take up the program whether they receive the encouragement or not. They are such that <span class="math inline">\(D_i^{1}=D_i^{0}=1\)</span>. I denote them <span class="math inline">\(T_i=a\)</span>.</li>
<li><strong>Never takers</strong>, who do not take up the program whether they receive the encouragement or not. They are such that <span class="math inline">\(D_i^{1}=D_i^{0}=0\)</span>. I denote them <span class="math inline">\(T_i=n\)</span>.</li>
<li><strong>Compliers</strong>, who take up the program when they receive the encouragement and do not when they do not receive the encouragement. They are such that <span class="math inline">\(D_i^{1}-D_i^{0}=1\)</span>. I denote them <span class="math inline">\(T_i=c\)</span>.</li>
<li><strong>Defiers</strong>, who do not take up the program when they receive the encouragement and take it up when they do not receive the encouragement. They are such that <span class="math inline">\(D_i^{1}-D_i^{0}=-1\)</span>. I denote them <span class="math inline">\(T_i=d\)</span>.</li>
</ul>
<p>We are now ready to state the assumptions needed for identification of the LATE.</p>

<div class="definition">
<p><span id="def:RandEncouragValid" class="definition"><strong>Definition 3.9  (Encouragement Validity)  </strong></span>We assume that the randomized allocation of the program does not interfere with how potential outcomes and self-selection are generated:</p>
<span class="math display">\[\begin{align*}
D_i &amp; = D_i^1R_i+(1-R_i)D_i^0 \\
Y_i &amp; = 
  \begin{cases}
    Y_i^{1,1} &amp; \text{ if } (R_i=1 \text{ and } D_i=1)   \\
    Y_i^{0,1} &amp; \text{ if } (R_i=1 \text{ and } D_i=0)    \\
    Y_i^{1,0} &amp; \text{ if } (R_i=0 \text{ and } D_i=1)    \\
    Y_i^{0,0} &amp; \text{ if } (R_i=0 \text{ and } D_i=0)    
  \end{cases}
\end{align*}\]</span>
</div>
<p> with <span class="math inline">\(Y_i^{1,1}\)</span>, <span class="math inline">\(Y_i^{0,1}\)</span>, <span class="math inline">\(Y_i^{1,0}\)</span>, <span class="math inline">\(Y_i^{0,0}\)</span>, <span class="math inline">\(D_i^1\)</span> and <span class="math inline">\(D^0_i\)</span> the same potential outcomes and self-selection decisions as in a routine allocation of the treatment.</p>

<div class="definition">
<p><span id="def:IndepEncourag" class="definition"><strong>Definition 3.10  (Independence of Encouragement)  </strong></span>We assume that the randomized allocation of the program is well done:</p>
<span class="math display">\[\begin{align*}
(Y_i^{1,1},Y_i^{0,1},Y_i^{0,0},Y_i^{1,0},D_i^1,D^0_i)\Ind R_i|E_i=1.
\end{align*}\]</span>
</div>


<div class="definition">
<p><span id="def:ExclRestr" class="definition"><strong>Definition 3.11  (Exclusion Restriction)  </strong></span>We assume that the randomized allocation of the program does not alter potential outcomes:</p>
<span class="math display">\[\begin{align*}
Y_i^{d,r} &amp; = Y_i^d \text{, }\forall (r,d)\in\left\{0,1\right\}^2.
\end{align*}\]</span>
</div>


<div class="definition">
<p><span id="def:Fstage" class="definition"><strong>Definition 3.12  (First Stage)  </strong></span>We assume that the encouragement does manage to increase participation:</p>
<span class="math display">\[\begin{align*}
  \Pr(D_i=1|R_i=1,E_i=1) &gt; \Pr(D_i=1|R_i=0,E_i=1).
\end{align*}\]</span>
</div>


<div class="definition">
<p><span id="def:Mono" class="definition"><strong>Definition 3.13  (Monotonicity)  </strong></span>We assume that the encouragement either increases participation for everyone or decreases participation for everyone:</p>
<span class="math display">\[\begin{align*}
  \text{ either } \forall i\text{, }D_i^1\geq D_i^0 \text{ or } \forall i\text{, }D_i^1\leq D_i^0 .
\end{align*}\]</span>
</div>

<p>Assumption <a href="RCT.html#def:Mono">3.13</a> means that we cannot have simultaneously individuals that are pushed by the encouragement into the treatment and individuals that are pushed out of the treatment. As a consequence, there cannot be compliers and defiers at the same time. There can only be compliers or defiers. For simplicity, in what follows, I assume that there are no defiers. This is without loss of generality, since, under Assumption <a href="RCT.html#def:Mono">3.13</a>, a redefinition of the treatment (<span class="math inline">\(\tilde{D}_i=-D_i\)</span>) moves the model in this section from one with only defiers to one with only compliers.</p>

<div class="theorem">
<p><span id="thm:IdentLATE" class="theorem"><strong>Theorem 3.9  (Identification in an Encouragement Design)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a>, <a href="RCT.html#def:IndepEncourag">3.10</a>, <a href="RCT.html#def:ExclRestr">3.11</a>, <a href="RCT.html#def:Fstage">3.12</a> and <a href="RCT.html#def:Mono">3.13</a>, the Wald estimator identifies the LATE:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{Wald} &amp; = \Delta^Y_{LATE},
\end{align*}\]</span>
</div>

<p>with:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{Wald} &amp; = \frac{\esp{Y_i|R_i=1,E_i=1} - \esp{Y_i|R_i=0,E_i=1}}{\Pr(D_i=1|R_i=1,E_i=1)-\Pr(D_i=1|R_i=0,E_i=1)}\\
  \Delta^Y_{LATE} &amp; = \esp{Y^1_i-Y^0_i|T_i=c,E_i=1}.
\end{align*}\]</span>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> See Section <a href="proofs.html#proofIdentLATE">A.2.1</a>.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> Theorem <a href="RCT.html#thm:IdentLATE">3.9</a> is pretty amazing. It shows that there exists a set of assumptions under which we can use an encouragement design to recover the effect of the treatment (<span class="math inline">\(D_i\)</span>) on outcomes, despite the fact that we have NOT randomized <span class="math inline">\(D_i\)</span>. The assumptions needed for that to happen are intuitive:
</div>

<ol style="list-style-type: decimal">
<li>The encouragement has to have no direct effect on the outcomes (Assumption <a href="RCT.html#def:ExclRestr">3.11</a>)</li>
<li>The encouragement has to have an effect on treatment uptake (Assumption <a href="RCT.html#def:Fstage">3.12</a>)</li>
<li>The encouragement does not generate two-way flows in and out of the treatment, but only a one-way flow (Assumption <a href="RCT.html#def:Mono">3.13</a>)</li>
</ol>
<p>Under these assumptions, the only way that we can see a difference in outcomes between those that receive the encouragement and those that do not is that the treatment has had an effect on those that have taken it because of the encouragement. It cannot be because of the encouragement itself, because of Assumption <a href="RCT.html#def:ExclRestr">3.11</a>. It cannot be because some people with particularly low outcomes have exited the program because of the encouragement, Assumption <a href="RCT.html#def:Mono">3.13</a> forbids it. And if we see no effect of the encouragement, it has to be that the treatment has no effect on the compliers as well, because Assumption <a href="RCT.html#def:Fstage">3.12</a> implies that they have received the treatment in the encouragement group and that they have not in the group without encouragement.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Less nice with Theorem <a href="RCT.html#thm:IdentLATE">3.9</a> is that we recover the effect only for a subgroup of individuals, the compliers. This raises two issues:
</div>

<ol style="list-style-type: decimal">
<li>The effect on the compliers (or LATE) is not the effect on the treated (TT). When the treatment is given in routine mode, without the encouragement, TT is actually equal to the effect on the always takers. There is nothing that tells us that the always takers react in the same way to the treatment as the compliers. As soon as the expected benefits of the treatment enter the decision of taking it up, always takers have larger treatment effects than compliers.</li>
<li>The identity of the compliers is unobserved. We cannot decide to allocate the treatment only to the compliers because they are defined by their counterfactual response to the encouragement. In both treatment arms, we do not know who the compliers are. We know they are among those who take up the program in the group receiving the encouragement. But there are also always takers that take up the program in this group. We know that they are among those that do not take up the program in the group that does not receive the encouragement. But never takers behave in the same way in that group.</li>
</ol>
<p>The only way to direct the treatment at the compliers is to use the encouragement. So, we end up evaluating the effect of the encouragement itself and not of the program. In that case, we do not need Assumptions <a href="RCT.html#def:ExclRestr">3.11</a>, <a href="RCT.html#def:Fstage">3.12</a> and <a href="RCT.html#def:Mono">3.13</a>, because they are not needed to identify the effect of the encouragement (see Section <a href="RCT.html#ITEEncourag">3.4.1.2</a> below).</p>
<p>In general, researchers believe that LATE tells them something about the magnitude of the effect beyond compliers. This is not warranted by the maths, but one can understand how a bayesian decision-maker may use the information from some subpopulation to infer what would happen to another. Comparing LATEs and TTs for similar treatments is an active area for reasearch. I know of no paper doing that extensively and nicely.</p>
<p>To generalize from the LATE to the TT, we can make the assumption that the impact on always takers is equal to the impact on compliers, but that seems a little far-fetched. <a href="https://www.nber.org/papers/w16566">Angrist and Fernandez-Val</a> propose to assume that the effect on compliers is equal to the effect on always takers conditional on some observed covariates. When outcomes are bounded (for example because they are between zero and one), we can try to bound the <span class="math inline">\(TT\)</span> using the <span class="math inline">\(LATE\)</span> (see <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2473">Huber, Laffers and Mellace (2017)</a>).</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> If you see a connexion between the conditions for the Wald estimator to identify LATE and the assumptions behind an IV estimator, you’re correct. The Wald estimator is actually an IV estimator (see Lemma <a href="#lem:WaldIV"><strong>??</strong></a> below).
</div>

</div>
<div id="ITEEncourag" class="section level4">
<h4><span class="header-section-number">3.4.1.2</span> Identification of the Intention to Treat Effect</h4>
<p>In this section, we are going to delineate how to identify the Intention to Treat Effect (ITE) in an Encouragement design. In an Encouragement design, ITE is the effect of receiving the encouragement. It is defined in a similar manner as in a Randomization After Eligibility design (see Definition <a href="RCT.html#def:ITE">3.8</a>): <span class="math inline">\(\Delta^Y_{ITE} = \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|E_i=1}\)</span>.</p>
<p>Under Assumption <a href="RCT.html#def:RandEncouragValid">3.9</a>, receiving the encouragement has several impacts:</p>
<ol style="list-style-type: decimal">
<li>Some individuals (the compliers) decide to enter the program,</li>
<li>Some individuals (the defiers) decide to exit the program,</li>
<li>The encouragement might have a direct effect on outcomes (<span class="math inline">\(Y_i^{d,1}\neq Y_i^{d,0}\)</span>).</li>
</ol>
<p>This last effect is the effect of receiving the encouragement that does not goes through participation into the program. For example, it is possible that sending an encouragement to take up a retirement program makes me save more for retirement, even if I end up not taking up the proposed program.</p>
<p>The two causal channels that are at work within the ITE can be seen more clearly when decomposing the ITE to make each type appear. We can do that because the four types define a partition of the sample space, that is a collection of mutually exclusive events whose union spans the whole space. As a consequence of that, conditioning on the union of the four types is the same thing as not conditioning on anything. Using this trick, we have:</p>
<span class="math display" id="eq:ITE3">\[\begin{align}
  \Delta^Y_{ITE} &amp; = \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|(T_i=a\cup T_i=c\cup T_i=d\cup T_i=n)\cap E_i=1}\nonumber\\
                &amp; = \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|T_i=a,E_i=1}\Pr(T_i=a|E_i=1)\nonumber\\
                &amp; \phantom{=}+ \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|T_i=c,E_i=1}\Pr(T_i=c|E_i=1)\nonumber\\
                &amp; \phantom{=}+ \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|T_i=d,E_i=1}\Pr(T_i=d|E_i=1)\nonumber\\
                &amp; \phantom{=}+ \esp{Y_i^{D_i^1,1}-Y_i^{D_i^0,0}|T_i=n,E_i=1}\Pr(T_i=n|E_i=1)\nonumber\\
                &amp; = \esp{Y_i^{1,1}-Y_i^{1,0}|T_i=a,E_i=1}\Pr(T_i=a|E_i=1)\nonumber\\
                &amp; \phantom{=}+ \esp{Y_i^{1,1}-Y_i^{0,0}|T_i=c,E_i=1}\Pr(T_i=c|E_i=1)\nonumber\\
                &amp; \phantom{=}+ \esp{Y_i^{0,1}-Y_i^{1,0}|T_i=d,E_i=1}\Pr(T_i=d|E_i=1)\nonumber\\
                &amp; \phantom{=}+ \esp{Y_i^{0,1}-Y_i^{0,0}|T_i=n,E_i=1}\Pr(T_i=n|E_i=1),\tag{3.2}
\end{align}\]</span>
<p>where the first equality follows from the four types defining a partition of the sample space, the second equality from the usual rule of conditional expectations and the fact that types are disjoint events, and the third equality from Assumption <a href="RCT.html#def:randeligvalid">3.6</a>.</p>
<p>We can now see that ITE is composed of four terms:</p>
<ol style="list-style-type: decimal">
<li>The effect of receiving the encouragment on the always takers. This effect is only the direct effect of the encouragement, and not the effect of the program since the always takers always take the program. This term cancels under Assumption <a href="RCT.html#def:ExclRestr">3.11</a>, when there is no direct effect of the encouragement on outcomes.</li>
<li>The effect of receiving the encouragement on compliers. This is both the effect of the encouragement and of the program. This is equal to the LATE under Assumption <a href="RCT.html#def:ExclRestr">3.11</a>.</li>
<li>The effect of receiving the encouragement on defiers. This is the difference between the direct effect of the encouragement and the effect of the program. This is equal to the opposite of the effect of the treatment on the defiers under Assumption <a href="RCT.html#def:ExclRestr">3.11</a>.</li>
<li>The effect of receiving the encouragement on never takers. This effect is only the direct effect of the encouragement, and not the effect of the program since the never takers never take the program. This term cancels under Assumption <a href="RCT.html#def:ExclRestr">3.11</a>.</li>
</ol>
<p>All these effects are weighted by the respective proportions of the types in the population. ITE is linked to LATE. This link can be made clearer:</p>

<div class="theorem">
<p><span id="thm:ITELATE" class="theorem"><strong>Theorem 3.10  (From ITE to Compliers and Defiers)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a> and <a href="RCT.html#def:ExclRestr">3.11</a>, ITE is equal to the effect on compliers minus the effect on defiers weighted by their respective proportions in the population:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \esp{Y_i^{1}-Y_i^{0}|T_i=c,E_i=1}\Pr(T_i=c|E_i=1)\\
                  &amp; \phantom{=}-\esp{Y_i^{1}-Y_i^{0}|T_i=d,E_i=1}\Pr(T_i=d|E_i=1).
\end{align*}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Under Assumption <a href="RCT.html#def:ExclRestr">3.11</a>, Equation <a href="RCT.html#eq:ITE3">(3.2)</a> becomes:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \esp{Y_i^{1}-Y_i^{1}|T_i=a,E_i=1}\Pr(T_i=a|E_i=1)\\
                &amp; \phantom{=}+ \esp{Y_i^{1}-Y_i^{0}|T_i=c,E_i=1}\Pr(T_i=c|E_i=1)\\
                &amp; \phantom{=}+ \esp{Y_i^{0}-Y_i^{1}|T_i=d,E_i=1}\Pr(T_i=d|E_i=1)\\
                &amp; \phantom{=}+ \esp{Y_i^{0}-Y_i^{0}|T_i=n,E_i=1}\Pr(T_i=n|E_i=1)
\end{align*}\]</span>
which proves the result.
</div>

<p>The previous theorem shows that Assumption <a href="RCT.html#def:ExclRestr">3.11</a> shuts down any direct effect of receiving the encouragement on outcomes. As a consequence of this assumption, the only impact that receiving the encouragement has on outcomes is through participation into the program. Hence, ITE is equal to the impact of the program on those who react to the encouragement: the compliers and the defiers, weighted by their respective proportions.</p>
<p>The problem with the result of Theorem <a href="RCT.html#thm:ITELATE">3.10</a> is that ITE contains two-way flows in and out of the program. If we want to know something about the effect of the program, and not only about the effect of the encouragement, we need to assume that defiers do not exist. That’s what Assumption <a href="RCT.html#def:Mono">3.13</a> does, as the following theorem shows:</p>

<div class="theorem">
<p><span id="thm:ITELATEMono" class="theorem"><strong>Theorem 3.11  (From ITE to LATE)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a>, <a href="RCT.html#def:ExclRestr">3.11</a> and <a href="RCT.html#def:Mono">3.13</a>, ITE is equal to the LATE multiplied by the proportion of compliers in the population:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \Delta^Y_{LATE}\Pr(T_i=c|E_i=1).
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> The result is straighforward using Assumption <a href="RCT.html#def:Mono">3.13</a> and Theorem <a href="RCT.html#thm:ITELATE">3.10</a>. Indeed, Assumption <a href="RCT.html#def:Mono">3.13</a> implies that <span class="math inline">\(\forall i\)</span>, <span class="math inline">\(D_i^1-D_i^0\geq 0\)</span> (choosing only the first “either” statement, without loss of generality). As a consequence, <span class="math inline">\(\Pr(T_i=d|E_i=1)=\Pr(D_i^1-D_i^0=-1|E_i=1)=0\)</span>.
</div>

<p>In order to move from the link between LATE and ITE to the mechanics of the Wald estimator, we need two additional identification results. The first result shows that ITE can be identified under fairly light conditions by a WW estimator. The second result shows that the proportion of people taking up the treatment when eligiblity is announced is also easily estimated from the data.</p>

<div class="theorem">
<p><span id="thm:ITEEncourag" class="theorem"><strong>Theorem 3.12  (Identification of ITE in an Encouragement Design)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a> and <a href="RCT.html#def:IndepEncourag">3.10</a>, ITE is identified by the With/Without comparison among eligibles:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{ITE} &amp; = \Delta^Y_{WW|E_i=1}.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{align*}
 \Delta^Y_{WW|E=1} &amp; =\esp{Y_i|R_i=1,E_i=1}-\esp{Y_i|R_i=0,E_i=1} \\
                   &amp; = \esp{Y_i^{D_i^1,1}|R_i=1,E_i=1}-\esp{Y_i^{D_i^0,0}|R_i=0,E_i=1} \\
                    &amp; = \esp{Y_i^{D_i^1,1}|E_i=1}-\esp{Y_i^{D_i^0,0}|E_i=1},
\end{align*}\]</span>
where the second equality follows from Assumption <a href="RCT.html#def:RandEncouragValid">3.9</a> and the third from Assumption <a href="RCT.html#def:IndepEncourag">3.10</a>.
</div>


<div class="theorem">
<p><span id="thm:prEncourag" class="theorem"><strong>Theorem 3.13  (Identification of the Proportion of Compliers)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a>, <a href="RCT.html#def:IndepEncourag">3.10</a> and <a href="RCT.html#def:Mono">3.13</a>, the proportion of compliers is identified by the difference between the proportion of people taking up the program among those receiving the encouragement and the proportion of individuals taking up the program among those not receiving the encouragement:</p>
<span class="math display">\[\begin{align*}
  \Pr(T_i=c|E_i=1) &amp; = \Pr(D_i=1|R_i=1,E_i=1)-\Pr(D_i=1|R_i=0,E_i=1).
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
<span class="math display">\[\begin{align*}
 \Pr(D_i=1|R_i=1,E_i=1) &amp; =\Pr(D_i=1\cap (T_i=a\cup T_i=c\cup T_i=d\cup T_i=n)|R_i=1,E_i=1) \\
                        &amp; = \Pr(D_i=1\cap T_i=a|R_i=1,E_i=1)\\
                        &amp; \phantom{=}+ \Pr(D_i=1\cap T_i=c|R_i=1,E_i=1)\\
                        &amp; \phantom{=} +\Pr(D_i=1\cap T_i=d|R_i=1,E_i=1)\\
                        &amp; \phantom{=} +\Pr(D_i=1\cap T_i=n|R_i=1,E_i=1)\\
                        &amp; = \Pr(T_i=a|R_i=1,E_i=1)\\
                        &amp; \phantom{=} +\Pr(T_i=c|R_i=1,E_i=1)\\
                        &amp; = \Pr(T_i=a|E_i=1)\\
                        &amp; \phantom{=} +\Pr(T_i=c|E_i=1),
\end{align*}\]</span>
<p>where the first equality follows the types being a partition of the sample space; the second equality from the fact that the types are disjoint sets; the third equality from the fact that <span class="math inline">\(T_i=a|R_i=1 \Rightarrow D_i=1\)</span> (so that <span class="math inline">\(\Pr(D_i=1\cap T_i=a|R_i=1,E_i=1)=\Pr(T_i=a|R_i=1,E_i=1)\)</span>), <span class="math inline">\(T_i=c|R_i=1 \Rightarrow D_i=1\)</span> (so that <span class="math inline">\(\Pr(D_i=1\cap T_i=c|R_i=1,E_i=1)=\Pr(T_i=c|R_i=1,E_i=1)\)</span>), <span class="math inline">\(T_i=d|R_i=1 \Rightarrow D_i=0\)</span> (so that <span class="math inline">\(\Pr(D_i=1\cap T_i=d|R_i=1,E_i=1)=0\)</span>) and <span class="math inline">\(T_i=n|R_i=1 \Rightarrow D_i=0\)</span> (so that <span class="math inline">\(\Pr(D_i=1\cap T_i=n|R_i=1,E_i=1)=0\)</span>); and the fourth equality from Assumption <a href="RCT.html#def:IndepEncourag">3.10</a> and Lemma <a href="proofs.html#lem:UnconfTypes">A.6</a>.</p>
</div>


<div class="corollary">
<p><span id="cor:EncouragWald" class="corollary"><strong>Corollary 3.2  (Wald estimator and ITE)  </strong></span>It follows from Theorems <a href="RCT.html#thm:ITEEncourag">3.12</a> and <a href="RCT.html#thm:prEncourag">3.13</a> that, under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a>, <a href="RCT.html#def:IndepEncourag">3.10</a> and <a href="RCT.html#def:Mono">3.13</a>, the Wald estimator is equal to the ITE divided by the propotion of compliers:</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{Wald|E=1} &amp; = \frac{\Delta^Y_{ITE}}{\Pr(T_i=c|E_i=1)}.
\end{align*}\]</span>
</div>

<p>As a consequence of Corollary <a href="RCT.html#cor:EncouragWald">3.2</a>, we see that the Wald estimator reweights the ITE, the effect of receiving an encouragement, by the proportion of people reacting to the encouragement by participating in the program, the compliers. From Theorem <a href="RCT.html#thm:ITELATE">3.10</a>, we know that this ratio will be equal to LATE if the Assumption <a href="RCT.html#def:ExclRestr">3.11</a> also holds, so that all the impact of the encouragement stems from entering the program. The encouragement serves as an instrument for program participation.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> The Encouragement design seems like magic. You do not assign randomly the program, but only an encouragement to take it, and you can recover the effect of the program anyway. The Encouragement design is less intrusive than Randomization After Self-Selection and Randomization of Eligibility. In an Encouragement design, you do not have to refuse the program to agents in the control group. You pay two types of prices for that:
</div>

<ol style="list-style-type: decimal">
<li>You only recover LATE, not TT</li>
<li>You have larger sampling noise.</li>
</ol>
<p>The intuition for this second point can be delineated using the very same apparatus that we have developed so far. So here goes. Under the assumptions made so far, it is easy to show that (omitting the conditioning on <span class="math inline">\(E_i=1\)</span> for simplicity):</p>
<span class="math display">\[\begin{align*}
  \Delta^Y_{WW|E=1} &amp; = \esp{Y_i^{1,1}|T_i=a,R_i=1}\Pr(T_i=a,|R_i=1)\\
                    &amp; \phantom{=}-\esp{Y_i^{1,0}|T_i=a,R_i=0}\Pr(T_i=a,|R_i=0)\\
                    &amp; \phantom{=}+ \esp{Y_i^{1,1}|T_i=c,R_i=1}\Pr(T_i=c|R_i=1)\\
                    &amp; \phantom{=}-\esp{Y_i^{0,0}|T_i=c,R_i=0}\Pr(T_i=c|R_i=0)\\
                    &amp; \phantom{=}+ \esp{Y_i^{0,1}|T_i=d,R_i=1}\Pr(T_i=d|R_i=1)\\
                    &amp; \phantom{=}-\esp{Y_i^{1,0}|T_i=d,R_i=0}\Pr(T_i=d|R_i=0)\\
                    &amp; \phantom{=}+ \esp{Y_i^{0,1}|T_i=n,R_i=1}\Pr(T_i=n|R_i=1)\\
                    &amp; \phantom{=}-\esp{Y_i^{0,0}|T_i=n,R_i=0}\Pr(T_i=n|R_i=0).
\end{align*}\]</span>
<p>The four parts of the equation account for comparisons among each type between the two treatment arms. The parts due to always takers and never takers cancel out under Assumptions <a href="RCT.html#def:IndepEncourag">3.10</a> and <a href="RCT.html#def:ExclRestr">3.11</a>. But this cancelling out only happens in the population. In a given sample, the sample equivalents of the two members of each difference do not have to be equal, and thus they do not cancel out, generating sampling noise. Ideally, we would like to enforce that the effect of the encouragement on always takers and never takers is null, as Assumption <a href="RCT.html#def:ExclRestr">3.11</a> imposes, but that would require observing the type variable <span class="math inline">\(T_i\)</span>. Unfortunately, we cannot now the type of each individual in the sample, since it is defined counterfactually. Maybe someday we’ll be able to use prior responses to the encouragement to identify the type of each individual and thus improve the precision of the Wald estimator.</p>
<p><strong><span style="font-variant: small-caps;">Explain de Chaisemartin.</span></strong></p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> what if we fear there are defiers. de Chaisemartin
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> In practice, we use a pseudo-RNG to allocate the randomized annoucement of the encouragement:
</div>

<span class="math display">\[\begin{align*}
  R_i^* &amp; \sim \mathcal{U}[0,1]\\
  R_i &amp; = 
  \begin{cases}
    1 &amp; \text{ if } R_i^*\leq .5 \land E_i=1\\
    0 &amp; \text{ if } R_i^*&gt; .5 \land E_i=1
  \end{cases} \\
  D_i &amp; = \uns{\bar{\alpha}+\theta\bar{\mu}+\psi R_i-C_i\geq0 \land E_i=1}
\end{align*}\]</span>
<p><span class="math inline">\(\psi\)</span> denotes the increase in agents’ valuation of the program after receiving the encouragement.</p>

<div class="example">
<span id="exm:unnamed-chunk-117" class="example"><strong>Example 3.22  </strong></span>Let’s see how the encouragement design works in our numerical example. Let’s choose a value for <span class="math inline">\(\psi\)</span> and add it to the vector of parameters.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">param &lt;-<span class="st"> </span><span class="kw">c</span>(param,<span class="fl">0.6</span>)
<span class="kw">names</span>(param) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;barmu&quot;</span>,<span class="st">&quot;sigma2mu&quot;</span>,<span class="st">&quot;sigma2U&quot;</span>,<span class="st">&quot;barY&quot;</span>,<span class="st">&quot;rho&quot;</span>,<span class="st">&quot;theta&quot;</span>,<span class="st">&quot;sigma2epsilon&quot;</span>,<span class="st">&quot;sigma2eta&quot;</span>,<span class="st">&quot;delta&quot;</span>,<span class="st">&quot;baralpha&quot;</span>,<span class="st">&quot;barc&quot;</span>,<span class="st">&quot;gamma&quot;</span>,<span class="st">&quot;sigma2V&quot;</span>,<span class="st">&quot;psi&quot;</span>)</code></pre></div>
<p>Let’s first compute the value of LATE in this new model. Let’s denote <span class="math inline">\(D_i^{*0}=\bar{\alpha}+\theta\bar{\mu}-C_i\)</span> the utility of agent <span class="math inline">\(i\)</span> absent the encouragement, with <span class="math inline">\(C_i=\bar{c} + \gamma \mu_i + V_i\)</span>. In order to be a complier, you have to have a utility of the program that is insufficient to make you apply for the program when you receive no encouragement (<span class="math inline">\(D_i^{*0}&lt;0\)</span>) and a positive utility of applying to the treatment after receiving the encouragement (<span class="math inline">\(D_i^{*0}+\psi\geq 0\)</span>). Compliers are thus such that <span class="math inline">\(-\psi\leq D_i^{*0}&lt;0\)</span>. LATE can thus be written as follows in our model:</p>
<span class="math display">\[\begin{align*}
  \Delta^y_{LATE} &amp; = \bar{\alpha}+ \theta\esp{\mu_i|\mu_i+U_i^B\leq\bar{y} \land -\psi\leq D_i^{*0}&lt;0},
\end{align*}\]</span>
<p>Using the same approach, we can also compute the proportion of compliers among eligibles. In our model, we indeed have:</p>
<span class="math display">\[\begin{align*}
  \Pr(T_i=c|E_i=1) &amp; = \Pr(-\psi\leq D_i^{*0}&lt;0|\mu_i+U_i^B\leq\bar{y}).
\end{align*}\]</span>
<p>Since all errors terms are normally distributed in our model, we can compute the package <code>tmvtnorm</code> to compute both LATE and the proportion of compliers among eligibles.</p>
<span class="math display">\[\begin{align*}
  (\mu_i,y_i^B,D_i^{*0}) &amp; = \mathcal{N}\left(\bar{\mu},\bar{\mu},\bar{\alpha}+(\theta-\gamma)\bar{\mu}-\bar{c},
                                        \left(\begin{array}{ccc}
                                              \sigma^2_{\mu} &amp; \sigma^2_{\mu} &amp; -\gamma\sigma^2_{\mu} \\
                                              \sigma^2_{\mu} &amp; \sigma^2_{\mu} + \sigma^2_{U} &amp; -\gamma\sigma^2_{\mu} \\
                                              -\gamma\sigma^2_{\mu} &amp; -\gamma\sigma^2_{\mu} &amp; \gamma^2\sigma^2_{\mu}+\sigma^2_{V}
                                              \end{array}
                                        \right)
                                      \right)
\end{align*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean.mu.yB.Dstar &lt;-<span class="st"> </span><span class="kw">c</span>(param[<span class="st">&#39;barmu&#39;</span>],param[<span class="st">&#39;barmu&#39;</span>],param[<span class="st">&#39;baralpha&#39;</span>]<span class="op">-</span><span class="st"> </span>param[<span class="st">&#39;barc&#39;</span>]<span class="op">+</span>(param[<span class="st">&#39;theta&#39;</span>]<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>])<span class="op">*</span>param[<span class="st">&#39;barmu&#39;</span>])
cov.mu.yB.Dstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(param[<span class="st">&#39;sigma2mu&#39;</span>],param[<span class="st">&quot;sigma2mu&quot;</span>],<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],
                            param[<span class="st">&quot;sigma2mu&quot;</span>],param[<span class="st">&#39;sigma2mu&#39;</span>]<span class="op">+</span>param[<span class="st">&#39;sigma2U&#39;</span>],<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],
                            <span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],<span class="op">-</span>param[<span class="st">&#39;gamma&#39;</span>]<span class="op">*</span>param[<span class="st">&quot;sigma2mu&quot;</span>],param[<span class="st">&quot;sigma2mu&quot;</span>]<span class="op">*</span>(param[<span class="st">&#39;gamma&#39;</span>])<span class="op">^</span><span class="dv">2</span><span class="op">+</span>param[<span class="st">&#39;sigma2V&#39;</span>]),<span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">byrow=</span><span class="ot">TRUE</span>)
<span class="co"># late</span>
lower.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span>param[<span class="st">&#39;psi&#39;</span>])
upper.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">Inf</span>,<span class="kw">log</span>(param[<span class="st">&#39;barY&#39;</span>]),<span class="dv">0</span>)
moments.cut &lt;-<span class="st"> </span><span class="kw">mtmvnorm</span>(<span class="dt">mean=</span>mean.mu.yB.Dstar,<span class="dt">sigma=</span>cov.mu.yB.Dstar,<span class="dt">lower=</span>lower.cut,<span class="dt">upper=</span>upper.cut)
delta.y.late &lt;-<span class="st"> </span>param[<span class="st">&#39;baralpha&#39;</span>]<span class="op">+</span><span class="st"> </span>param[<span class="st">&#39;theta&#39;</span>]<span class="op">*</span>moments.cut<span class="op">$</span>tmean[<span class="dv">1</span>]
<span class="co"># proportion of compliers</span>
lower.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span><span class="ot">Inf</span>,<span class="op">-</span><span class="ot">Inf</span>)
upper.cut &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">Inf</span>,<span class="kw">log</span>(param[<span class="st">&#39;barY&#39;</span>]),<span class="ot">Inf</span>)
pr.compliers &lt;-<span class="st"> </span><span class="kw">ptmvnorm.marginal</span>(<span class="dt">xn=</span><span class="dv">0</span>,<span class="dt">n=</span><span class="dv">3</span>,<span class="dt">mean=</span>mean.mu.yB.Dstar,<span class="dt">sigma=</span>cov.mu.yB.Dstar,<span class="dt">lower=</span>lower.cut,<span class="dt">upper=</span>upper.cut)<span class="op">-</span><span class="kw">ptmvnorm.marginal</span>(<span class="dt">xn=</span><span class="op">-</span>param[<span class="st">&#39;psi&#39;</span>],<span class="dt">n=</span><span class="dv">3</span>,<span class="dt">mean=</span>mean.mu.yB.Dstar,<span class="dt">sigma=</span>cov.mu.yB.Dstar,<span class="dt">lower=</span>lower.cut,<span class="dt">upper=</span>upper.cut)
delta.y.ite &lt;-<span class="st"> </span>delta.y.late<span class="op">*</span>pr.compliers</code></pre></div>
<p>The value of <span class="math inline">\(\Delta^y_{LATE}\)</span> in the population is thus 0.173. The proportion of compliers among eligibles in the population is 0.272. As a consequence of Corollary <a href="RCT.html#cor:EncouragWald">3.2</a>, we can compute ITE as the product of LATE and the proportion of compliers. In our example, ITE is thus equal to 0.047 in the population.</p>
<p>Now let’s simulate a new sample with the encouragement delivered randomly among eligibles. I’m also defining the potential outcomes <span class="math inline">\(D^1_i\)</span> and <span class="math inline">\(D^0_i\)</span> and the types <span class="math inline">\(T_i\)</span> for later use.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># I&#39;m changing the seed because with the usual one, I get a negative estimate of the treatment effect: lots of sampling noise!</span>
<span class="kw">set.seed</span>(<span class="dv">12345</span>)
N &lt;-<span class="dv">1000</span>
mu &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,param[<span class="st">&quot;barmu&quot;</span>],<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2mu&quot;</span>]))
UB &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2U&quot;</span>]))
yB &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st"> </span>UB 
YB &lt;-<span class="st"> </span><span class="kw">exp</span>(yB)
epsilon &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2epsilon&quot;</span>]))
eta&lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="kw">sqrt</span>(param[<span class="st">&quot;sigma2eta&quot;</span>]))
U0 &lt;-<span class="st"> </span>param[<span class="st">&quot;rho&quot;</span>]<span class="op">*</span>UB <span class="op">+</span><span class="st"> </span>epsilon
y0 &lt;-<span class="st"> </span>mu <span class="op">+</span><span class="st">  </span>U0 <span class="op">+</span><span class="st"> </span>param[<span class="st">&quot;delta&quot;</span>]
alpha &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span><span class="st">  </span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>mu <span class="op">+</span><span class="st"> </span>eta
y1 &lt;-<span class="st"> </span>y0<span class="op">+</span>alpha
Y0 &lt;-<span class="st"> </span><span class="kw">exp</span>(y0)
Y1 &lt;-<span class="st"> </span><span class="kw">exp</span>(y1)

<span class="co">#random allocation of encouragement among eligibles</span>
E &lt;-<span class="st"> </span><span class="kw">ifelse</span>(YB<span class="op">&lt;=</span>param[<span class="st">&quot;barY&quot;</span>],<span class="dv">1</span>,<span class="dv">0</span>)
Rs &lt;-<span class="st"> </span><span class="kw">runif</span>(N)
R &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Rs<span class="op">&lt;=</span>.<span class="dv">5</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
V &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,param[<span class="st">&quot;sigma2V&quot;</span>])
Dindex &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;psi&quot;</span>]<span class="op">*</span>R<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
Ds &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
y &lt;-<span class="st"> </span>y1<span class="op">*</span>Ds<span class="op">+</span>y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
Y &lt;-<span class="st"> </span>Y1<span class="op">*</span>Ds<span class="op">+</span>Y0<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>Ds)
D &lt;-<span class="st"> </span>Ds

<span class="co"># types</span>
Dindex1 &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;psi&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
Dindex0 &lt;-<span class="st"> </span>param[<span class="st">&quot;baralpha&quot;</span>]<span class="op">+</span>param[<span class="st">&quot;theta&quot;</span>]<span class="op">*</span>param[<span class="st">&quot;barmu&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;barc&quot;</span>]<span class="op">-</span>param[<span class="st">&quot;gamma&quot;</span>]<span class="op">*</span>mu<span class="op">-</span>V
D1 &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex1<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
D0 &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Dindex0<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
AT &lt;-<span class="st"> </span><span class="kw">ifelse</span>(D1<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>D0<span class="op">==</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>)
NT &lt;-<span class="st"> </span><span class="kw">ifelse</span>(D1<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>D0<span class="op">==</span><span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)
Co &lt;-<span class="st"> </span><span class="kw">ifelse</span>(D1<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>D0<span class="op">==</span><span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>)

<span class="co"># figures</span>
Ncompliers &lt;-<span class="st"> </span><span class="kw">sum</span>(Co)
NElig &lt;-<span class="st"> </span><span class="kw">sum</span>(E)
PrCoElig &lt;-<span class="st"> </span>Ncompliers<span class="op">/</span>NElig
LATEs &lt;-<span class="st"> </span><span class="kw">mean</span>(alpha[Co<span class="op">==</span><span class="dv">1</span>])
ITEs &lt;-<span class="st"> </span>LATEs<span class="op">*</span>PrCoElig</code></pre></div>
<p>In our sample of <span class="math inline">\(N=\)</span> 1000 individuals, there are only 216 eligibles, and among them 67 compliers. The proportion of compliers among eligibles is thus 0.31. Sample size decreases fast in an encouragement design. The sample LATE is equal to 0.14. The sample ITE is equal to 0.044.</p>
</div>
</div>
<div id="estimating-the-local-average-treatment-effect-and-the-intention-to-treat-effect" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Estimating the Local Average Treatment Effect and the Intention to Treat Effect</h3>
<p>Classically, we present the results of an Encouragement design in three stages:</p>
<ol style="list-style-type: decimal">
<li>We show the <strong>first stage</strong> regression of <span class="math inline">\(D_i\)</span> on <span class="math inline">\(R_i\)</span>: this estimates the impact of the encouragement on participation into the program and estimates the proportion of compliers.</li>
<li>We show the <strong>reduced form</strong> regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(R_i\)</span>: this estimates the impact of the encouragement on outcomes, also called ITE.</li>
<li>We finally show the <strong>structural</strong> regression of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(D_i\)</span> using <span class="math inline">\(R_i\)</span> as an instrument, which estimates the LATE.</li>
</ol>
<div id="FStage" class="section level4">
<h4><span class="header-section-number">3.4.2.1</span> First stage regression</h4>
<p>The first stage regression is simply to get an estimate of the effect of the encouragement on participation into the program. If there is no effect of the encouragement on participation, we might as well stop there, since there will be no compliers and no effect to estimate. Note that if we observe an effect on the encouragement on outcomes without any effect on participation, we have to accept the fact that the encouragement might have had a direct effect on outcomes and thus that the exclusion restriction assumption does not hold.</p>
<p>Let’s denote this effect of <span class="math inline">\(R_i\)</span> on <span class="math inline">\(D_i\)</span> <span class="math inline">\(\Delta^{D,R}_{TT}=\esp{D_i^1-D_i^0|R_i=1,E_i=1}\)</span>. It is a treatment on the treated since we want to estimate the effect of the cnouragement onthose who have received it. Actually, <span class="math inline">\(\Delta^{D,R}_{TT}\)</span> is also equal to <span class="math inline">\(\Delta^{D,R}_{ATE}\)</span>, since those who have received the encouragement are a random sample of the eligibles.</p>
<p>How to estimate the effect of <span class="math inline">\(R_i\)</span> on <span class="math inline">\(D_i\)</span>? When estimating the effect of the encouragement, we are in a Brute Force design among eligibles, so that the appropriate estimator is the With/Without estimator among eligibles:</p>

<div class="theorem">
<span id="thm:EncouragFStage" class="theorem"><strong>Theorem 3.14  (Identification of the First Stage Effect in an Encouragment Design)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a> and <a href="RCT.html#def:IndepEncourag">3.10</a>, the WW estimator identifies the First Stage Effect (the effect of <span class="math inline">\(R_i\)</span> on <span class="math inline">\(D_i\)</span>):
<span class="math display">\[\begin{align*}
  \Delta^{D,R}_{WW} &amp; = \Delta^{D,R}_{TT}.
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> This is a direct consequence of Theorem <a href="RCT.html#thm:BFATE">3.1</a>.
</div>

As we have seen in Chapter @ref{FPCI}, the WW estimator is identical to an OLS estimator: The OLS coefficient <span class="math inline">\(\beta\)</span> in the following regression:
<span class="math display">\[\begin{align*}
    D_i &amp;  = \alpha +  \beta R_i + U_i
    \end{align*}\]</span>
<p>is thus the <span class="math inline">\(WW\)</span> estimator.</p>
Finally, the advantage of using OLS other the direct WW comparison is that it gives you a direct estimate of sampling noise (see next section) but also that it enables you to condition on additional covariates in the regression: The OLS coefficient <span class="math inline">\(\beta\)</span> in the following regression:
<span class="math display">\[\begin{align*}
    D_i &amp;  = \alpha +  \beta R_i + \gamma&#39; X_i + U_i
    \end{align*}\]</span>
<p>is a consistent (and even unbiased) estimate of the ATE.</p>
<p><strong><span style="font-variant: small-caps;">Center covariates at mean?</span></strong></p>

<div class="example">
<span id="exm:unnamed-chunk-119" class="example"><strong>Example 3.23  </strong></span>In our numerical example, we can compare all these estimators.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">WW.D.R &lt;-<span class="st"> </span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">0</span>])
reg.D.R.ols &lt;-<span class="st"> </span><span class="kw">lm</span>(D[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>R[E<span class="op">==</span><span class="dv">1</span>])
reg.D.R.ols.yB &lt;-<span class="st"> </span><span class="kw">lm</span>(D[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>R[E<span class="op">==</span><span class="dv">1</span>]<span class="op">+</span>yB[E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p><span class="math inline">\(\hat{\Delta}^{D,R}_{WW} =\)</span> 0.213, while <span class="math inline">\(\hat{\Delta}^{D,R}_{OLS}=\)</span> 0.213 which is exactly equal, as expected, to the WW estimator. When controlling for <span class="math inline">\(y^B_i\)</span>, we have: <span class="math inline">\(\hat{\Delta}^{D,R}_{OLS(y^B)}=\)</span> 0.233.</p>
<p>Under monotonicity, <span class="math inline">\(\Delta^{D,R}_{TT}\)</span> is equal to the proportion of compliers among eligibles. Indeed, this is the proportion of eligibles that participate when receiving the encouragement and that does not participate when not receiving it. In our example, the proportion of compliers among eligibles is 0.272 in the population and 0.31 in the sample. We are thus underestimating the true proportino of compliers, which is going to make us overestimate the LATE.</p>
</div>
<div id="reduced-form-regression" class="section level4">
<h4><span class="header-section-number">3.4.2.2</span> Reduced form regression</h4>
<p>The reduced form regression aims at estimating the ITE, that is the impact of receiving the encouragement on outcomes. From Theorem <a href="RCT.html#thm:ITEEncourag">3.12</a>, we know that the WW estimator among eligibles identifies the ITE in the population. As a consequence of now classical results, the OLS estimator without control variables is equivalent to the WW estimator and the OLS estimator conditioning on covariates might increase precision.</p>

<div class="example">
<span id="exm:unnamed-chunk-120" class="example"><strong>Example 3.24  </strong></span>In our numerical example, we can compare all these estimators.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">WW.y.R &lt;-<span class="st"> </span><span class="kw">mean</span>(y[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>])<span class="op">-</span><span class="kw">mean</span>(y[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">0</span>])
reg.y.R.ols &lt;-<span class="st"> </span><span class="kw">lm</span>(y[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>R[E<span class="op">==</span><span class="dv">1</span>])
reg.y.R.ols.yB &lt;-<span class="st"> </span><span class="kw">lm</span>(y[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>R[E<span class="op">==</span><span class="dv">1</span>]<span class="op">+</span>yB[E<span class="op">==</span><span class="dv">1</span>])</code></pre></div>
<p><span class="math inline">\(\hat{\Delta}^{y,R}_{WW} =\)</span> 0.179, while <span class="math inline">\(\hat{\Delta}^{y,R}_{OLS}=\)</span> 0.179 which is exactly equal, as expected, to the WW estimator. When controlling for <span class="math inline">\(y^B_i\)</span>, we have: <span class="math inline">\(\hat{\Delta}^{y,R}_{OLS(y^B)}=\)</span> 0.108. In our example, the ITE is 0.047 in the population and 0.044 in the sample. Without conditioning on <span class="math inline">\(Y_i^B\)</span>, we are thus overestimating the true ITE by a lot. The consequence is that we are going to overestimate the LATE as well.</p>
</div>
<div id="structural-regression" class="section level4">
<h4><span class="header-section-number">3.4.2.3</span> Structural regression</h4>
<p>There are four ways to compute the LATE:</p>
<ol style="list-style-type: decimal">
<li>We can directly compute the sample equivalent to the Wald estimator defined in Theorem <a href="RCT.html#thm:IdentLATE">3.9</a>.</li>
<li>We can divide our estimate of the ITE by the proportion of compliers, as Corollary <a href="RCT.html#cor:EncouragWald">3.2</a> suggests.</li>
<li>We can run a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> using <span class="math inline">\(R\)</span> as an instrumental variable.</li>
<li>We can run a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> using <span class="math inline">\(R\)</span> as an instrumental variable and controlling for some variables <span class="math inline">\(X\)</span>.</li>
</ol>
<p>It turns out that, in the absence of control variables, the first three estimators are fully equivalent. Corollary <a href="RCT.html#cor:EncouragWald">3.2</a> has already shown that the first two approaches are equivalent in the population. Theorem <a href="RCT.html#thm:WaldIV">3.15</a> below shows that the Wald estimator is equivalent to an IV estimator.</p>
<p>For simplicity, in all that follows, I am working only in the subgroup of eligible individuals. That means that I’m setting <span class="math inline">\(E_i=1\)</span> for everyone, so that <span class="math inline">\(N\)</span> is the number of eligible individuals.</p>
<div id="using-the-wald-estimator" class="section level5">
<h5><span class="header-section-number">3.4.2.3.1</span> Using the Wald estimator</h5>
<p>The empirical counterpart to the Wald estimator is the difference in mean outcomes between treatment and controls divided by the difference in participation rates between the two groups:</p>
<span class="math display">\[\begin{align*}
\hat{\Delta}^Y_{Wald} &amp; = \frac{\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N Y_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N Y_i(1-R_i)}{\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N D_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N D_i(1-R_i)}
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-121" class="example"><strong>Example 3.25  </strong></span>Let’s check how this works in our numerical example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mean.y.R.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(y[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>])
mean.y.R.<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(y[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">0</span>])
mean.D.R.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>])
mean.D.R.<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">0</span>])
delta.y.Wald &lt;-<span class="st"> </span>(mean.y.R.<span class="dv">1</span><span class="op">-</span>mean.y.R.<span class="dv">0</span>)<span class="op">/</span>(mean.D.R.<span class="dv">1</span><span class="op">-</span>mean.D.R.<span class="dv">0</span>)</code></pre></div>
<p>The numerator of the Wald estimator is equal to 7.059 <span class="math inline">\(-\)</span> 6.88 <span class="math inline">\(=\)</span> 0.179. The denominator of the Wald estimator is equal to 0.704 <span class="math inline">\(-\)</span> 0.491 <span class="math inline">\(=\)</span> 0.213. Overall, the Wald estimator of the LATE is equal to 0.179 <span class="math inline">\(\div\)</span> 0.213 <span class="math inline">\(=\)</span> 0.841.</p>
<p>Remember that the true LATE is equal to 0.173. We are thus severely overestimating the LATE. We’ll understand why in the next section.</p>
</div>
<div id="using-the-ite" class="section level5">
<h5><span class="header-section-number">3.4.2.3.2</span> Using the ITE</h5>
<p>We know from Corollary <a href="RCT.html#cor:EncouragWald">3.2</a> that dividing the ITE by the proportion of compliers gives the Wald estimator. From Theorem <a href="RCT.html#thm:ITEEncourag">3.12</a>, we know that the ITE can be estimated using the sample equivalent to the With/Without estimator as follows:</p>
<span class="math display">\[\begin{align*}
\hat{\Delta}^{Y,R}_{WW} &amp; = \frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N Y_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N Y_i(1-R_i).
\end{align*}\]</span>
<p>From Theorem <a href="RCT.html#thm:prEncourag">3.13</a>, we also know that the proportion of compliers can be estimated using the sample equivalent to the With/Without estimator as follows:</p>
<span class="math display">\[\begin{align*}
\hat{\Delta}^{D,R}_{WW} &amp; = \frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N D_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N D_i(1-R_i).
\end{align*}\]</span>

<div class="example">
<span id="exm:unnamed-chunk-122" class="example"><strong>Example 3.26  </strong></span>Let’s check that this unfolds in our numerical example.
</div>
<p> We already know that the estimated ITE is equal to 0.179, which is equal to the numerator of the Wald estimator. We also now that the proportion of compliers in our sample is equal to 0.213. As a consequence, again, the Wald estimator of the LATE is equal to 0.179 <span class="math inline">\(\div\)</span> 0.213 <span class="math inline">\(=\)</span> 0.841. Without surprise, we obtain exactly the same results as when using the Wald estimator directly. The two approaches are numerically equivalent.</p>
<p>Again, our estimator of the LATE, the Wald estimator, severelt overestimates the LATE. The Wald estimator is equal to 0.841 while the true LATE is equal to 0.173. What is the reason for this mistake? There are actually two:</p>
<ol style="list-style-type: decimal">
<li>We are overestimating the ITE (truth: 0.047; estimate: 0.179).</li>
<li>We are underestimating the proportion of compliers (truth: 0.272; estimate: 0.213).</li>
</ol>
<p>The combination of these two mistakes generates the large discrepancy that we see between our estimate of the LATE and its true value. This error comes for covariates that are not distributed identically in the treatment and control groups. Maybe controlling for some of them would improve our fit. In order to to that, we need the IV estimator.</p>
</div>
<div id="using-the-iv-estimator" class="section level5">
<h5><span class="header-section-number">3.4.2.3.3</span> Using the IV estimator</h5>
<p>A very useful result is that the Wald estimator can be computed as an IV estimator. The following theorem proves that point:</p>

<div class="theorem">
<p><span id="thm:WaldIV" class="theorem"><strong>Theorem 3.15  (Wald is IV)  </strong></span>Under the assumption that there is at least one individual with <span class="math inline">\(R_i=1\)</span> and <span class="math inline">\(D_i=1\)</span>, the coefficient <span class="math inline">\(\beta\)</span> in the following regression estimated using <span class="math inline">\(R_i\)</span> as an IV:</p>
<span class="math display">\[\begin{align*}
        Y_i &amp;  = \alpha + \beta D_i + U_i
    \end{align*}\]</span>
<p>is the Wald estimator:</p>
<span class="math display">\[\begin{align*}
\hat{\beta}_{IV} &amp; = \frac{\frac{1}{N}\sum_{i=1}^N\left(Y_i-\frac{1}{N}\sum_{i=1}^NY_i\right)\left(R_i-\frac{1}{N}\sum_{i=1}^NR_i\right)}{\frac{1}{N}\sum_{i=1}^N\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)\left(R_i-\frac{1}{N}\sum_{i=1}^NR_i\right)} \\
                                &amp; = \frac{\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N Y_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N Y_i(1-R_i)}{\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N D_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N D_i(1-R_i)}\\
                  &amp; = \hat{\Delta}^Y_{Wald} 
\end{align*}\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> See in section <a href="proofs.html#proofWaldIV">A.2.2</a> in the appendix.
</div>

<p>Theorem <a href="RCT.html#thm:WaldIV">3.15</a> is super powerful since it enables us to directly use the IV estimator to compute the Wald estimator. In order to do so, we’re going to use the estimator <code>ivreg</code> in the <code>AER</code> package.</p>

<div class="example">
<span id="exm:unnamed-chunk-124" class="example"><strong>Example 3.27  </strong></span>Let’s see how the IV estimator performs in our numerical example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.2sls.encourage &lt;-<span class="st"> </span><span class="kw">ivreg</span>(y[E<span class="op">==</span><span class="dv">1</span>]<span class="op">~</span>Ds[E<span class="op">==</span><span class="dv">1</span>]<span class="op">|</span>R[E<span class="op">==</span><span class="dv">1</span>])
beta.IV &lt;-<span class="st"> </span>reg.y.R.2sls.encourage<span class="op">$</span>coef[<span class="dv">2</span>]</code></pre></div>
<p><span class="math inline">\(\hat{\beta}_{IV}=\)</span> 0.841, which is equal to the Wald estimator, as Theorem <a href="RCT.html#thm:WaldIV">3.15</a> predicted.</p>
</div>
<div id="using-the-iv-estimator-conditioning-on-covariates" class="section level5">
<h5><span class="header-section-number">3.4.2.3.4</span> Using the IV estimator conditioning on covariates</h5>
<p>One nice thing about the IV estimator is that we can use it to control for additional covariates <span class="math inline">\(X\)</span>. Estimating the following equation with <span class="math inline">\(R_i\)</span> and <span class="math inline">\(X_i\)</span> as instruments:</p>
<span class="math display">\[\begin{align*}
    Y_i &amp;  = \alpha +  \beta D_i + \gamma&#39; X_i + U_i
  \end{align*}\]</span>
<p>recovers <span class="math inline">\(\beta_{IV}(X)\)</span>, which is an estimate of the LATE under linearity assumptions on the potential outcomes.</p>
<p><strong><span style="font-variant: small-caps;">Expand on that</span></strong> <strong><span style="font-variant: small-caps;">Center covariates at mean?</span></strong></p>

<div class="example">
<span id="exm:unnamed-chunk-125" class="example"><strong>Example 3.28  </strong></span>Let’s see how this work in our numerical example, when we condition on <span class="math inline">\(y^B_i\)</span>.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.yB.2sls.encourage &lt;-<span class="st"> </span><span class="kw">ivreg</span>(y[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>Ds[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>] <span class="op">|</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>])
beta.IV.yB &lt;-<span class="st"> </span>reg.y.R.yB.2sls.encourage<span class="op">$</span>coef[<span class="dv">2</span>]</code></pre></div>
<p><span class="math inline">\(\hat{\beta}_{IV}(y^B)=\)</span> 0.464. Remember that the value of <span class="math inline">\(\Delta^y_{LATE}\)</span> in the population is thus 0.173. All of our estimators have overshoot. The worse are the ones not conditioning on <span class="math inline">\(y^B\)</span>. It seems that conditioning on <span class="math inline">\(y^B\)</span> improves the estimator slightly. So part of the estimation error in the Wald estimator probably comes from an imbalance in <span class="math inline">\(y_i^B\)</span> between the treatment and control groups. Let’s check that this is the case.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.yB.R.ols.encourage &lt;-<span class="st"> </span><span class="kw">lm</span>(yB[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>])
delta.yB.WW.R &lt;-<span class="st"> </span>reg.yB.R.ols.encourage<span class="op">$</span>coef[<span class="dv">2</span>]</code></pre></div>
<p>The difference in <span class="math inline">\(y_i^B\)</span> among treated and controls in our example is 0.075. This is enough to account for the bias on the ITE.</p>
<p><strong><span style="font-variant: small-caps;">Expand on that</span></strong></p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> One key question that remains is that whether the structural parameter <span class="math inline">\(\beta(X)\)</span> is still equal to the ratio of the reduced form parameter and the first stage parameter obtained by running OLS conditionnal on <span class="math inline">\(X\)</span>.
</div>


<div class="example">
<span id="exm:unnamed-chunk-127" class="example"><strong>Example 3.29  </strong></span>Let’s examine what happens in our example.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg.y.R.yB.ols.encourage &lt;-<span class="st"> </span><span class="kw">lm</span>(y[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>])
ITE.yB &lt;-<span class="st"> </span>reg.y.R.yB.ols.encourage<span class="op">$</span>coef[<span class="dv">2</span>]

reg.D.R.yB.ols.encourage &lt;-<span class="st"> </span><span class="kw">lm</span>(D[E<span class="op">==</span><span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>R[E<span class="op">==</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>yB[E<span class="op">==</span><span class="dv">1</span>])
prCo.yB &lt;-<span class="st"> </span>reg.D.R.yB.ols.encourage<span class="op">$</span>coef[<span class="dv">2</span>]

Wald.yB &lt;-<span class="st"> </span>ITE.yB<span class="op">/</span>prCo.yB</code></pre></div>
<p>We find that the ITE conditional on <span class="math inline">\(y^B_i\)</span> is equal to 0.108 while the proportion of compliers conditioning on <span class="math inline">\(y_i^B\)</span> is equal to 0.233. Overall the ratio of these two, which we could call the Wald ratio after conditioning on <span class="math inline">\(y_i^B\)</span> is equal to 0.464. This is actually equal to the IV estimator including <span class="math inline">\(y_i^B\)</span> as a covariate: <span class="math inline">\(\hat{\beta}_{IV}(y^B)=\)</span> 0.464. So running the reduced form and first stage regressions separately and dividing the coefficients on <span class="math inline">\(R_i\)</span> recovers the LATE even when conditioning on covariates? That’s pretty neat and opens up the route for a variety of new estimation techniques called <code>split sample</code> estimators, developed by <a href="http://economics.mit.edu/files/398">Angrist and Krueger</a>. We’ll take more about them later.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> We might want to control nonparametrically on the covariates instead of imposing a linear regression. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0304407606001023">Frolich’s Wald matching estimator</a> enables to do just that. Its implementation will become clearer after Chapter <a href="sec-OM.html#sec:OM">5</a>.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> The last thing we want to check is whether conditioning on covariates improve precision. It seems to be the case in our example with one dataset. Let’s see what happens over sampling repetitions.
</div>


<div class="example">
<span id="exm:unnamed-chunk-130" class="example"><strong>Example 3.30  </strong></span>Let’s run some Monte Carlo simulations for the sampling noise of IV with and without conditining on <span class="math inline">\(y_i^B\)</span>.
</div>

<div class="figure" style="text-align: center"><span id="fig:MonteCarloHistEncourage"></span>
<img src="STCI_files/figure-html/MonteCarloHistEncourage-1.png" alt="Distribution of the $Wald$ and $Wald(X)$ estimator in an encouragement design over replications of samples of different sizes" width="50%" /><img src="STCI_files/figure-html/MonteCarloHistEncourage-2.png" alt="Distribution of the $Wald$ and $Wald(X)$ estimator in an encouragement design over replications of samples of different sizes" width="50%" />
<p class="caption">
Figure 3.7: Distribution of the <span class="math inline">\(Wald\)</span> and <span class="math inline">\(Wald(X)\)</span> estimator in an encouragement design over replications of samples of different sizes
</p>
</div>
</div>
</div>
</div>
<div id="estimating-sampling-noise-3" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Estimating sampling noise</h3>
<p>As always, we can estimate sampling noise either using the CLT or resampling methods. Using the CLT, we can derive the following formula for the distribution of the Bloom estimator: Theorem <a href="RCT.html#thm:asymWald">3.16</a> shows the asymptotic distribution of <span class="math inline">\(\hat{\Delta}^Y_{Wald}\)</span>:</p>

<div class="theorem">
<p><span id="thm:asymWald" class="theorem"><strong>Theorem 3.16  (Asymptotic Distribution of <span class="math inline">\(\hat{\Delta}^Y_{Wald}\)</span>)  </strong></span>Under Assumptions <a href="RCT.html#def:RandEncouragValid">3.9</a>, <a href="RCT.html#def:IndepEncourag">3.10</a>, <a href="RCT.html#def:ExclRestr">3.11</a>, <a href="RCT.html#def:Fstage">3.12</a>, we have:</p>
<span class="math display">\[\begin{align*}
  \sqrt{N}(\hat{\Delta}^Y_{Wald}-\Delta^Y_{LATE}) &amp;  \stackrel{d}{\rightarrow}
  \mathcal{N}\left(0,\frac{1}{(p^D_1-p^D_0)^2}\left[\left(\frac{p^D}{p^R}\right)^2\frac{\var{Y_i|R_i=0}}{1-p^R}+\left(\frac{1-p^D}{1-p^R}\right)^2\frac{\var{Y_i|R_i=1}}{p^R}\right]\right)\\
\end{align*}\]</span>
<p>Adding Assumption <a href="RCT.html#def:Mono">3.13</a>, the variance of the Wald estimator can be further decomposed as follows:</p>
<span class="math display">\[\begin{align*}
\sigma^2_{\hat{\Delta}^Y_{Wald}}  &amp; = \left(\frac{p^D}{p^R}\right)^2\frac{\var{Y_i^0|T_i=C}}{p^C(1-p^R)}+\left(\frac{1-p^D}{1-p^R}\right)^2\frac{\var{Y^1_i|T_i=C}}{p^Cp^R}\\
                                  &amp; \phantom{=} +\frac{(p^{AT}(1-p^R)-p^{NT}p^R)^2+p^R(1-p^R)}{(p^Cp^R(1-p^R))^2}\left[p^{AT}\var{Y_i^1|T_i=AT}+p^{NT}\var{Y^0_i|T_i=NT}\right]
\end{align*}\]</span>
</div>

<p>with <span class="math inline">\(p^D=\Pr(D_i=1)\)</span>, <span class="math inline">\(p^R=\Pr(R_i=1)\)</span>, <span class="math inline">\(p^{D}_1=\Pr(D_i=1|R_i=1)\)</span>, <span class="math inline">\(p^{D}_0=\Pr(D_i=1|R_i=0)\)</span>, <span class="math inline">\(p^t=\Pr(T_i=t)\)</span>, with <span class="math inline">\(t\in\left\{AT,NT,C,D\right\}\)</span>.</p>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> See Section <a href="proofs.html#ProofAsymWald">A.2.3</a>.
</div>


<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> Theorem <a href="RCT.html#thm:asymWald">3.16</a> shows that the effective sample size of an encouragement design is equal to the number of compliers. Indeed, the denominator of the variance of the Wald Estimatior depends on <span class="math inline">\(p^D_1-p^D_0\)</span>, which is an estimate of the proportion of compliers, under Assumption <a href="RCT.html#def:Mono">3.13</a>.</p>
Theorem <a href="RCT.html#thm:asymWald">3.16</a> also shows that there is a price to pay for the fact that we cannot enforce the effect on always takers and never takers is actually zero. Indeed, as the second formula shows, it is not only the variance of the oucomes of the compliers that appears in the formula, but also the variances of the outcomes of the always takers and never takers, therefore increasing sampling noise.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> In order to compute the formula in Theorem <a href="RCT.html#thm:asymWald">3.16</a>, we can use a plug-in estimator or the IV standard error estimate robist to heteroskedasticity. Here is a simple function in order to compute the plug-in estimator:
</div>


<div class="example">
<span id="exm:unnamed-chunk-134" class="example"><strong>Example 3.31  </strong></span>Let us derive the CLT-based estimates of sampling noise using both the plug-in estimator and the IV standard errors without conditioning on covariates first. For the sake of the example, I’m working with a sample of size <span class="math inline">\(N=1000\)</span>.
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.Encourag.simuls &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.encourage[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;Wald&#39;</span>]<span class="op">-</span>delta.y.late),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.Encourag.IV.plugin &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">var.Encourage.plugin</span>(<span class="dt">pD1=</span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">1</span>]),<span class="dt">pD0=</span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>R<span class="op">==</span><span class="dv">0</span>]),<span class="dt">pD=</span><span class="kw">mean</span>(D[E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">pR=</span><span class="kw">mean</span>(R[E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">V0=</span><span class="kw">var</span>(y[R<span class="op">==</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">V1=</span><span class="kw">var</span>(y[R<span class="op">==</span><span class="dv">1</span> <span class="op">&amp;</span><span class="st"> </span>E<span class="op">==</span><span class="dv">1</span>]),<span class="dt">N=</span><span class="kw">length</span>(y[E<span class="op">==</span><span class="dv">1</span>])))
sn.Encourag.IV.homo &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.2sls.encourage)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.Encourag.IV.hetero &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.2sls.encourage,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>True 99% sampling noise (from the simulations) is 1.166. 99% sampling noise estimated using the plug-in estimator is 2.124. 99% sampling noise estimated using default IV standard errors is 23.134. 99% sampling noise estimated using heteroskedasticity robust IV standard errors is 2.119.</p>
<p>Conditioning on <span class="math inline">\(y_i^B\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sn.Encourag.simuls.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">quantile</span>(<span class="kw">abs</span>(simuls.encourage.yB[[<span class="st">&#39;1000&#39;</span>]][,<span class="st">&#39;Wald&#39;</span>]<span class="op">-</span>delta.y.late),<span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.99</span>))
sn.Encourag.IV.homo.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcov</span>(reg.y.R.yB.2sls.encourage)[<span class="dv">2</span>,<span class="dv">2</span>])
sn.Encourag.IV.hetero.yB &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">qnorm</span>((.<span class="dv">99</span><span class="op">+</span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">vcovHC</span>(reg.y.R.yB.2sls.encourage,<span class="dt">type=</span><span class="st">&#39;HC2&#39;</span>)[<span class="dv">2</span>,<span class="dv">2</span>])</code></pre></div>
<p>True 99% sampling noise (from the simulations) is 0.705. 99% sampling noise estimated using default IV standard errors is 0.988. 99% sampling noise estimated using heteroskedasticity robust IV standard errors is 0.975.</p>
</div>
</div>
<div id="sec:threats" class="section level2">
<h2><span class="header-section-number">3.5</span> Threats to the validity of RCTs</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="FPSI.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="NE.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/chabefer/STCI/blob/master/03_RCT.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["STCI.pdf"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 1
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
