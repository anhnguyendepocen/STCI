# Meta-analysis and Publication Bias {#sec:meta}

When several research teams work on a similar topic, they obtain and publish several estimates for the same program of for similar programs.
For example, teams of doctors regularly test the same treatment on different samples or populations in order to refine the estimated effect.
Similarly, economists report on the effects of similar types of programs (Conditional and Unconditional Cash Transfers, Job Training Programs, microcredit, etc) implemented in different countries.

Meta-analysis aims at summarizing and synthetizing the available evidence with two main goals in mind:

  1. Increasing precision by providing an average estimated effect combining several estimates
  2. Explaining variations in treatment effectiveness by relating changes in effect size to changes in sample characteristics.
  
One key issue that meta-analysis has to face -- actually, we all have to face it, meta-analysis simply makes it more apparent -- is that of publication bias.
Publication bias is due to the fact that referees and editors have a marked preference for publishing statistically significant results. 
The problem with this approach is that the distribution of published results is going to be censored on the left: we will have more statistically significant results in the published record, and as a consequence, the average published result will be an upward biased estimate of the true treatment effect in the population.
This is potentially a very severe problem if the amount of censoring due to publication bias is large.
Eventually, this hinges on the true distribution of treatment effects: if it is centered on zero or close to zero, we run the risk of having very large publication bias.

In this chapter, I present first the tools for meta-analysis, and I then move on to testing and correcting for publication bias.

## Meta-analysis

There are several approaches and refinements to meta-analysis. 
In this section, I am going to present only the most important ones.
I'll defer the reader to other more specialized publications if needed.

I first present the basics of meta-analysis: the constitution and structure of the sample.
Second, I present the simplest method to aggregate effects: the weighted average.
Third, I present tests for deciding whether the effects are from a homogeneous or a heterogeneous population.
Fourth, I explain how to conduct a meta-analysis when effects are heterogeneous.
Fifth, I cover the meta-regression that we use to account for heterogeneity in treatment effects.
Finally, I explain why usual intuitive methods such as vote-counting are biased.

### Basic setting

The basic setting for a meta-analysis is that you have access to a list of estimates for the effect of a given program and for their precision.
These estimates come from the literature, searching published and unpublished sources alike.
This data is usually collected after an extensive search of bibliographic databases.
Then, one has to select among all the studies selected by the search the ones that are actualy relevant.
This is the most excruciating part of a meta-analysis, since a lot of the studies selected by hte search algorithm are actually irrelevant.
Finally, one has to extract from each relevant paper an estimate of the effect of the treatment and of its precision.
In general, one tries to choose standardized estimates such as the effect size (see Section \@ref(sec:effectsize) for a definition) and its standard error. 
After all this process, we should end up with a dataset like: $\left\{(\hat{\theta}_k,\hat{\sigma}_k)\right\}_{k=1}^N$, with $\hat{\theta}_k$ the estimated effect size, $\hat{\sigma}_k$ its estimated standard error, and $N$ the number of included studies.

```{example}
Let's see how such a dataset would look like? 
Let's build one from our simulations.
```

```{r confintervalESCLT,dependson=c('monte.carlo'),eval=TRUE,echo=TRUE,results='hide',warning=FALSE,error=FALSE,message=FALSE,fig.cap=paste('Example data set: effect sizes and confidence intervals with $\\delta=$',delta.2,sep=' '),fig.align='center',out.width=cst,fig.pos='htbp',fig.height=8,fig.width=12}
N.sample <- c(100,1000,10000,100000)
N.plot.ES.CLT <- c(10,7,2,1)
data.meta <- data.frame(ES=numeric(),
                          se=numeric())

se.ww.CLT.ES <- function(N,v1,v0,p){
  return(sqrt((v1/p+v0/(1-p))/N)/v0)
}

for (k in 1:length(N.sample)){
  set.seed(1234)
  simuls.ww[[k]]$se.ES <- se.ww.CLT.ES(N.sample[[k]],simuls.ww[[k]][,'V1'],simuls.ww[[k]][,'V0'],simuls.ww[[k]][,'p'])
  test.ES <- simuls.ww[[k]][sample(N.plot.ES.CLT[[k]]),c('ES','se.ES')]
  test.ES$N <- rep(N.sample[[k]],N.plot.ES.CLT[[k]])
  data.meta <- rbind(data.meta,test.ES)
  }

data.meta$id <- 1:nrow(data.meta)
#data.meta$N <- factor(data.meta$N,levels(N.sample))

  ggplot(data.meta, aes(x=as.factor(id), y=ES)) +
      geom_bar(position=position_dodge(), stat="identity", colour='black') +
      geom_errorbar(aes(ymin=ES-qnorm((delta.2+1)/2)*se.ES, ymax=ES+qnorm((delta.2+1)/2)*se.ES), width=.2,position=position_dodge(.9),color='blue') +
      geom_hline(aes(yintercept=ES(param)), colour="#990000", linetype="dashed")+
      xlab("Studies")+
      ylab("Effect size")+
      theme_bw()

```

Figure \@ref(fig:confintervalESCLT) shows the resulting sample. 
I've selected `r N.plot.ES.CLT[[1]]` studies with $N=$ `r N.sample[[1]]`, `r N.plot.ES.CLT[[2]]` studies with $N=$ `r N.sample[[2]]`, `r N.plot.ES.CLT[[3]]` studies with $N=$ `r N.sample[[3]]`, and `r N.plot.ES.CLT[[4]]` study with $N=$ `r N.sample[[4]]`.
The studies are represented in that order, mimicking the increasing sample size of studies that accumulate evidence on a treatment, probably with studies with a small sample size at first, and only large studies at the end for the most promising treatments.

### Meta-analysis as a weighted average

The key idea of meta-analysis is to combine the effect size estimates stemming from different studies, weighing them by their relative precision.

```{definition,metaweights,name='Weighted Meta-Analytic Estimator'}
The weighted meta-analytic estimator is
$$
\bar{\theta} = \sum_{k=1}^Nw_k\hat{\theta}_k \text{ with } w_k=\frac{\frac{1}{\hat{\sigma}^2_k}}{\sum_{k=1}^N\frac{1}{\hat{\sigma}^2_k}}.
$$
```

Under some assumptions, the estimator $\bar{\theta}$ converges to the true effect of the treatment.
Let's delineate these assumptions.

```{definition,metahomo,name='Homogeneous Treatment Effect'}
Each $\hat{\theta}_k$ converges to the same treatment effect $\theta$.
```

Assumption \@ref(def:metahomo) imposes that all the studies have been drawn from the same population, where the treatment effect is a constant.

```{definition,metaind,name='Independence of Estimates'}
The $\hat{\theta}_k$ are independent from each other.
```

Assumption \@ref(def:metaind) imposes that all the studies estimates are independent from each other.
That means that they do not share sampling units and that they are not affected by common shocks.

Under these assumptions, we can show two important results.

```{theorem,metafixedcons,name='Consistency of the Weighted Meta-Analytic Estimator'}
Under Assumptions \@ref(def:metahomo) and \@ref(def:metaind), when the sample size of each study goes to infinity, $\bar{\theta}\approx\theta$.
```

```{proof}
The Law of Large Number applied to each sample gives the fact that the estimator is a weighted sum of $\theta$ with weights summing to one. 
Hence the result.
```

Theorem \@ref(thm:metafixedcons) says that the error we are making around the true effect of the treatment goes to zero as the sample size in each study decrease.
This is great: aggregating the studies is thus going to get us to the truth.

```{remark}
One interesting question is whether Theorem \@ref(thm:metafixedcons) also holds when the size of the individual studies remains fixed and the number of studies goes to infinity, which seems a more natural way to do asymptotics in a meta-analysis. 
I'm pretty sure that is the case.
Indeed, the studies constitute an enormous sample in which we take the average outcomes of the treated on the one hand and of the untreated on the other.
These averages differ from the usual ones in the Law of Large Numbers only by the fact that the weights are not equal to one.
But they (i) are independent from the outcomes and (ii) sum to one.
As a consequence, I'm pretty sure the Law of Large Numbers also apply in this dimension.
```

**<span style="font-variant:small-caps;">Check if this is a consequence of Kolmogorov's Law of Large Numbers.</span>**

```{theorem,metafixeddis,name='Asymptotic Distribution of the Weighted Meta-Analytic Estimator'}
Under Assumptions \@ref(def:metahomo) and \@ref(def:metaind), when the sample size of each study goes to infinity, $\bar{\theta}\stackrel{d}{\rightarrow}\mathcal{N}(\theta,\sigma^2)$, with
$$
\sigma^2 = \frac{1}{\sum_{k=1}^N\frac{1}{\sigma^2_k}}.
$$
```

```{proof}
**<span style="font-variant:small-caps;">To do using the Lindenberg-Levy version of the Central Limit Theorem.</span>**
```

Theorem \@ref(thm:metafixeddis) shows that the distribution of the weighted meta-analytic estimator converges to a normal, which is very convenient in order to compute sampling noise.
In order to obtain an estimator $\hat{\sigma}^2$ of the variance of the meta-analytic estimator, we can simply replace the individual variance terms by their estimates: $\hat{\sigma}_k^2$.

```{remark}
I've taken Theorem \@ref(thm:metafixeddis) from Hedges and Olkin, but I think it is much more interesting and correct when the asymptotics goes in the number of studies.
```

```{remark}
According to Hedges and Olkin, the weighted meta-analytic estimator is the most efficient estimator available.
```

```{r WMAE,eval=TRUE,echo=TRUE,results='hide',warning=FALSE,error=FALSE,message=FALSE}
wmae <- function(theta,sigma2){
  return(c(weighted.mean(theta,(1/sigma2)/(sum(1/sigma2))),1/sum(1/sigma2)))
}
```

```{example}
Let's use our meta-analytic estimator to estimate the effect size of our treatment.
```
The estimated treatment effect size with our sample is `r round(wmae(data.meta$ES,data.meta$se.ES^2)[[1]],2)` $\pm$ `r round(qnorm((delta.2+1)/2)*sqrt(wmae(data.meta$ES,data.meta$se.ES^2)[[2]]),2)`.
A very simple way to implement such an estimator in R is to use the `rma` command of the `metafor` package.

```{r metafor.example,eval=TRUE,echo=TRUE,results='markup',warning=FALSE,error=FALSE,message=FALSE}
data.meta$var.ES <- data.meta$se.ES^2
meta.example.FE <- rma(yi = data.meta$ES,vi=data.meta$var.ES,method="FE")
summary(meta.example.FE)
```

As seen above, the `metafor` package yields a meta-analytic estimate of `r round(coef(meta.example.FE),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*summary(meta.example.FE)$se,2)`, as we have found using the weighted meta-analytic estimator.

It is customary to present the results of a meta-analysis using a forest plot.
A forest plows all the individual estimates along with the aggregated estimate.
Figure \@ref(fig:FEforestplot) presents the forest plot for our example using the very convenient `forest` function in the `metafor` package:

```{r FEforestplot,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE,results=FALSE,fig.cap='Example data set: forest plot', fig.align='center', out.width=cst, fig.pos='htbp', fig.height=8, fig.width=12}
forest(meta.example.FE,slab = paste('Study',data.meta$id,sep=' '),xlab='Estimated Meta-analytic Parameter')
```

### Constantly updated meta-analysis

Constantly updated meta-analysis performs the meta-analysis in a progressive manner, as the results keep arriving.
This is a very important tool that enables us to aggregate constantly the information coming from different studies.
Moreover, restrospectively, it helps us to assess when we would have reached enough precision so that we could have foregone an additional study.
The way constantly updated meta-analysis works is simply by performing a new meta-analysis each time a new results pops up.

```{example}
Figure \@ref(fig:cumWMAE) shows how constantly updated meta-analysis works in our example.
```

```{r cumWMAE,eval=TRUE,echo=TRUE,results='hide',warning=FALSE,error=FALSE,message=FALSE,fig.cap='Constantly updated meta-analysis',fig.subcap=c('Initial data set','Constantly updated dataset'), fig.align='center', out.width="50%", fig.pos='htbp', fig.height=8, fig.width=12}
cum.wmae.1 <- function(k,theta,sigma2){
  return(c(weighted.mean(theta[1:k],(1/sigma2[1:k])/(sum(1/sigma2[1:k]))),1/sum(1/sigma2[1:k])))
}

cum.wmae <- function(theta,sigma2){
  return(sapply(1:length(theta),cum.wmae.1,theta=theta,sigma2=sigma2))
}

cum.test <- as.data.frame(t(cum.wmae(data.meta$ES,data.meta$se.ES^2)))
colnames(cum.test) <- c('cum.ES','cum.var')
cum.test$id <- 1:nrow(cum.test)
cum.test$cum.se.ES <- sqrt(cum.test$cum.var)

  ggplot(data.meta, aes(x=forcats::fct_rev(as.factor(id)), y=ES)) +
      geom_bar(position=position_dodge(), stat="identity", colour='black') +
      geom_errorbar(aes(ymin=ES-qnorm((delta.2+1)/2)*se.ES, ymax=ES+qnorm((delta.2+1)/2)*se.ES), width=.2,position=position_dodge(.9),color='blue') +
      geom_hline(aes(yintercept=ES(param)), colour="#990000", linetype="dashed")+
      xlab("Studies")+
      ylab("Initial effect size")+
      theme_bw()+
      coord_flip()

  ggplot(cum.test, aes(x=forcats::fct_rev(as.factor(id)), y=cum.ES)) +
      geom_bar(position=position_dodge(), stat="identity", colour='black') +
      geom_errorbar(aes(ymin=cum.ES-qnorm((delta.2+1)/2)*cum.se.ES, ymax=cum.ES+qnorm((delta.2+1)/2)*cum.se.ES), width=.2,position=position_dodge(.9),color='blue') +
      geom_hline(aes(yintercept=ES(param)), colour="#990000", linetype="dashed")+
      xlab("Studies")+
      ylab("Cumulative effect size")+
      theme_bw()+
      coord_flip()

```

Figure \@ref(fig:cumWMAE) shows that combining several imprecise estimates might help you reach the same precision as running a larger experiment.  
For instance, cumulating the first 10 studies with a small sample size ($N=$ `r N.sample[[1]]`), the meta-analytic effect is estimated at `r round(select(filter(cum.test,id==10),cum.ES),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*select(filter(cum.test,id==10),cum.se.ES),2)`. 
This is very close to the individual estimate obtained from the first estimate with a larger sample size (sample 11 on Figure \@ref(fig:cumWMAE), with $N=$ `r N.sample[[2]]`): `r round(select(filter(data.meta,id==11),ES),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*select(filter(data.meta,id==11),se.ES),2)`. 
Both estimates actually have the exact same precision (because they actually have the same sample size).
The same is true when combining the first 17 studies.
The meta-analytic effect is estimated at `r round(select(filter(cum.test,id==17),cum.ES),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*select(filter(cum.test,id==17),cum.se.ES),2)`, while the effect estimated using one unique RCT with a larger sample size (sample 18 on Figure \@ref(fig:cumWMAE), with $N=$ `r N.sample[[3]]`) is  `r round(select(filter(data.meta,id==18),ES),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*select(filter(data.meta,id==18),se.ES),2)`. 
Finally, the same result occurs when combining the first 19 studies.
The meta-analytic effect is estimated at `r round(select(filter(cum.test,id==19),cum.ES),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*select(filter(cum.test,id==19),cum.se.ES),2)`, while the effect estimated using one unique RCT with a larger sample size (sample 20 on Figure \@ref(fig:cumWMAE), with $N=$ `r N.sample[[4]]`) is  `r round(select(filter(data.meta,id==20),ES),2)` $\pm$ `r round(qnorm((delta.2+1)/2)*select(filter(data.meta,id==20),se.ES),2)`. 

As a conclusion, constantly updated meta-analysis would have each time delivered the same result than the one found with a much larger study, rendering this additional study almost irrelevant.
This is a very important result: beyond the apparent messiness of the first noisy estimates in Figures \@ref(fig:confintervalESCLT) and \@ref(fig:FEforestplot) lies an order that can be retrieved and made apparent using constantly updated meta-analysis.
Sometimes, the answer is right there in front of our eyes, we just lack the ability to see it.
Constantly updated meta-analysis serves as a binocular to magnify what is there.
Think about how costly it woud be to run a very large study, just to find out that the we did not really need it because we had known the result all along.

```{remark}
Something pretty cool is that I can reproduce Figure \@ref(fig:cumWMAE) using the `metafor` package with much less lines of code.
```

```{r cumWMAEmetafor,eval=TRUE,echo=TRUE,results='hide',warning=FALSE,error=FALSE,message=FALSE,fig.cap='Constantly updated meta-analysis with the `metafor` package',fig.subcap=c('Initial data set','Constantly updated dataset'), fig.align='center', out.width="50%", fig.pos='htbp', fig.height=8, fig.width=12}
forest(meta.example.FE,slab = paste('Study',data.meta$id,sep=' '),xlab='Estimated Meta-analytic Parameter')
cumul.meta.example.FE <- cumul(meta.example.FE, order=data.meta$id)
forest(cumul.meta.example.FE,slab = paste('Study',data.meta$id,sep=' '),xlab='Estimated Meta-analytic Cumulated Parameter')
```

You can also call each of the individual results of the cumulative meta-analysis using `cumul.meta.example.FE$estimate`. 
For example, the cumulated effect size after the 10 first studies is equal to `r round(cumul.meta.example.FE$estimate[[10]],2)` $\pm$ `r round(qnorm((delta.2+1)/2)*cumul.meta.example.FE$se[[10]],2)`.

### Testing for the homogeneity of treatment effects

One key assumption that we have just made so far is that of homogeneous treatment effect.
We have worked under the assumption that each study was drawn from the same population, where the treatment effect is a constant.
Why would the treatment effects differ in each study?

  1. We do not study exactly the same treatment, but a family of similar treatments.
  Each individual study covers a particular iteration of the treatment, each with its idiosyncratic parameterization.
  The particular value of the transfer in a Cash Transfer program, or of the conditions to receive it, or the length of payment, whether it is in one time or over some period, might make a difference, for example.
  The same is true for Job Training Programs, Payments for Environmenetal Services, microcredit, graduation programs, nudges, etc.
  Actually, most programs that economists study differ from one implementation to the next.
  In psychology and medecine, most treatments are accompanied by a rigorous protocol that makes them much more homogeneous.
  2. The population on which the treatment is applied varies.
  For example, similar Job Training Programs or microcredit initiatives might have very different outcomes depending on the business cycle.
  Education interventions might have very different effects depending on the background of the students on which they are tested.
  A drug might interact with patients' phenotype and genotype to generate different effects, and the populations from which the experimental samples are drawn do not have to be similar.
  As an extreme example, think of a vaccine tested in a population where the prevalence of a disease is null.
  The treatment effect is zero.
  Now, test the vaccine in a population where the disease is endemic: the treatment effect might be huge.

We can also think that the treatment effect differs in each sample because the composition of the sample changes.
But this is normal variation due to sampling noise and is to be expected.
It is not the same thing as assuming that the population effect differs.

What can we do in order to test whether there is heterogeneity in treatment effects?
One way is to build an index comparing the usual variation in treatment effects stemming from sampling noise to the one stemming from variation between studies.
If we find that the variation between studies dwarves the variation due to sampling noise in each study, then there is some heterogeneity for sure.
One statistics that does that is the $Q$ statistic where the variation in treatment effects between studies is estimated using the difference between the individual effect size and the average one squared:

\begin{align*}
  Q & = \sum_{k=1}^N\frac{(\hat{\theta}_k-\bar{\theta})^2}{\hat{\sigma}^2_k}.
\end{align*}

What is great with the $Q$ statistic is that, under the Null hypothesis that all the treatment effects are equal to the same constant, it is distributed asymptotically as a $\chi^2$ distribution with $N-1$ degrees of freedom, and thus it can directly be used to test for the hypothesis of homogeneous treatment effects.

```{example}
In our example, we have already computed the $Q$ statistic when we have used the `rma` function in the `metafor` package. 
In order to access it, we just need to extract it using `meta.example.FE$QE` for the $Q$ statistic and `meta.example.FE$QEp` for its p-value.  
```
The $Q$ statistic in our example has value `r round(meta.example.FE$QE,2)`, with associated p-value `r round(meta.example.FE$QEp,2)`.
We end up not rejecting homogeneity, which is correct.

```{remark}
The problem with using test statistics for testing for treatment effect homogeneity is that, when precision increases, we might end up rejecting homogeneity despite the fact that it is there.
```

**<span style="font-variant:small-caps;">Test with $N=10^5$.</span>**

```{remark}
The $\chi^2$ distribution with $k$ degrees of freedom is asymptotically distributed as a normal with mean $k$ and variance $2k$.
So, when $k$ is large, a good rule of thumb for assessing the homogeneity of the treatment effect estimates is to compare the $Q$ statistic to the number of studies. 
If it is much larger, homogeneity is probably not guaranteed.
```

### Meta-analysis when treatment effects are heterogeneous



### Meta-regression

### Why vote-counting does not work

## Publication bias

### Sources of publication bias

### Detecting and correcting for publication bias

#### P-curving

#### PET-PEESE

#### Kasy and Andrews approach

#### Last approach

### Vote counting and publication bias
