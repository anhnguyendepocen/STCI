# (PART) Methods of Causal Inference {-}

# Randomized Controlled Trials {#RCT}

The most robust and rigorous method that has been devised by social scientists to estimate the effect of an intervention on an outcome is the Randomized Controlled Trial (RCT).
RCTs are used extensively in the field to evaluate a wide array of programs, from development, labor and education interventions to environmental nudges to website and search engine features.

The key feature of an RCT is the introduction by the researcher of randomness in the allocation of the treatment.
Individuals with $R_i=1$, where $R_i$ denotes the outcome of a random event, such as a coin toss, have a higher probability of receiving the treatment.
Potential outcomes have the same distribution in both $R_i=1$ and $R_i=0$ groups.
If we observe different outcomes between the treatment and control group, it has to be because of the causal effect of the treatment, since both groups only differ by the proportion of treated and controls.

The most attractive feature of RCTs is that researchers enforce the main identification assumption (we do not have to assume that it holds, we can make sure that it does).
This property of RCTs distinguishes them from all the other methods that we are going to learn in this class.

In this lecture, we are going to study how to estimate the effect of an intervention on an outcome using RCTs. 
We are especially going to study the various types of designs and what can be recovered from them using which technique.
For each design, we are going to detail which treatment effect it enables us to identify, how to obtain a sample estimate of this treatment effect and how to estimate the associated sampling noise.
The main substantial difference between these four designs are the types of treatment effect parameters that they enable us to recover.
Sections \@ref(sec:design1) to \@ref(sec:design4) of this lecture introduces the four designs and how to analyze them.

Unfortunately, RCTs are not bullet proof.
They suffer from problems that might make their estimates of causal effects badly biased.
Section \@ref(sec:threats) surveys the various threats and what we can do to try to minimize them.

## Brute Force Design {#sec:design1}

In the Brute Force Design, eligible individuals are randomly assigned to the treatment irrespective of their willingness to accept it and have to comply with the assignment.
This is a rather dumb procedure but it is very easy to analyze and that is why I start with it.
With the Brute Force Design, you can recover the average effect of the treatment on the whole population.
This parameter is generally called the Average Treatment Effect (ATE).

In this section, I am going to detail the assumptions required for the Brute Force Design to identify the ATE, how to form an estimator of the ATE and how to estimate its sampling noise.

### Identification

In the Brute Force Design, we need two assumptions for the ATE to be identified in the population: Independence and Brute Force Validity.

```{definition,independence,name="Independence"}
We assume that the allocation of the program is independent of potential outcomes: 
\begin{align*}
  R_i\Ind(Y_i^0,Y_i^1).
\end{align*}
```

Here, $\Ind$ codes for independence or random variables.
Independence can be enforced by the randomized allocation. 

We need a second assumption for the Brute Force Design to work:

```{definition,BF,name="Brute Force Validity"}
We assume that the randomized allocation of the program is mandatory and does not interfere with how potential outcomes are generated:
\begin{align*}
Y_i & = 
  \begin{cases}
    Y_i^1 & \text{ if } R_i=1  \\
    Y_i^0 & \text{ if } R_i=0      
  \end{cases}
\end{align*}
with $Y_i^1$ and $Y_i^0$ the same potential outcomes as defined in Lecture~0 with a routine allocation of the treatment.
```

Under both Idependence and Brute Force Validity, we have the follwing result:

```{theorem,BFATE,name="Identification in the Brute Force Design"}
Under Assumptions \@ref(def:independence) and \@ref(def:BF), the WW estimator identifies the Average Effect of the Treatment (ATE):
\begin{align*}
  \Delta^Y_{WW} & = \Delta^Y_{ATE},
\end{align*}

```

with:
\begin{align*}
  \Delta^Y_{WW} & = \esp{Y_i|R_i=1} - \esp{Y_i|R_i=0} \\
  \Delta^Y_{ATE} & = \esp{Y_i^1-Y_i^0}. 
\end{align*}

```{proof}
\begin{align*}
  \Delta^Y_{WW} & = \esp{Y_i|R_i=1} - \esp{Y_i|R_i=0} \\
                & = \esp{Y^1_i|R_i=1} - \esp{Y^0_i|R_i=0} \\
                & = \esp{Y_i^1}-\esp{Y_i^0}\\
                & = \esp{Y_i^1-Y_i^0},
\end{align*}
where the first equality uses Assumption \@ref(def:BF), the second equality Assumption \@ref(def:independence) and the last equality the linearity of the expectation operator.
```

```{remark}
As you can see from Theorem \@ref(thm:BFATE), ATE is the average effect of the treatment on the whole population, those who would be eligible for it and those who would not. 
ATE differs from TT because the effect of the treatment might be correlated with treatment intake. 
It is possible that the treatment has a bigger (resp. smaller) effect on treated individuals.
In that case, ATE is higher (resp. smaller) than TT.
```

```{remark}
Another related design is the Brute Force Design among Eligibles.
In this design, you impose the treatment status only among eligibles, irrespective of whether they want the treatment or not.
It can be operationalized using the selection rule used in Section \@ref(sec:design2).
```

```{example}
Let's use the example to illustrate the concept of ATE.
Let's generate data with our usual parameter values without allocating the treatment yet:
```

```{r parambis,eval=TRUE,echo=TRUE,results='hide'}
param <- c(8,.5,.28,1500,0.9,0.01,0.05,0.05,0.05,0.1)
names(param) <- c("barmu","sigma2mu","sigma2U","barY","rho","theta","sigma2epsilon","sigma2eta","delta","baralpha")
```

```{r simulbis,eval=TRUE,echo=TRUE,results='hide'}
set.seed(1234)
N <-1000
mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
UB <- rnorm(N,0,sqrt(param["sigma2U"]))
yB <- mu + UB 
YB <- exp(yB)
Ds <- rep(0,N)
Ds[YB<=param["barY"]] <- 1 
epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
U0 <- param["rho"]*UB + epsilon
y0 <- mu +  U0 + param["delta"]
alpha <- param["baralpha"]+  param["theta"]*mu + eta
y1 <- y0+alpha
Y0 <- exp(y0)
Y1 <- exp(y1)
```

In the sample, the ATE is the average difference between $y_i^1$ and $y_i^0$, or -- the expectation operator being linear -- the difference between average $y_i^1$ and average $y_i^0$.
In our sample, the former is equal to `r round(mean(y1-y0),3)` and the latter to `r round(mean(y1)-mean(y0),3)`.

In the population, the ATE is equal to: 

\begin{align*}
\Delta^y_{ATE} & = \esp{Y_i^1-Y_i^0} \\
              & = \esp{\alpha_i} \\
              & = \bar{\alpha}+\theta\bar{\mu}.
\end{align*}

Let's write a function to compute the value of the ATE and of TT (we derived the formula for TT in the previous lecture):
```{r delta.y,eval=TRUE,echo=TRUE,results='hide'}
delta.y.ate <- function(param){
  return(param["baralpha"]+param["theta"]*param["barmu"])
}
delta.y.tt <- function(param){
  return(param["baralpha"]+param["theta"]*param["barmu"]-param["theta"]*((param["sigma2mu"]*dnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))/(sqrt(param["sigma2mu"]+param["sigma2U"])*pnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))))
}
```

In the population, with our parameter values, $\Delta^y_{ATE}=$ `r round(delta.y.ate(param),3)` and $\Delta^y_{TT}=$ `r round(delta.y.tt(param),3)`.
In our case, selection into the treatment is correlated with lower outcomes, so that $TT\leq ATE$.

In order to implement the Brute Force Design in practice in a sample, we simply either draw a coin repeatedly for each member of the sample, assigning for example, all "heads" to the treatment and all "tails" to the control.
Because it can be a little cumbersome, it is possible to replace the coin toss by a pseudo-Random Number Generator (RNG), which is is an algorithm that tries to mimic the properties of random draws.
When generating the samples in the numerical exmples, I actually use a pseudo-RNG.
For example, we can draw from a uniform distribution on $[0,1]$ and allocate to the treatment all the individuals whose draw is smaller than 0.5:

\begin{align*}
  R_i^* & \sim \mathcal{U}[0,1]\\
  R_i & = 
  \begin{cases}
    1 & \text{ if } R_i^*\leq .5 \\
    0 & \text{ if } R_i^*> .5 
  \end{cases}
\end{align*}

The advantage of using a uniform law is that you can set up proportions of treated and controls easily.

```{example}
In our numerical example, the following R code generates two random groups, one treated and one control, and imposes the Assumption of Brute Force Validity:
```

```{r random.BFD,eval=TRUE,echo=TRUE,results='hide'}
# randomized allocation of 50% of individuals
Rs <- runif(N)
R <- ifelse(Rs<=.5,1,0)
y <- y1*R+y0*(1-R)
Y <- Y1*R+Y0*(1-R)
```

```{remark}
It is interesting to stop for one minute to think about how the Brute Force Design solves the FPCI.
First, with the ATE, the counterfactual problem is more severe than in the case of the TT.
In the routine mode of the program, where only eligible individuals receive the treatment, both parts of the ATE are unobserved:
```

  * $\esp{Y_i^1}$ is unobserved since we only observe the expected value of outcomes for the treated $\esp{Y_i^1|D_i=1}$, and they do not have to be the same.
  * $\esp{Y_i^0}$ is unobserved since we only observe the expected value of outcomes for the untreated $\esp{Y_i^0|D_i=0}$, and they do not have to be the same.

What the Brute Force Design does, is that it allocates randomly one part of the sample to the treatment, so that we see $\esp{Y_i^1|R_i=1}=\esp{Y_i^1}$ and one part to the control so that we see $\esp{Y_i^0|R_i=0}=\esp{Y_i^0}$.

### Estimating ATE

#### Using the WW estimator

In order to estimate ATE in a sample where the treatment has been randomized using a Brute Force Design, we simply use the sample equivalent of the With/Without estimator:
\begin{align*}
  \hat{\Delta}^Y_{WW} & = \frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N Y_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N Y_i(1-R_i).
\end{align*}

```{example}
In our numerical example, the WW estimator can be computed as follows in the sample:
```

```{r delta.y.ww,eval=TRUE,echo=TRUE,results='hide'}
delta.y.ww <- mean(y[R==1])-mean(y[R==0])
```

The WW estimator of the ATE in the sample is equal to `r round(delta.y.ww,3)`.
Let's recall that the true value of ATE is `r round(delta.y.ate(param),3)` in the population and `r round(mean(y1-y0),3)` in the sample.

We can also see in our example how the Brute Force Design approximates the counterfactual expectation $\esp{y_i^1}$ and its sample equivalent mean $\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^1_i$ by the observed mean in the treated sample $\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N y_iR_i$.
In our example, the sample value of the counterfactual mean potential outcome $\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^1_i$ is equal to `r round(mean(y1),3)` and the sample value of its observed counterpart is `r round(mean(y[R==1]),3)`.
Similarly, the sample value of the counterfactual mean potential outcome $\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^0_i$ is equal to `r round(mean(y0),3)` and the sample value of its observed counterpart is `r round(mean(y[R==0]),3)`.

#### Using OLS

As we have seen in Lecture 0, the WW estimator is numerically identical to the OLS estimator of a linear regression of outcomes on treatment:
The OLS coefficient $\beta$ in the following regression: 
\begin{align*}
  	Y_i &  = \alpha +  \beta R_i + U_i
	\end{align*}
is the WW estimator.

```{example}
In our numerical example, we can run the OLS regression as follows:
```

```{r delta.y.ols,eval=TRUE,echo=TRUE,results='hide'}
reg.y.R.ols <- lm(y~R)
```

$\hat{\Delta}^y_{OLS}=$ `r round(reg.y.R.ols$coef[2],3)` which is exactly equal, as expected, to the WW estimator: `r round(delta.y.ww,3)`.


#### Using OLS conditioning on covariates

The advantage of using OLS other the direct WW comparison is that it gives you a direct estimate of sampling noise (see next section) but also that it enables you to condition on additional covariates in the regression:
The OLS coefficient $\beta$ in the following regression: 
\begin{align*}
  	Y_i &  = \alpha +  \beta R_i + \gamma' X_i + U_i
	\end{align*}
is a consistent (and even unbiased) estimate of the ATE.

**<span style="font-variant:small-caps;">proof needed, especially assumption of linearity. Also, is interaction between $X_i$ and $R_i$ needed?</span>**

```{example}
In our numerical example, we can run the OLS regression conditioning on $y_i^B$ as follows:
```

```{r delta.y.ols.yB,eval=TRUE,echo=TRUE,results='hide'}
reg.y.R.ols.yB <- lm(y~R + yB)
```

$\hat{\Delta}^y_{OLSX}=$ `r round(reg.y.R.ols.yB$coef[2],3)`.
Note that $\hat{\Delta}^y_{OLSX}\neq\hat{\Delta}^y_{WW}$.
There is no numerical equivalence between the two estimators.

```{remark}
Why would you want to condition on covariates in an RCT?
Indeed, covariates should be balanced by randomization and thus there does not seem to be a rationale for conditioning on potential confounders, since there should be none.
The main reason why we condition on covariates is to decrease sampling noise.
Remember that sampling noise is due to imbalances between confounders in the treatment and control group.
Since these imbalances are not systematic, the WW estimator is unbiased.
We can also make the bias due to these unbalances as small as we want by choosing an adequate sample size (the WW estimator is consistent).
But for a given sample size, these imbalances generate sampling noise around the true ATE.
Conditioning on covariates helps decrease sampling noise by accounting for imbalances due to observed covariates.
If observed covariates explain a large part of the variation in outcomes, conditioning on them is going to prevent a lot of sampling noise from occuring.
```

```{example}
In order to make the gains in precision from conditioning on covariates apparent, let's use Monte Carlo simulations of our numerical example.
```

```{r montecarlobruteforceww,dependson='param',eval=TRUE,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE,results='hide',cache=TRUE}
monte.carlo.brute.force.ww <- function(s,N,param){
  set.seed(s)
  mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
  UB <- rnorm(N,0,sqrt(param["sigma2U"]))
  yB <- mu + UB 
  YB <- exp(yB)
  Ds <- rep(0,N)
  Ds[YB<=param["barY"]] <- 1 
  epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
  eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
  U0 <- param["rho"]*UB + epsilon
  y0 <- mu +  U0 + param["delta"]
  alpha <- param["baralpha"]+  param["theta"]*mu + eta
  y1 <- y0+alpha
  Y0 <- exp(y0)
  Y1 <- exp(y1)
  # randomized allocation of 50% of individuals
  Rs <- runif(N)
  R <- ifelse(Rs<=.5,1,0)
  y <- y1*R+y0*(1-R)
  Y <- Y1*R+Y0*(1-R)
  reg.y.R.ols <- lm(y~R)
  return(c(reg.y.R.ols$coef[2],sqrt(vcovHC(reg.y.R.ols,type='HC2')[2,2])))
}

simuls.brute.force.ww.N <- function(N,Nsim,param){
  simuls.brute.force.ww <- as.data.frame(matrix(unlist(lapply(1:Nsim,monte.carlo.brute.force.ww,N=N,param=param)),nrow=Nsim,ncol=2,byrow=TRUE))
  colnames(simuls.brute.force.ww) <- c('WW','se')
  return(simuls.brute.force.ww)
}

sf.simuls.brute.force.ww.N <- function(N,Nsim,param){
  sfInit(parallel=TRUE,cpus=2*ncpus)
  sfLibrary(sandwich)
  sim <- as.data.frame(matrix(unlist(sfLapply(1:Nsim,monte.carlo.brute.force.ww,N=N,param=param)),nrow=Nsim,ncol=2,byrow=TRUE))
  sfStop()
  colnames(sim) <- c('WW','se')
  return(sim)
}

Nsim <- 1000
#Nsim <- 10
N.sample <- c(100,1000,10000,100000)
#N.sample <- c(100,1000,10000)
#N.sample <- c(100,1000)
#N.sample <- c(100)

simuls.brute.force.ww <- lapply(N.sample,sf.simuls.brute.force.ww.N,Nsim=Nsim,param=param)
names(simuls.brute.force.ww) <- N.sample
```

```{r montecarlobruteforcewwyB,dependson='param',eval=TRUE,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE,results='hide',cache=TRUE}
monte.carlo.brute.force.ww.yB <- function(s,N,param){
  set.seed(s)
  mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
  UB <- rnorm(N,0,sqrt(param["sigma2U"]))
  yB <- mu + UB 
  YB <- exp(yB)
  Ds <- rep(0,N)
  Ds[YB<=param["barY"]] <- 1 
  epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
  eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
  U0 <- param["rho"]*UB + epsilon
  y0 <- mu +  U0 + param["delta"]
  alpha <- param["baralpha"]+  param["theta"]*mu + eta
  y1 <- y0+alpha
  Y0 <- exp(y0)
  Y1 <- exp(y1)
  # randomized allocation of 50% of individuals
  Rs <- runif(N)
  R <- ifelse(Rs<=.5,1,0)
  y <- y1*R+y0*(1-R)
  Y <- Y1*R+Y0*(1-R)
  reg.y.R.yB.ols <- lm(y~R + yB)
  return(c(reg.y.R.yB.ols$coef[2],sqrt(vcovHC(reg.y.R.yB.ols,type='HC2')[2,2])))
}

simuls.brute.force.ww.yB.N <- function(N,Nsim,param){
  simuls.brute.force.ww.yB <- as.data.frame(matrix(unlist(lapply(1:Nsim,monte.carlo.brute.force.ww.yB,N=N,param=param)),nrow=Nsim,ncol=2,byrow=TRUE))
  colnames(simuls.brute.force.ww.yB) <- c('WW','se')
  return(simuls.brute.force.ww.yB)
}

sf.simuls.brute.force.ww.yB.N <- function(N,Nsim,param){
  sfInit(parallel=TRUE,cpus=2*ncpus)
  sfLibrary(sandwich)
  sim <- as.data.frame(matrix(unlist(sfLapply(1:Nsim,monte.carlo.brute.force.ww.yB,N=N,param=param)),nrow=Nsim,ncol=2,byrow=TRUE))
  sfStop()
  colnames(sim) <- c('WW','se')
  return(sim)
}

Nsim <- 1000
#Nsim <- 10
N.sample <- c(100,1000,10000,100000)
#N.sample <- c(100,1000,10000)
#N.sample <- c(100,1000)
#N.sample <- c(100)

simuls.brute.force.ww.yB <- lapply(N.sample,sf.simuls.brute.force.ww.yB.N,Nsim=Nsim,param=param)
names(simuls.brute.force.ww.yB) <- N.sample
```

```{r montecarlohistbruteforcewwyB,dependson='monte.carlo.brute.force.ww.yB',eval=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='hide',fig.cap='Distribution of the $WW$ and $OLSX$ estimators in a Brute Force design over replications of samples of different sizes',fig.subcap=c('$WW$','OLSX'),fig.align='center',out.width="50%",fig.pos='htbp'}
par(mfrow=c(2,2))
for (i in 1:length(simuls.brute.force.ww)){
  hist(simuls.brute.force.ww[[i]][,'WW'],breaks=30,main=paste('N=',as.character(N.sample[i])),xlab=expression(hat(Delta^y)[WW]),xlim=c(-0.15,0.55))
  abline(v=delta.y.ate(param),col="red")
}
par(mfrow=c(2,2))
for (i in 1:length(simuls.brute.force.ww.yB)){
  hist(simuls.brute.force.ww.yB[[i]][,'WW'],breaks=30,main=paste('N=',as.character(N.sample[i])),xlab=expression(hat(Delta^y)[OLSX]),xlim=c(-0.15,0.55))
  abline(v=delta.y.ate(param),col="red")
}
```

Figure \@ref(fig:montecarlohistbruteforcewwyB) shows that the gains in precision from conditioning on $y_i^B$ are spectacular in our numerical example. 
They basically correspond to a gain in one order of magnitude of sample size: the precision of the $OLSX$ estimator conditioning on $y_i^B$ with a sample size of 100 is similar to the precision of the $OLS$ estimator not conditioning on $y_i^B$ with a sample size of $1000$.
This large gain in precision is largely due to the fact that $y_i$ and $y_i^B$ are highly correlated.
Not all covariates perform so well in actual samples in the social sciences.

```{remark}
The ability to condition on covariates in order to decrease sampling noise is a blessing but can also be a curse when combined with significance testing.
Indeed, you can now see that you can run a lot of regressions (with and without some covariates, interactions, etc) and maybe report only the statistically significant ones.
This is a bad practice that will lead to publication bias and inflated treatment effects.
Several possibilities in order to avoid that:
```

  1. Pre-register your analysis and explain which covariates you are going to use (with which interactions, etc) so that you cannot cherry pick your favorite results ex-post.
  2. Use a stratified design for your RCT (more on this in Lecture 6) so that the important covariates are already balanced between treated and controls.
  3. If unable to do all of the above, report results from regressions without controls and with various sets of controls.
  We do not expect the various treatment effect estimates to be the same (they cannot be, otherwise, they would have similar sampling noise), but we expect the following pattern: conditioning should systematically decrease sampling noise, not increase the treatment effect estimate.
  If conditioning on covariates makes a treatment effect significant, pay attention to why: is it because of a decrease in sampling noise (expected and OK) or because of an increase in treatment effect (beware specification search).

**<span style="font-variant:small-caps;">Revise that especially in light of Chapter \@ref(sec:meta)</span>**

```{remark}
You might not be happy with the assumption of linearity needed to use OLS to control for covariates.
I have read somewhere (forgot where) that this should not be much of a problem since covariates are well balanced between groups by randomization, and thus a linear first approximation to the function relating $X_i$ to $Y_i$ should be fine.
I tend not to buy that argument much.
I have to run simulations with a non linear relation between outcomes and controls and see how linear OLS performs.
If you do not like the linearity assumption, you can always use any of the nonparametric observational methods presented in Chapter \@ref(sec:OM).
```

#### Estimating Sampling Noise

In order to estimate sampling noise, you can either use the CLT-based approach or resampling, either using the bootstrap or randomization inference.
In Section \@ref(sec:estimsampnoise), we have already discussed how to estimate sampling noise when using the WW estimator that we are using here.
We are going to use the default and heteroskedasticity-robust standard errors from OLS, which are both CLT-based.
Only the heteroskedasticity-robust standard errors are valid under the assumptions that we have made so far.
Homoskedasticity would require constant treatment effects.
Heteroskedasticity being small in our numerical example, that should not matter much, but it could in other applications.

```{example}
Let us first estimate sampling noise for the simple WW estimator without control variables, using the OLS estimator.
```

```{r noiseolsBF,eval=TRUE,echo=TRUE,results='hide'}
sn.BF.simuls <- 2*quantile(abs(simuls.brute.force.ww[['1000']][,'WW']-delta.y.ate(param)),probs=c(0.99))
sn.BF.OLS.hetero <- 2*qnorm((delta+1)/2)*sqrt(vcovHC(reg.y.R.ols,type='HC2')[2,2])
sn.BF.OLS.homo <- 2*qnorm((delta+1)/2)*sqrt(vcov(reg.y.R.ols)[2,2])
```

The true value of the 99\% sampling noise with a sample size of 1000 and no control variables is stemming from the simulations is `r round(sn.BF.simuls,3)`.
The 99\% sampling noise estimated using heteroskedasticity robust OLS standard errors is `r round(sn.BF.OLS.hetero,3)`.
The 99\% sampling noise estimated using default OLS standard errors is `r round(sn.BF.OLS.homo,3)`.

Let us now estimate sampling noise for the simple WW estimator conditioning on $y_i^B$, using the OLS estimator.

```{r noiseolsyBBF,eval=TRUE,echo=TRUE,results='hide'}
sn.BF.simuls.yB <- 2*quantile(abs(simuls.brute.force.ww.yB[['1000']][,'WW']-delta.y.ate(param)),probs=c(0.99))
sn.BF.OLS.hetero.yB <- 2*qnorm((delta+1)/2)*sqrt(vcovHC(reg.y.R.ols.yB,type='HC2')[2,2])
sn.BF.OLS.homo.yB <- 2*qnorm((delta+1)/2)*sqrt(vcov(reg.y.R.ols.yB)[2,2])
```

The true value of the 99\% sampling noise with a sample size of 1000 and no control variables is stemming from the simulations is `r round(sn.BF.simuls.yB,3)`.
The 99\% sampling noise estimated using heteroskedasticity robust OLS standard errors is `r round(sn.BF.OLS.hetero.yB,3)`.
The 99\% sampling noise estimated using default OLS standard errors is `r round(sn.BF.OLS.homo.yB,3)`.

Let's see how all of this works on average.
Figure \@ref(fig:sampnoisewwBFCLTplot) shows that overall the sampling nois eis much lower with $OLSX$ than with $WW$, as expected from Figure \@ref(fig:montecarlohistbruteforcewwyB).
The CLT-based estimator of sampling noise accounting for heteroskedasticity (in blue) recovers true sampling noise (in red) pretty well.
Figure \@ref(fig:sampnoisewwBFCLTall) shows that the CLT-based estimates of sampling noise are on point, except for $N=10000$, where the CLT slightly overestimates true sampling noise. 
Figure \@ref(fig:confintervalCLTBF) shows what happens when conditioning on $Y^B$ in a selection of 40 samples.
The reduction in samplong noise is pretty drastic here.

```{r sampnoisewwBFCLTplot,eval=TRUE,echo=TRUE,fig.cap='Average CLT-based approximations of sampling noise in the Brute Force design for $WW$ and $OLSX$ over replications of samples of different sizes (true sampling noise in red)',fig.align='center',out.width="50%",fig.pos='htbp'}
for (k in (1:length(N.sample))){
  simuls.brute.force.ww[[k]]$CLT.noise <- 2*qnorm((delta+1)/2)*simuls.brute.force.ww[[k]][,'se']
  simuls.brute.force.ww.yB[[k]]$CLT.noise <- 2*qnorm((delta+1)/2)*simuls.brute.force.ww.yB[[k]][,'se']
}

samp.noise.ww.BF <- sapply(lapply(simuls.brute.force.ww,`[`,,1),samp.noise,delta=delta)
precision.ww.BF <- sapply(lapply(simuls.brute.force.ww,`[`,,1),precision,delta=delta)
names(precision.ww.BF) <- N.sample
signal.to.noise.ww.BF <- sapply(lapply(simuls.brute.force.ww,`[`,,1),signal.to.noise,delta=delta,param=param)
names(signal.to.noise.ww.BF) <- N.sample
table.noise.BF <- cbind(samp.noise.ww.BF,precision.ww.BF,signal.to.noise.ww.BF)
colnames(table.noise.BF) <- c('sampling.noise', 'precision', 'signal.to.noise')
table.noise.BF <- as.data.frame(table.noise.BF)
table.noise.BF$N <- as.numeric(N.sample)
table.noise.BF$ATE <- rep(delta.y.ate(param),nrow(table.noise.BF))
for (k in (1:length(N.sample))){
  table.noise.BF$CLT.noise[k] <- mean(simuls.brute.force.ww[[k]]$CLT.noise)
}
table.noise.BF$Method <- rep("WW",nrow(table.noise.BF))

samp.noise.ww.BF.yB <- sapply(lapply(simuls.brute.force.ww.yB,`[`,,1),samp.noise,delta=delta)
precision.ww.BF.yB <- sapply(lapply(simuls.brute.force.ww.yB,`[`,,1),precision,delta=delta)
names(precision.ww.BF.yB) <- N.sample
signal.to.noise.ww.BF.yB <- sapply(lapply(simuls.brute.force.ww.yB,`[`,,1),signal.to.noise,delta=delta,param=param)
names(signal.to.noise.ww.BF.yB) <- N.sample
table.noise.BF.yB <- cbind(samp.noise.ww.BF.yB,precision.ww.BF.yB,signal.to.noise.ww.BF.yB)
colnames(table.noise.BF.yB) <- c('sampling.noise', 'precision', 'signal.to.noise')
table.noise.BF.yB <- as.data.frame(table.noise.BF.yB)
table.noise.BF.yB$N <- as.numeric(N.sample)
table.noise.BF.yB$ATE <- rep(delta.y.ate(param),nrow(table.noise.BF.yB))
for (k in (1:length(N.sample))){
  table.noise.BF.yB$CLT.noise[k] <- mean(simuls.brute.force.ww.yB[[k]]$CLT.noise)
}
table.noise.BF.yB$Method <- rep("OLSX",nrow(table.noise.BF))

table.noise.BF.tot <- rbind(table.noise.BF,table.noise.BF.yB)
table.noise.BF.tot$Method <- factor(table.noise.BF.tot$Method,levels=c("WW","OLSX"))

ggplot(table.noise.BF.tot, aes(x=as.factor(N), y=ATE,fill=Method)) +
  geom_bar(position=position_dodge(), stat="identity", colour='black') +
  geom_errorbar(aes(ymin=ATE-sampling.noise/2, ymax=ATE+sampling.noise/2), width=.2,position=position_dodge(.9),color='red') +
  geom_errorbar(aes(ymin=ATE-CLT.noise/2, ymax=ATE+CLT.noise/2), width=.2,position=position_dodge(.9),color='blue') +
  xlab("Sample Size")+
  theme_bw()+
  theme(legend.position=c(0.85,0.88))
```


```{r sampnoisewwBFCLTall,eval=TRUE,echo=TRUE,fig.cap='Distribution of the CLT approximation of sampling noise in the Brute Force design for $WW$ and $OLSX$ over replications of samples of different sizes (true sampling noise in red)',fig.subcap=c('$WW$','OLSX'),fig.align='center',out.width="50%",fig.pos='htbp'}
par(mfrow=c(2,2))
for (i in 1:length(simuls.brute.force.ww)){
  hist(simuls.brute.force.ww[[i]][,'CLT.noise'],main=paste('N=',as.character(N.sample[i])),xlab=expression(hat(2*bar(epsilon))[WW]),xlim=c(min(table.noise.BF[i,colnames(table.noise)=='sampling.noise'],min(simuls.brute.force.ww[[i]][,'CLT.noise'])),max(table.noise.BF[i,colnames(table.noise)=='sampling.noise'],max(simuls.brute.force.ww[[i]][,'CLT.noise']))))
  abline(v=table.noise.BF[i,colnames(table.noise)=='sampling.noise'],col="red")
}
par(mfrow=c(2,2))
for (i in 1:length(simuls.brute.force.ww.yB)){
  hist(simuls.brute.force.ww.yB[[i]][,'CLT.noise'],main=paste('N=',as.character(N.sample[i])),xlab=expression(hat(2*bar(epsilon))[OLSX]),xlim=c(min(table.noise.BF.yB[i,colnames(table.noise)=='sampling.noise'],min(simuls.brute.force.ww.yB[[i]][,'CLT.noise'])),max(table.noise.BF.yB[i,colnames(table.noise)=='sampling.noise'],max(simuls.brute.force.ww.yB[[i]][,'CLT.noise']))))
  abline(v=table.noise.BF.yB[i,colnames(table.noise)=='sampling.noise'],col="red")
}
```

```{r confintervalCLTBF,dependson=c('monte.carlo'),eval=TRUE,echo=TRUE,results='hide',warning=FALSE,error=FALSE,message=FALSE,fig.cap=paste('CLT-based confidence intervals of $\\hat{WW}$ and $\\hat{OLSX}$ for $\\delta=$',delta,'over sample replications for various sample sizes (true confidence intervals in red)',sep=' '),fig.subcap=c('$WW$','OLSX'),fig.align='center',out.width="50%",fig.pos='htbp',fig.height=12,fig.width=10}
N.plot <- 40
plot.list <- list()
limx <- list(c(-0.65,1.25),c(-0.1,0.5),c(0,0.30),c(0,0.25))

for (k in 1:length(N.sample)){
  set.seed(1234)
  test.CLT.BF <- simuls.brute.force.ww[[k]][sample(N.plot),c('WW','CLT.noise')]
  test.CLT.BF <- as.data.frame(cbind(test.CLT.BF,rep(samp.noise(simuls.brute.force.ww[[k]][,'WW'],delta=delta),N.plot)))
  colnames(test.CLT.BF) <- c('WW','CLT.noise','sampling.noise')
  test.CLT.BF$id <- 1:N.plot
  plot.test.CLT.BF <- ggplot(test.CLT.BF, aes(x=as.factor(id), y=WW)) +
      geom_bar(position=position_dodge(), stat="identity", colour='black') +
      geom_errorbar(aes(ymin=WW-sampling.noise/2, ymax=WW+sampling.noise/2), width=.2,position=position_dodge(.9),color='red') +
      geom_errorbar(aes(ymin=WW-CLT.noise/2, ymax=WW+CLT.noise/2), width=.2,position=position_dodge(.9),color='blue') +
      geom_hline(aes(yintercept=delta.y.ate(param)), colour="#990000", linetype="dashed")+
      ylim(limx[[k]][1],limx[[k]][2])+
      xlab("Sample id")+
      theme_bw()+
      ggtitle(paste("N=",N.sample[k]))
  plot.list[[k]] <- plot.test.CLT.BF
}
plot.CI.BF <- plot_grid(plot.list[[1]],plot.list[[2]],plot.list[[3]],plot.list[[4]],ncol=1,nrow=length(N.sample))
print(plot.CI.BF)

plot.list <- list()
for (k in 1:length(N.sample)){
  set.seed(1234)
  test.CLT.BF.yB <- simuls.brute.force.ww.yB[[k]][sample(N.plot),c('WW','CLT.noise')]
  test.CLT.BF.yB <- as.data.frame(cbind(test.CLT.BF.yB,rep(samp.noise(simuls.brute.force.ww.yB[[k]][,'WW'],delta=delta),N.plot)))
  colnames(test.CLT.BF.yB) <- c('WW','CLT.noise','sampling.noise')
  test.CLT.BF.yB$id <- 1:N.plot
  plot.test.CLT.BF.yB <- ggplot(test.CLT.BF.yB, aes(x=as.factor(id), y=WW)) +
      geom_bar(position=position_dodge(), stat="identity", colour='black') +
      geom_errorbar(aes(ymin=WW-sampling.noise/2, ymax=WW+sampling.noise/2), width=.2,position=position_dodge(.9),color='red') +
      geom_errorbar(aes(ymin=WW-CLT.noise/2, ymax=WW+CLT.noise/2), width=.2,position=position_dodge(.9),color='blue') +
      geom_hline(aes(yintercept=delta.y.ate(param)), colour="#990000", linetype="dashed")+
      ylim(limx[[k]][1],limx[[k]][2])+
      xlab("Sample id")+
      ylab("OLSX")+
      theme_bw()+
      ggtitle(paste("N=",N.sample[k]))
  plot.list[[k]] <- plot.test.CLT.BF.yB
}
plot.CI.BF.yB <- plot_grid(plot.list[[1]],plot.list[[2]],plot.list[[3]],plot.list[[4]],ncol=1,nrow=length(N.sample))
print(plot.CI.BF.yB)

```

## Randomization After Self-Selection {#sec:design2}

In Randomization After Self-Selection, individuals are randomly assigned to the treatment after having expressed their willingness to receive it.
This design is able to recover the average effect of the Treatment on the Treated (TT).

In order to explain this design clearly, and especially to make it clear how it differs from the following one (randomization after eligibility), I have to introduce a slightly more complex selection rule that we have seen so far, one that includes self-selection, \textit{i.e.} take-up decisions by agents.
We are going to assume that there are two steps in agents' participation process:

  * Eligibility: agents' eligibility is assessed first, giving rise to a group of eligible individuals ($E_i=1$) and a group of non eligible individuals ($E_i=0$). 
  * Self-selection: eligible agents can then decide whether they want to take-up the proposed treatment or not. 
    $D_i=1$ for those who do.
    $D_i=0$ for those who do not.
    By convention, ineligibles have $D_i=0$.

```{example}
In our numerical example, here are the equations operationalizing these notions:
```
\begin{align*}
E_i & = \uns{y_i^B\leq\bar{y}} \\
D_i & = \uns{\underbrace{\bar{\alpha}+\theta\bar{\mu}-C_i}_{D_i^*}\geq0 \land E_i=1} \\
C_i & = \bar{c} + \gamma \mu_i + V_i\\
V_i & \sim \mathcal{N}(0,\sigma^2_V)
\end{align*}

Eligibility is still decided based on pre-treatment outcomes being smaller than a threshold level $\bar{y}$.
Self-selection among eligibles is decided by the net utility of the treatment $D_i^*$ being positive.
Here, the net utility is composed of the average gain from the treatment (assuming agents cannot foresee their idiosyncratic gain from the treatment apart from the part depending on $\mu_i$) $\bar{\alpha}+\theta\bar{\mu}$ minus the cost of participation $C_i$.
The cost of participation in turn depends on a constant, on $\mu_i$ and on a random shock orthogonal to everything else $V_i$.
This cost might represent the administrative cost of applying for the treatment and the opportunity cost of participating into the treatment (foregone earnings and/or cost of time).
Conditional on eligiblity, self-selection is endogenous in this model since both the gains and the cost of participation depend on $\mu_i$.
Costs depend on $\mu_i$ since most productive people may face lower administrative costs but a higher opportunity cost of time.

Let's choose some values for the new parameters:

```{r paramselfselect,dependson='param',eval=TRUE,echo=TRUE,results='hide'}
param <- c(param,-6.25,0.9,0.5)
names(param) <- c("barmu","sigma2mu","sigma2U","barY","rho","theta","sigma2epsilon","sigma2eta","delta","baralpha","barc","gamma","sigma2V")
```

and let's generate a new dataset:

```{r simulselfselect,dependson='param.self.select',eval=TRUE,echo=TRUE,results='hide'}
set.seed(1234)
N <-1000
mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
UB <- rnorm(N,0,sqrt(param["sigma2U"]))
yB <- mu + UB 
YB <- exp(yB)
E <- ifelse(YB<=param["barY"],1,0)
V <- rnorm(N,0,param["sigma2V"])
Dstar <- param["baralpha"]+param["theta"]*param["barmu"]-param["barc"]-param["gamma"]*mu-V
Ds <- ifelse(Dstar>=0 & E==1,1,0)
epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
U0 <- param["rho"]*UB + epsilon
y0 <- mu +  U0 + param["delta"]
alpha <- param["baralpha"]+  param["theta"]*mu + eta
y1 <- y0+alpha
Y0 <- exp(y0)
Y1 <- exp(y1)
```

Let's compute the value of the TT parameter in this new model:

\begin{align*}
  \Delta^y_{TT} & = \bar{\alpha}+ \theta\esp{\mu_i|\mu_i+U_i^B\leq\bar{y} \land \bar{\alpha}+\theta\bar{\mu}-\bar{c}-\gamma\mu_i-V_i\geq0}
\end{align*}

To compute the expectation of a doubly censored normal, I use the package `tmvtnorm`.

\begin{align*}
  (\mu_i,y_i^B,D_i^*) & = \mathcal{N}\left(\bar{\mu},\bar{\mu},\bar{\alpha}+(\theta-\gamma)\bar{\mu}-\bar{c},
                                        \left(\begin{array}{ccc}
                                              \sigma^2_{\mu} & \sigma^2_{\mu} & -\gamma\sigma^2_{\mu} \\
                                              \sigma^2_{\mu} & \sigma^2_{\mu} + \sigma^2_{U} & -\gamma\sigma^2_{\mu} \\
                                              -\gamma\sigma^2_{\mu} & -\gamma\sigma^2_{\mu} & \gamma^2\sigma^2_{\mu}+\sigma^2_{V}
                                              \end{array}
                                        \right)
                                      \right)
\end{align*}

```{r ttselfselect,eval=TRUE,echo=TRUE,results='hide'}
mean.mu.yB.Dstar <- c(param['barmu'],param['barmu'],param['baralpha']- param['barc']+(param['theta']-param['gamma'])*param['barmu'])
cov.mu.yB.Dstar <- matrix(c(param['sigma2mu'],param["sigma2mu"],-param['gamma']*param["sigma2mu"],
                            param["sigma2mu"],param['sigma2mu']+param['sigma2U'],-param['gamma']*param["sigma2mu"],
                            -param['gamma']*param["sigma2mu"],-param['gamma']*param["sigma2mu"],param["sigma2mu"]*(param['gamma'])^2+param['sigma2V']),3,3,byrow=TRUE)
lower.cut <- c(-Inf,-Inf,0)
upper.cut <- c(Inf,log(param['barY']),Inf)
moments.cut <- mtmvnorm(mean=mean.mu.yB.Dstar,sigma=cov.mu.yB.Dstar,lower=lower.cut,upper=upper.cut)
delta.y.tt <- param['baralpha']+ param['theta']*moments.cut$tmean[1]
delta.y.ww.self.select <- mean(y[R==1 & Ds==1])-mean(y[R==0 & Ds==1])
```

The value of $\Delta^y_{TT}$ in our illustration is now `r round(delta.y.tt,3)`.

### Identification

With Randomization After Self-Selection, identification requires two assumptions:

```{definition,indepss,name='Independence Among Self-Selected'}
We assume that the randomized allocation of the program among applicants is well done: 

\begin{align*}
  R_i\Ind(Y_i^0,Y_i^1)|D_i=1.
\end{align*}

```

Independence can be enforced by the randomized allocation of the treatment among the eligible applicants. 

We need a second assumption:

```{definition,rassval,name='Randomization After Self-Selection Validity'}
We assume that the randomized allocation of the program does not interfere with how potential outcomes and self-selection are generated:

\begin{align*}
Y_i & = 
  \begin{cases}
    Y_i^1 & \text{ if } (R_i=1 \text{ and } D_i=1)   \\
    Y_i^0 & \text{ if } (R_i=0 \text{ and } D_i=1) \text{ or } D_i=0
  \end{cases}
\end{align*}
```

with $Y_i^1$, $Y_i^0$ and $D_i$ the same potential outcomes and self-selection decisions as in a routine allocation of the treatment.

Under these assumptions, we have the following result:

```{theorem,idrass,name='Identification With Randomization After Self-Selection'}
Under Assumptions \@ref(def:indepss) and \@ref(def:rassval), the WW estimator among the self-selected identifies TT:

\begin{align*}
  \Delta^Y_{WW|D=1} & = \Delta^Y_{TT},
\end{align*}

```

with:

\begin{align*}
  \Delta^Y_{WW|D=1} & = \esp{Y_i|R_i=1,D_i=1} - \esp{Y_i|R_i=0,D_i=1}.
\end{align*}

```{proof}
\begin{align*}
  \Delta^Y_{WW|D=1} & = \esp{Y_i|R_i=1,D_i=1} - \esp{Y_i|R_i=0,D_i=1} \\
                    & = \esp{Y^1_i|R_i=1,D_i=1} - \esp{Y^0_i|R_i=0,D_i=1} \\
                    & = \esp{Y_i^1|D_i=1}-\esp{Y_i^0|D_i=1}\\
                    & = \esp{Y_i^1-Y_i^0|D_i=1},
\end{align*}
where the second equality uses Randomization After Self-Selection Validity, the third equality Independence Among Self-Selected and the last equality the linearity of the expectation operator.
```

```{remark}
The key intuitions for how Randomization After Self-Selection solves the FPCI are:
```

  * By allowing for eligibilty and self-selection, we identify the agents that would benefit from the treatment in routine mode (the treated).
  * By randomly denying the treatment to some of the treated, we can estimate the counterfactual outcome of the treated by looking at the counterfactual outcome of the denied applicants: $\esp{Y_i^0|D_i=1}=\esp{Y_i|R_i=0,D_i=1}$.

```{remark}
In practice, we use a pseudo-RNG to generate a random allocation among applicants:
```
\begin{align*}
  R_i^* & \sim \mathcal{U}[0,1]\\
  R_i & = 
  \begin{cases}
    1 & \text{ if } R_i^*\leq .5 \land D_i=1\\
    0 & \text{ if } R_i^*> .5 \land D_i=1
  \end{cases}
\end{align*}

```{example}
In our numerical example, the following R code generates two random groups, one treated and one control, and imposes the Assumption of Randomization After Self-Selection Validity:
```

```{r randomRASS,eval=TRUE,echo=TRUE,results='hide'}
#random allocation among self-selected
Rs <- runif(N)
R <- ifelse(Rs<=.5 & Ds==1,1,0)
y <- y1*R+y0*(1-R)
Y <- Y1*R+Y0*(1-R)
```

### Estimating TT

#### Using the WW Estimator

As in the case of the Brute Force Design, we can use the WW estimator to estimate the effect of the program with Randomization After Self-Selection, except that this time the WW estimator is applied among applicant to the program only:

\begin{align*}
  \hat{\Delta}^Y_{WW|D=1} & = \frac{1}{\sum_{i=1}^N D_iR_i}\sum_{i=1}^N Y_iD_iR_i-\frac{1}{\sum_{i=1}^N D_i(1-R_i)}\sum_{i=1}^N D_iY_i(1-R_i).
\end{align*}

```{example}
In our numerical example, we can form the WW estimator among applicants:
```

```{r wwselfselect,eval=TRUE,echo=TRUE,results='hide'}
delta.y.ww.self.select <- mean(y[R==1 & Ds==1])-mean(y[R==0 & Ds==1])
```

WW among applicants is equal to `r round(delta.y.ww.self.select,3)`.
It is actually rather far from the true value of `r round(delta.y.tt,3)`, which reminds us that unbiasedness does not mean that a given sample will not suffer from a large bias.
We just drew a bad sample where confounders are not very well balanced.

#### Using OLS

As in the Brute Force Design with the ATE, we can estimate the TT parameter with Randomization After Self-Selection using the OLS estimator.
In the following regression run among applicants only (with $D_i=1$), $\beta$ estimates TT:

\begin{align*}
    Y_i &  = \alpha +  \beta R_i + U_i.
	\end{align*}

As a matter of fact, the OLS estimator without control variables is numerically equivalent to the WW estimator.

```{example}
In our numerical example, here is the OLS regression:
```

```{r deltayolsselfselect,eval=TRUE,echo=TRUE,results='hide'}
reg.y.R.ols.self.select <- lm(y[Ds==1]~R[Ds==1])
```

The value of the OLS estimator is `r round(reg.y.R.ols.self.select$coef[2],3)`, which is identical to the WW estimator among applicants.

#### Using OLS Conditioning on Covariates

We might want to condition on covariates in order to reduce the amount of sampling noise.
Parametrically, we can run the following OLS regression among applicants (with $D_i=1$):

\begin{align*}
    Y_i &  = \alpha +  \beta R_i + \gamma' X_i + U_i.
\end{align*}

$\beta$ estimates the TT.

**<span style="font-variant:small-caps;">Needed: proof. Especially check whether we need to center covariates at the mean of the treatment group. I think so.</span>**

We can also use Matching to obtain a nonparametric estimator.

```{example}
Let us first compute the OLS estimator conditioning on $y_i^B$:
```

```{r deltayyBolsselfselect,eval=TRUE,echo=TRUE,results='hide'}
reg.y.R.yB.ols.self.select <- lm(y[Ds==1] ~ R[Ds==1] + yB[Ds==1])
```

Our estimate of TT after conditioning on $y_i^B$ is `r round(reg.y.R.yB.ols.self.select$coef[2],3)`.
Conditioning on $y_i^B$ has been able to solve part of the bias of the WW problem estimator.

Let's now check whether conditioning on OLS has brought an improvement in terms of decreased sampling noise.

```{r montecarloselfselectww,dependson='param.self.select',eval=TRUE,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE,results='hide',cache=TRUE}
monte.carlo.self.select.ww <- function(s,N,param){
  set.seed(s)
  mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
  UB <- rnorm(N,0,sqrt(param["sigma2U"]))
  yB <- mu + UB 
  YB <- exp(yB)
  E <- ifelse(YB<=param["barY"],1,0)
  V <- rnorm(N,0,param["sigma2V"])
  Dstar <- param["baralpha"]+param["theta"]*param["barmu"]-param["barc"]-param["gamma"]*mu-V
  Ds <- ifelse(Dstar>=0 & E==1,1,0)
  epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
  eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
  U0 <- param["rho"]*UB + epsilon
  y0 <- mu +  U0 + param["delta"]
  alpha <- param["baralpha"]+  param["theta"]*mu + eta
  y1 <- y0+alpha
  Y0 <- exp(y0)
  Y1 <- exp(y1)
  
  #random allocation among self-selected
  Rs <- runif(N)
  R <- ifelse(Rs<=.5 & Ds==1,1,0)
  y <- y1*R+y0*(1-R)
  Y <- Y1*R+Y0*(1-R)
  return(mean(y[R==1 & Ds==1])-mean(y[R==0 & Ds==1]))
}

simuls.self.select.ww.N <- function(N,Nsim,param){
  simuls.self.select.ww <- matrix(unlist(lapply(1:Nsim,monte.carlo.self.select.ww,N=N,param=param)),nrow=Nsim,ncol=1,byrow=TRUE)
  colnames(simuls.self.select.ww) <- c('WW')
  return(simuls.self.select.ww)
}

sf.simuls.self.select.ww.N <- function(N,Nsim,param){
  sfInit(parallel=TRUE,cpus=8)
  sim <- matrix(unlist(sfLapply(1:Nsim,monte.carlo.self.select.ww,N=N,param=param)),nrow=Nsim,ncol=1,byrow=TRUE)
  sfStop()
  colnames(sim) <- c('WW')
  return(sim)
}

Nsim <- 1000
#Nsim <- 10
N.sample <- c(100,1000,10000,100000)
#N.sample <- c(100,1000,10000)
#N.sample <- c(100,1000)
#N.sample <- c(100)

simuls.self.select.ww <- lapply(N.sample,sf.simuls.self.select.ww.N,Nsim=Nsim,param=param)
names(simuls.self.select.ww) <- N.sample
```

```{r montecarloselfselectyBww,dependson='param.self.select',eval=TRUE,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE,results='hide',cache=TRUE}
monte.carlo.self.select.yB.ww <- function(s,N,param){
  set.seed(s)
  mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
  UB <- rnorm(N,0,sqrt(param["sigma2U"]))
  yB <- mu + UB 
  YB <- exp(yB)
  E <- ifelse(YB<=param["barY"],1,0)
  V <- rnorm(N,0,param["sigma2V"])
  Dstar <- param["baralpha"]+param["theta"]*param["barmu"]-param["barc"]-param["gamma"]*mu-V
  Ds <- ifelse(Dstar>=0 & E==1,1,0)
  epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
  eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
  U0 <- param["rho"]*UB + epsilon
  y0 <- mu +  U0 + param["delta"]
  alpha <- param["baralpha"]+  param["theta"]*mu + eta
  y1 <- y0+alpha
  Y0 <- exp(y0)
  Y1 <- exp(y1)
  
  #random allocation among self-selected
  Rs <- runif(N)
  R <- ifelse(Rs<=.5 & Ds==1,1,0)
  y <- y1*R+y0*(1-R)
  Y <- Y1*R+Y0*(1-R)
  reg.y.R.yB.ols.self.select <- lm(y[Ds==1] ~ R[Ds==1] + yB[Ds==1])
  return(reg.y.R.yB.ols.self.select$coef[2])
}

simuls.self.select.yB.ww.N <- function(N,Nsim,param){
  simuls.self.select.yB.ww <- matrix(unlist(lapply(1:Nsim,monte.carlo.self.select.yB.ww,N=N,param=param)),nrow=Nsim,ncol=1,byrow=TRUE)
  colnames(simuls.self.select.yB.ww) <- c('WW')
  return(simuls.self.select.yB.ww)
}

sf.simuls.self.select.yB.ww.N <- function(N,Nsim,param){
  sfInit(parallel=TRUE,cpus=8)
  sim <- matrix(unlist(sfLapply(1:Nsim,monte.carlo.self.select.yB.ww,N=N,param=param)),nrow=Nsim,ncol=1,byrow=TRUE)
  sfStop()
  colnames(sim) <- c('WW')
  return(sim)
}

Nsim <- 1000
#Nsim <- 10
N.sample <- c(100,1000,10000,100000)
#N.sample <- c(100,1000,10000)
#N.sample <- c(100,1000)
#N.sample <- c(100)

simuls.self.select.yB.ww <- lapply(N.sample,sf.simuls.self.select.yB.ww.N,Nsim=Nsim,param=param)
names(simuls.self.select.yB.ww) <- N.sample
```

```{r montecarlohistselfselectww,dependson='monte.carlo.self.select.ww',eval=TRUE,echo=TRUE,warning=FALSE,error=FALSE,message=FALSE,results='hide',fig.cap='Distribution of the $WW$ and $OLSX$ estimator with randomization after self-selection over replications of samples of different sizes',fig.subcap=c('$WW$','$OLSX$'),fig.align='center',out.width="50%",fig.pos='htbp'}
par(mfrow=c(2,2))
for (i in 1:length(simuls.self.select.ww)){
  hist(simuls.self.select.ww[[i]][,'WW'],breaks=30,main=paste('N=',as.character(N.sample[i])),xlab=expression(hat(Delta^yWW)),xlim=c(-0.15,0.55))
  abline(v=delta.y.tt,col="red")
}
par(mfrow=c(2,2))
for (i in 1:length(simuls.self.select.yB.ww)){
  hist(simuls.self.select.yB.ww[[i]][,'WW'],breaks=30,main=paste('N=',as.character(N.sample[i])),xlab=expression(hat(Delta^yWW)),xlim=c(-0.15,0.55))
  abline(v=delta.y.tt,col="red")
}
```

Figure \@ref(fig:montecarlohistselfselectww) shows that, in our example, conditioning on covariates improves precision by the same amount as an increase in sample size by almost one order of magnitude. 

#### Estimating Sampling Noise

In order to estimate precision, we can either use the CLT, deriving sampling noise from the heteroskedasticity-robust standard error OLS estimates, or we can use some form of resampling as the bootstrap or randomization inference.

```{example}
Let us derive the CLT-based estimates of sampling noise using the OLS standard errors without conditioning on covariates first.
```

```{r noiseolsRASS,eval=TRUE,echo=TRUE,results='hide'}
sn.RASS.simuls <- 2*quantile(abs(simuls.self.select.ww[['1000']][,'WW']-delta.y.tt),probs=c(0.99))
sn.RASS.OLS.homo <- 2*qnorm((.99+1)/2)*sqrt(vcov(reg.y.R.ols.self.select)[2,2])
sn.RASS.OLS.hetero <- 2*qnorm((.99+1)/2)*sqrt(vcovHC(reg.y.R.ols.self.select,type='HC2')[2,2])
```

True 99\% sampling noise (from the simulations) is `r round(sn.RASS.simuls,3)`.
99\% sampling noise estimated using default OLS standard errors is `r round(sn.RASS.OLS.homo,3)`.
99\% sampling noise estimated using heteroskedasticity robust OLS standard errors is `r round(sn.RASS.OLS.hetero,3)`.

Conditioning on covariates:

```{r noiseolsRASSyB,eval=TRUE,echo=TRUE,results='hide'}
sn.RASS.simuls.yB <- 2*quantile(abs(simuls.self.select.yB.ww[['1000']][,'WW']-delta.y.tt),probs=c(0.99))
sn.RASS.OLS.homo.yB <- 2*qnorm((.99+1)/2)*sqrt(vcov(reg.y.R.yB.ols.self.select)[2,2])
sn.RASS.OLS.hetero.yB <- 2*qnorm((.99+1)/2)*sqrt(vcovHC(reg.y.R.yB.ols.self.select,type='HC2')[2,2])
```

True 99\% sampling noise (from the simulations) is `r round(sn.RASS.simuls.yB,3)`.
99\% sampling noise estimated using default OLS standard errors is `r round(sn.RASS.OLS.homo.yB,3)`.
99\% sampling noise estimated using heteroskedasticity robust OLS standard errors is `r round(sn.RASS.OLS.hetero.yB,3)`.


## Randomization After Eligibility {#sec:design3}

## Encouragement Design {#sec:design4}

## Threats to the validity of RCTs {#sec:threats}
