# (PART) Methods of Causal Inference {-}

# Randomized Controlled Trials {#RCT}

The most robust and rigorous method that has been devised by social scientists to estimate the effect of an intervention on an outcome is the Randomized Controlled Trial (RCT).
RCTs are used extensively in the field to evaluate a wide array of programs, from development, labor and education interventions to environmental nudges to website and search engine features.

The key feature of an RCT is the introduction by the researcher of randomness in the allocation of the treatment.
Individuals with $R_i=1$, where $R_i$ denotes the outcome of a random event, such as a coin toss, have a higher probability of receiving the treatment.
Potential outcomes have the same distribution in both $R_i=1$ and $R_i=0$ groups.
If we observe different outcomes between the treatment and control group, it has to be because of the causal effect of the treatment, since both groups only differ by the proportion of treated and controls.

The most attractive feature of RCTs is that researchers enforce the main identification assumption (we do not have to assume that it holds, we can make sure that it does).
This property of RCTs distinguishes them from all the other methods that we are going to learn in this class.

In this lecture, we are going to study how to estimate the effect of an intervention on an outcome using RCTs. 
We are especially going to study the various types of designs and what can be recovered from them using which technique.
For each design, we are going to detail which treatment effect it enables us to identify, how to obtain a sample estimate of this treatment effect and how to estimate the associated sampling noise.
The main substantial difference between these four designs are the types of treatment effect parameters that they enable us to recover.
Sections \@ref(sec:design1) to \@ref(sec:design4) of this lecture introduces the four designs and how to analyze them.

Unfortunately, RCTs are not bullet proof.
They suffer from problems that might make their estimates of causal effects badly biased.
Section \@ref(sec:threats) surveys the various threats and what we can do to try to minimize them.

## Brute Force Design {#sec:design1}

In the Brute Force Design, eligible individuals are randomly assigned to the treatment irrespective of their willingness to accept it and have to comply with the assignment.
This is a rather dumb procedure but it is very easy to analyze and that is why I start with it.
With the Brute Force Design, you can recover the average effect of the treatment on the whole population.
This parameter is generally called the Average Treatment Effect (ATE).

In this section, I am going to detail the assumptions required for the Brute Force Design to identify the ATE, how to form an estimator of the ATE and how to estimate its sampling noise.

### Identification

In the Brute Force Design, we need two assumptions for the ATE to be identified in the population: Independence and Brute Force Validity.

```{definition,independence,name="Independence"}
We assume that the allocation of the program is independent of potential outcomes: 
\begin{align*}
  R_i\Ind(Y_i^0,Y_i^1).
\end{align*}
```

Here, $\Ind$ codes for independence or random variables.
Independence can be enforced by the randomized allocation. 

We need a second assumption for the Brute Force Design to work:

```{definition,BF,name="Brute Force Validity"}
We assume that the randomized allocation of the program is mandatory and does not interfere with how potential outcomes are generated:
\begin{align*}
Y_i & = 
  \begin{cases}
    Y_i^1 & \text{ if } R_i=1  \\
    Y_i^0 & \text{ if } R_i=0      
  \end{cases}
\end{align*}
with $Y_i^1$ and $Y_i^0$ the same potential outcomes as defined in Lecture~0 with a routine allocation of the treatment.
```

Under both Idependence and Brute Force Validity, we have the follwing result:

```{theorem,BFATE,name="Identification in the Brute Force Design"}
Under Assumptions \@ref(def:independence) and \@ref(def:BF), the WW estimator identifies the Average Effect of the Treatment (ATE):
\begin{align*}
  \Delta^Y_{WW} & = \Delta^Y_{ATE},
\end{align*}

```

with:
\begin{align*}
  \Delta^Y_{WW} & = \esp{Y_i|R_i=1} - \esp{Y_i|R_i=0} \\
  \Delta^Y_{ATE} & = \esp{Y_i^1-Y_i^0}. 
\end{align*}

```{proof}
\begin{align*}
  \Delta^Y_{WW} & = \esp{Y_i|R_i=1} - \esp{Y_i|R_i=0} \\
                & = \esp{Y^1_i|R_i=1} - \esp{Y^0_i|R_i=0} \\
                & = \esp{Y_i^1}-\esp{Y_i^0}\\
                & = \esp{Y_i^1-Y_i^0},
\end{align*}
where the first equality uses Assumption \@ref(def:BF), the second equality Assumption \@ref(def:independence) and the last equality the linearity of the expectation operator.
```

```{remark}
As you can see from Theorem \@ref(thm:BFATE), ATE is the average effect of the treatment on the whole population, those who would be eligible for it and those who would not. 
ATE differs from TT because the effect of the treatment might be correlated with treatment intake. 
It is possible that the treatment has a bigger (resp. smaller) effect on treated individuals.
In that case, ATE is higher (resp. smaller) than TT.
```

```{remark}
Another related design is the Brute Force Design among Eligibles.
In this design, you impose the treatment status only among eligibles, irrespective of whether they want the treatment or not.
It can be operationalized using the selection rule used in Section \@ref(sec:design2).
```

```{example}
Let's use the example to illustrate the concept of ATE.
Let's generate data with our usual parameter values without allocating the treatment yet:
```

```{r parambis,eval=TRUE,echo=TRUE,results='hide'}
param <- c(8,.5,.28,1500,0.9,0.01,0.05,0.05,0.05,0.1)
names(param) <- c("barmu","sigma2mu","sigma2U","barY","rho","theta","sigma2epsilon","sigma2eta","delta","baralpha")
```

```{r simulbis,eval=TRUE,echo=TRUE,results='hide'}
set.seed(1234)
N <-1000
mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
UB <- rnorm(N,0,sqrt(param["sigma2U"]))
yB <- mu + UB 
YB <- exp(yB)
Ds <- rep(0,N)
Ds[YB<=param["barY"]] <- 1 
epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
U0 <- param["rho"]*UB + epsilon
y0 <- mu +  U0 + param["delta"]
alpha <- param["baralpha"]+  param["theta"]*mu + eta
y1 <- y0+alpha
Y0 <- exp(y0)
Y1 <- exp(y1)
```

In the sample, the ATE is the average difference between $y_i^1$ and $y_i^0$, or -- the expectation operator being linear -- the difference between average $y_i^1$ and average $y_i^0$.
In our sample, the former is equal to `r round(mean(y1-y0),3)` and the latter to `r round(mean(y1)-mean(y0),3)`.

In the population, the ATE is equal to: 

\begin{align*}
\Delta^y_{ATE} & = \esp{Y_i^1-Y_i^0} \\
              & = \esp{\alpha_i} \\
              & = \bar{\alpha}+\theta\bar{\mu}.
\end{align*}

Let's write a function to compute the value of the ATE and of TT (we derived the formula for TT in the previous lecture):
```{r delta.y,eval=TRUE,echo=TRUE,results='hide'}
delta.y.ate <- function(param){
  return(param["baralpha"]+param["theta"]*param["barmu"])
}
delta.y.tt <- function(param){
  return(param["baralpha"]+param["theta"]*param["barmu"]-param["theta"]*((param["sigma2mu"]*dnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))/(sqrt(param["sigma2mu"]+param["sigma2U"])*pnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))))
}
```

In the population, with our parameter values, $\Delta^y_{ATE}=$ `r round(delta.y.ate(param),3)` and $\Delta^y_{TT}=$ `r round(delta.y.tt(param),3)`.
In our case, selection into the treatment is correlated with lower outcomes, so that $TT\leq ATE$.

In order to implement the Brute Force Design in practice in a sample, we simply either draw a coin repeatedly for each member of the sample, assigning for example, all "heads" to the treatment and all "tails" to the control.
Because it can be a little cumbersome, it is possible to replace the coin toss by a pseudo-Random Number Generator (RNG), which is is an algorithm that tries to mimic the properties of random draws.
When generating the samples in the numerical exmples, I actually use a pseudo-RNG.
For example, we can draw from a uniform distribution on $[0,1]$ and allocate to the treatment all the individuals whose draw is smaller than 0.5:

\begin{align*}
  R_i^* & \sim \mathcal{U}[0,1]\\
  R_i & = 
  \begin{cases}
    1 & \text{ if } R_i^*\leq .5 \\
    0 & \text{ if } R_i^*> .5 
  \end{cases}
\end{align*}

The advantage of using a uniform law is that you can set up proportions of treated and controls easily.

```{example}
In our numerical example, the following R code generates two random groups, one treated and one control, and imposes the Assumption of Brute Force Validity:
```

```{r random.BFD,eval=TRUE,echo=TRUE,results='hide'}
# randomized allocation of 50% of individuals
Rs <- runif(N)
R <- ifelse(Rs<=.5,1,0)
y <- y1*R+y0*(1-R)
Y <- Y1*R+Y0*(1-R)
```

```{remark}
It is interesting to stop for one minute to think about how the Brute Force Design solves the FPCI.
First, with the ATE, the counterfactual problem is more severe than in the case of the TT.
In the routine mode of the program, where only eligible individuals receive the treatment, both parts of the ATE are unobserved:
```

  * $\esp{Y_i^1}$ is unobserved since we only observe the expected value of outcomes for the treated $\esp{Y_i^1|D_i=1}$, and they do not have to be the same.
  * $\esp{Y_i^0}$ is unobserved since we only observe the expected value of outcomes for the untreated $\esp{Y_i^0|D_i=0}$, and they do not have to be the same.

What the Brute Force Design does, is that it allocates randomly one part of the sample to the treatment, so that we see $\esp{Y_i^1|R_i=1}=\esp{Y_i^1}$ and one part to the control so that we see $\esp{Y_i^0|R_i=0}=\esp{Y_i^0}$.

### Estimating ATE

#### Using the WW estimator

In order to estimate ATE in a sample where the treatment has been randomized using a Brute Force Design, we simply use the sample equivalent of the With/Without estimator:
\begin{align*}
  \hat{\Delta}^Y_{WW} & = \frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N Y_iR_i-\frac{1}{\sum_{i=1}^N (1-R_i)}\sum_{i=1}^N Y_i(1-R_i).
\end{align*}

```{example}
In our numerical example, the WW estimator can be computed as follows in the sample:
```

```{r delta.y.ww,eval=TRUE,echo=TRUE,results='hide'}
delta.y.ww <- mean(y[R==1])-mean(y[R==0])
```

The WW estimator of the ATE in the sample is equal to `r round(delta.y.ww,3)`.
Let's recall that the true value of ATE is `r round(delta.y.ate(param),3)` in the population and `r round(mean(y1-y0),3)` in the sample.

We can also see in our example how the Brute Force Design approximates the counterfactual expectation $\esp{y_i^1}$ and its sample equivalent mean $\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^1_i$ by the observed mean in the treated sample $\frac{1}{\sum_{i=1}^N R_i}\sum_{i=1}^N y_iR_i$.
In our example, the sample value of the counterfactual mean potential outcome $\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^1_i$ is equal to `r round(mean(y1),3)` and the sample value of its observed counterpart is `r round(mean(y[R==1]),3)`.
Similarly, the sample value of the counterfactual mean potential outcome $\frac{1}{\sum_{i=1}^N}\sum_{i=1}^N y^0_i$ is equal to `r round(mean(y0),3)` and the sample value of its observed counterpart is `r round(mean(y[R==0]),3)`.

#### Using OLS

As we have seen in Lecture 0, the WW estimator is numerically identical to the OLS estimator of a linear regression of outcomes on treatment:
The OLS coefficient $\beta$ in the following regression: 
\begin{align*}
  	Y_i &  = \alpha +  \beta R_i + U_i
	\end{align*}
is the WW estimator.

```{example}
In our numerical example, we can run the OLS regression as follows:
```

```{r delta.y.ols,eval=TRUE,echo=TRUE,results='hide'}
reg.y.R.ols <- lm(y~R)
```

$\hat{\Delta}^y_{OLS}=$ `r round(reg.y.R.ols$coef[2],3)` which is exactly equal, as expected, to the WW estimator: `r round(delta.y.ww,3)`.


#### Using OLS conditioning on covariates

The advantage of using OLS other the direct WW comparison is that it gives you a direct estimate of sampling noise (see next section) but also that it enables you to condition on additional covariates in the regression:
The OLS coefficient $\beta$ in the following regression: 
\begin{align*}
  	Y_i &  = \alpha +  \beta R_i + \gamma' X_i + U_i
	\end{align*}
is a consistent (and even unbiased) estimate of the ATE.

**proof needed, especially assumption of linearity.**
**Also, is interaction between $X_i$ and $R_i$ needed? See lecture 3.**

```{example}
In our numerical example, we can run the OLS regression conditioning on $y_i^B$ as follows:
```

```{r delta.y.ols.yB,eval=TRUE,echo=TRUE,results='hide'}
reg.y.R.ols.yB <- lm(y~R + yB)
```

$\hat{\Delta}^y_{OLSX}=$ `r round(reg.y.R.ols.yB$coef[2],3)`.
Note that $\hat{\Delta}^y_{OLSX}\neq\hat{\Delta}^y_{WW}$.
There is no numerical equivalence between the two estimators.

```{remark}
Why would you want to condition on covariates in an RCT?
Indeed, covariates should be balanced by randomization and thus there does not seem to be a rationale for conditioning on potential confounders, since there should be none.
The main reason why we condition on covariates is to decrease sampling noise.
Remember that sampling noise is due to imbalances between confounders in the treatment and control group.
Since these imbalances are not systematic, the WW estimator is unbiased.
We can also make the bias due to these unbalances as small as we want by choosing an adequate sample size (the WW estimator is consistent).
But for a given sample size, these imbalances generate sampling noise around the true ATE.
Conditioning on covariates helps decrease sampling noise by accounting for imbalances due to observed covariates.
If observed covariates explain a large part of the variation in outcomes, conditioning on them is going to prevent a lot of sampling noise from occuring.
```

## Randomization After Self-Selection {#sec:design2}

## Randomization After Eligibility {#sec:design3}

## Encouragement Design {#sec:design4}

## Threats to the validity of RCTs {#sec:threats}
